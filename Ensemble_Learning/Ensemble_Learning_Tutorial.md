# 2022 Business Analytics Chapter 4: Ensemble Learning ğŸ¤¹â€â™€ï¸
## Python Tutorial: Bagging(Random Forest) vs. Boosting(Gradient Boosting & CatBoost)
### 2022010558 ê¹€ì§€í˜„ğŸ²

<br/>

<br/>

# Ensemble Learning: Overview
í”„ë‘ìŠ¤ì–´ë¡œ 'ensemble'ì€ 'ì¡°í™”' í˜¹ì€ 'í†µì¼'ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ì°¨ìš©í•˜ì—¬ ì´ë¦„ ì§€ì–´ì§„ 'Ensemble Learning'ì˜ ì˜ë¯¸ëŠ”, ë‹¨ì¼ ëª¨ë¸ë“¤ì„ ì—¬ëŸ¬ ê°œ ëª¨ì•„ í•™ìŠµì‹œì¼œ, ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ ê°’ì„ ì·¨í•©í•˜ëŠ” ë°©ì‹ì„ ëœ»í•©ë‹ˆë‹¤. ë”°ë¼ì„œ Ensemble Learningì€ ì¼ë°˜ì ìœ¼ë¡œ ë‹¨ì¼ ëª¨ë¸ë³´ë‹¤ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ìë‘í•œë‹¤ëŠ” íŠ¹ì§•ì„ ê°€ì§‘ë‹ˆë‹¤.   

**ê·¸ë ‡ë‹¤ë©´ Ensemble Learningìœ¼ë¡œ ì–´ë–»ê²Œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ê±¸ê¹Œìš”?**

<br/>

## <span style="color:darkblue">Bais</span>-<span style="color:purple">Variance</span> Decomposition
ìš°ë¦¬ê°€ ë§Œë“œëŠ” ì˜ˆì¸¡ ëª¨ë¸ì€ ë¯¸ë˜ì— ë“¤ì–´ì˜¤ëŠ” ìƒˆë¡œìš´ x ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ì˜¤ì°¨ì˜ ê¸°ëŒ“ê°’ì„ ì¤„ì´ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•˜ë©°, ì´ë•Œ **ì˜¤ì°¨**ì˜ ê¸°ëŒ“ê°’ì€ ëª¨ë¸ì˜ **Bias(í¸í–¥)** ì™€ **Varaince(ë¶„ì‚°)** ë¡œ ë¶„í•´ë©ë‹ˆë‹¤.   

<p align="center">
    <img src="Img/Noise.PNG" width="650"/>
</p>

ë¬¼ë¡  ì˜¤ì°¨ì—ëŠ” Biasì™€ Varianceë§Œ ìˆëŠ” ê²ƒì´ ì•„ë‹™ë‹ˆë‹¤. Noiseë¼ í•˜ì—¬, ë°ì´í„° ìˆ˜ì§‘ ì‹œì— ìì—° ë°œìƒì ìœ¼ë¡œ ì¼ì–´ë‚˜ëŠ” ë¶ˆê°€í”¼í•œ ë³€ë™ë„ ìˆì£ . ì´ì „ [Anomaly Detection íŠœí† ë¦¬ì–¼](https://github.com/Im-JihyunKim/BusinessAnalytics/blob/main/Anomaly_Detection/Anomaly_Detection_Tutorial.md)ì—ì„œë„ ë‹¤ë£¨ì—ˆì§€ë§Œ, ë…¸ì´ì¦ˆëŠ” ì •í™•í•œ ì¶”ì •ì€ ë¶ˆê°€ëŠ¥í•˜ê³ , ë‹¤ë§Œ ì„œë¡œ ë…ë¦½ì ì´ê³  ì¼ì •í•œ ë¶„ì‚°ì„ ê°€ì§„ë‹¤ê³  ì„ì˜ë¡œ ê°€ì •í•˜ëŠ” ê°’ì…ë‹ˆë‹¤. ê·¸ë¦¬ê³  ì˜ ëª¨ë¥´ë‹ˆ ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •í•˜ì£ .   

ê·¸ë ‡ë‹¤ë©´ Biasì™€ VarianceëŠ” ë¬´ì—‡ì¼ê¹Œìš”? ìœ„ ìŠ¬ë¼ì´ë“œì—ë„ ì í˜€ìˆì§€ë§Œ, ë¨¼ì € **<span style="color:darkblue">BiasëŠ” ëª¨ë¸ì„ ë°˜ë³µì ìœ¼ë¡œ í•™ìŠµì‹œì¼°ì„ ë•Œ ë„ì¶œë˜ëŠ” ì˜ˆì¸¡ ê°’ì˜ í‰ê· **ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. í‰ê· ì ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ì •í™•í•œ ì¶”ì •ì´ ê°€ëŠ¥í•œì§€ë¥¼ ì¸¡ì •í•˜ëŠ” ì§€í‘œì´ì£ . **<span style="color:purple">VarianceëŠ” ëª¨ë¸ì„ ë°˜ë³µì ìœ¼ë¡œ í•™ìŠµì‹œì¼°ì„ ë•Œ ê°œë³„ì ì¸ ì˜ˆì¸¡ ê°’ì´ í‰ê·  ì˜ˆì¸¡ ê°’ê³¼ ì–¼ë§ˆë‚˜ ì°¨ì´ ë‚˜ëŠ”ì§€ë¥¼ ì¸¡ì •**í•˜ëŠ” ì§€í‘œì…ë‹ˆë‹¤. ì˜ˆì¸¡ ì¶”ì • ê°’ì˜ í¸ì°¨ë¥¼ ê³„ì‚°í•˜ëŠ” ì§€í‘œì¸ ê²ƒì´ì£ .

ì´ë•Œ ëª¨ë¸ì˜ ì˜¤ì°¨ê°€ Biasì™€ Varianceë¡œ Decomposition ëœë‹¤ëŠ” ì˜ë¯¸ëŠ” ë­˜ê¹Œìš”? ë§Œì¼ ìš°ë¦¬ì˜ ì˜ˆì¸¡ Taskê°€ Regressionì´ê³ , ì˜ˆì¸¡ ì˜¤ì°¨ë¥¼ MSEë¥¼ í†µí•´ ê³„ì‚°í•œë‹¤ê³  í•´ë´…ì‹œë‹¤. ê·¸ë ‡ë‹¤ë©´ ë°ì´í„° $x_0$ê°€ ë“¤ì–´ì™”ì„ ë•Œ ëª¨ë¸ì˜ ErrorëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

$$Expected \ MSE(x_0) = E[y-\hat{F}(x)|x=x_0]^2$$

ì´ë•Œ ì¶”ì • ê°’ì€ ì–¸ì œë‚˜ ë…¸ì´ì¦ˆë¥¼ ê°€ì§€ê¸° ë•Œë¬¸ì—, epsilonì„ ì´ìš©í•´ì„œ ì•„ë˜ì²˜ëŸ¼ ë‹¤ì‹œ í‘œê¸°í•  ìˆ˜ ìˆê² ì£ . NosieëŠ” ë…ë¦½ìœ¼ë¡œ ê°€ì •í•˜ë‹ˆ ë°”ê¹¥ìœ¼ë¡œ ë‹¤ì‹œ ë¹¼ì„œ $\sigma ^2$ë¡œ í‘œí˜„í•©ë‹ˆë‹¤.

$$\begin{aligned}
Expected \ MSE(x_0) &= E[y-\hat{F}(x)|x=x_0]^2 \\
&= E[F^*(x_0)+Îµ - \hat{F}(x_0)]^2 \\
&= E[F^*(x_0) - \hat{F}(x_0)]^2 + \sigma ^2
\end{aligned}$$

ì•ì„œ ì–¸ê¸‰í•œ Biasì™€ Varianceì— ëŒ€í•œ ê°œë…ì„ ì§šì–´ë´…ì‹œë‹¤. ë‘˜ ëª¨ë‘ "ì˜ˆì¸¡ ê°’ì˜ í‰ê·  $\bar{F}(x)$"ì„ ì´ìš©í•˜ì—¬ ê³„ì‚°ë©ë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ìœ„ ìˆ˜ì‹ì—ì„œ ì˜ˆì¸¡ ê°’ì˜ í‰ê· ì„ ë”í•˜ê³  ë¹¼ì£¼ë©´ ì–´ë–»ê²Œ ë ê¹Œìš”? ë™ì¼í•œ ê°’ì„ ë”í•˜ê³  ë¹¼ì£¼ë‹ˆ ìˆ˜ì‹ì€ ë™ì¼í•´ì§€ê² ì£ .

$$\begin{aligned}
Expected \ MSE(x_0) &= E[y-\hat{F}(x)|x=x_0]^2 \\
&= E[F^*(x_0)+Îµ - \hat{F}(x_0)]^2 \\
&= E[F^*(x_0) - \hat{F}(x_0)]^2 + \sigma ^2 \\
&= E[F^*(x_0) - \bar{F}(x_0) + \bar{F}(x_0) \hat{F}(x_0)]^2 + \sigma ^2
\end{aligned}$$

ë§¨ ì•„ë˜ ìˆ˜ì‹ì„ ë³´ë‹ˆ $(A+B)^2$ ì˜ ê¼´ì…ë‹ˆë‹¤. ì´ë¥¼ í’€ì–´ì„œ ì „ê°œí•´ë³´ê² ìŠµë‹ˆë‹¤.

$$\begin{aligned}
Expected \ MSE(x_0) &= E[y-\hat{F}(x)|x=x_0]^2 \\
&= E[F^*(x_0)+Îµ - \hat{F}(x_0)]^2 \\
&= E[F^*(x_0) - \hat{F}(x_0)]^2 + \sigma ^2 \\
&= E[F^*(x_0) - \bar{F}(x_0) + \bar{F}(x_0) \hat{F}(x_0)]^2 + \sigma ^2 \\
&= E[F^*(x_0)-\bar{F}(x_0)]^2 + E[\bar{F}(x_0)-\hat{F}(x_0)]^2+Ïƒ^2 \\
&= \color{Purple} \color{DarkBlue} [F^*(x_0)-\bar{F}(x_0)]^2 \color{Black}+ \color{Purple} E[\bar{F}(x_0)-\hat{F}(x_0)]^2 \color{Black}+Ïƒ^2 \\
&=\color{DarkBlue}Bias^2(F(x_0)) \color{Black}+ \color{Purple}Var(\hat{F}(x_0)) \color{Black} +Ïƒ^2
\end{aligned}$$

ë¨¼ì € $\color{darkblue}F^*(x_0)-\bar{F}(x_0)$ì€ $x_0$ê°€ ì§„ì§œ ì •ë‹µ $F^*$ì™€ $x_0$ê°€ ì…ë ¥ë˜ì—ˆì„ ë•Œ ëª¨ë¸ë“¤ì˜ ì˜ˆì¸¡ í‰ê·  $\bar{F}(x_0)$ ê°„ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•œ ì‹ì…ë‹ˆë‹¤. ì¦‰, ì´ëŠ” **<span style="color:darkblue">Bias(í¸í–¥)** ë¥¼ ì˜ë¯¸í•˜ëŠ” ì‹ì¸ ê²ƒì…ë‹ˆë‹¤.   

$\color{purple}\bar{F}(x_0)-\hat{F}(x_0)$ ì€ $x_0$ê°€ ì…ë ¥ë˜ì—ˆì„ ë•Œ ëª¨ë¸ë“¤ì´ ì˜ˆì¸¡í•œ ê°’ì˜ í‰ê·  $\bar{F}(x_0)$ì™€ ê°œë³„ ì˜ˆì¸¡ ê°’ $\hat{F}(x_0)$ ê°„ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•œ ì‹ì…ë‹ˆë‹¤. ì´ëŠ” **<span style="color:purple">Variance(ë¶„ì‚°)** ì„ ì˜ë¯¸í•˜ëŠ” ì‹ì´ ë˜ê² ì£ .

ê²°ë¡ ì ìœ¼ë¡œ ì„ì˜ì˜ ë¯¸ë˜ ë°ì´í„° $x_0$ì˜ ì˜¤ì°¨ ê¸°ëŒ“ê°’ì€, ëª¨ë¸ì˜ **<span style="color:darkblue">Bias(í¸í–¥)** ì™€ **<span style="color:purple">Variance(ë¶„ì‚°)**, ê·¸ë¦¬ê³  **Natural Errorì¸ Noise** ì´ 3ê°€ì§€ ìš”ì†Œë¡œ ë¶„í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  NoiseëŠ” ìì—° ë°œìƒì ì´ê³  ë¶ˆê°€í”¼í•œ ë³€ë™ì´ë‹ˆ, ìš°ë¦¬ê°€ ì¤„ì—¬ë‚˜ê°ˆ ìˆ˜ ìˆëŠ” ê²ƒì€ Biasì™€ Varianceê°€ ë˜ê² ì£ .   

### <span style="color:darkblue">Bias</span>ì™€ <span style="color:purple">Variance</span>ì— ë”°ë¥¸ ëª¨ë¸ êµ¬ë¶„
ê·¸ë ‡ë‹¤ë©´ ì˜ˆì¸¡ ëª¨ë¸ì€ ìœ„ì—ì„œ ë§í•œ ì˜¤ë¥˜ì— ë”°ë¼, Biasê°€ ë‚®ì€ ëª¨ë¸ê³¼ Varianceê°€ ë‚®ì€ ëª¨ë¸ë¡œ êµ¬ë¶„í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.   

**<span style="color:darkblue">ëª¨ë¸ì˜ Bias**ê°€ í¬ê³  ì‘ë‹¤ëŠ” ê²ƒì€ ê²°êµ­ ë¬´ìŠ¨ ì˜ë¯¸ì¼ê¹Œìš”?
- **<span style="color:darkblue">ëª¨ë¸ì˜ Biasê°€ í¬ë‹¤ë©´**, ìš°ë¦¬ì˜ í•™ìŠµ ë°ì´í„°ì˜ íŒ¨í„´ì„ ì œëŒ€ë¡œ ë°˜ì˜í•˜ì§€ ëª»í–ˆë‹¤ëŠ” ì˜ë¯¸, ì¦‰ Training Errorê°€ í¬ë‹¤ëŠ” ì˜ë¯¸ì¼ ê²ƒì…ë‹ˆë‹¤.
- **<span style="color:darkblue">ëª¨ë¸ì˜ Biasê°€ ë‚®ë‹¤ë©´**, í•™ìŠµ ë°ì´í„°ì˜ íŒ¨í„´ì„ ì œëŒ€ë¡œ ë°˜ì˜í•˜ì—¬ì„œ Training Errorë¥¼ ìµœì†Œí™” í•œë‹¤ëŠ” ì˜ë¯¸ì´ê² ì£ . ì‹¤ì œ ê°’ê³¼ ì˜ˆì¸¡ ê°’ í‰ê· ì˜ ì°¨ì´ê°€ ì‘ë‹¤ëŠ” ê²ƒì´ë‹ˆê¹Œìš”.

**<span style="color:purple">ëª¨ë¸ì˜ Variance**ê°€ í¬ê³  ì‘ë‹¤ëŠ” ê²ƒì€ ë‹¤ìŒê³¼ ê°™ì€ ì˜ë¯¸ë¥¼ ê°€ì§‘ë‹ˆë‹¤.
- **<span style="color:purple">ëª¨ë¸ì˜ Varianceê°€ í¬ë‹¤ë©´**, ë°ì´í„°ê°€ ë°”ë€ë‹¤ë©´, ëª¨ë¸ì˜ ì˜ˆì¸¡ ê°’ì— ë§ì€ ë³€ë™ì´ ì˜ˆìƒëœë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.
- **<span style="color:purple">ëª¨ë¸ì˜ Varianceê°€ ë‚®ë‹¤ë©´**, ë°ì´í„° ë° í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ ë°”ë€ë‹¤ í•˜ë”ë¼ë„, ì˜ˆì¸¡ ê°’ì— ë³€ë™ì´ í¬ì§€ ì•Šë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤. VarianceëŠ” ëª¨ë¸ì˜ ì˜ˆì¸¡ ê°’ì´ í‰ê·  ì˜ˆì¸¡ ê°’ìœ¼ë¡œë¶€í„° í¼ì§„ ì •ë„ë¥¼ ì¸¡ì •í•˜ëŠ” ê²ƒì´ë‹ˆ, ì˜ˆì¸¡ ê°’ ì‚¬ì´ì— í° ì°¨ì´ê°€ ì—†ë‹¤ëŠ” ê²ƒì´ì£ .

ì´ë¥¼ í†µí•´ì„œ ëª¨ë¸ì„ ì•„ë˜ì™€ ê°™ì´ ì´ 4ê°€ì§€ ê²½ìš°ë¡œ êµ¬ë¶„í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.
<p align="center">
    <img src="Img/Bias_Variance.PNG" width="650"/>
</p>

Case 1ì€ Biasì™€ Varianceê°€ ëª¨ë‘ ë†’ì€ ê²½ìš°ë¡œ, ì„±ëŠ¥ì´ ë§¤ìš° ì¢‹ì§€ ì•Šì€ Worst Caseì…ë‹ˆë‹¤. **ìš°ë¦¬ê°€ ë‹¤ë£¨ëŠ” ëŒ€ë¶€ë¶„ì˜ ì˜ˆì¸¡ ëª¨ë¸ì€ Case 2ì™€ Case 3ë¡œ êµ¬ë¶„**í•  ìˆ˜ ìˆì£ .
- **<span style="color:darkblue">Case 2ëŠ” Biasê°€ ë‚®ê³  Varianceê°€ ë†’ì€ ëª¨ë¸**ì…ë‹ˆë‹¤.
    - Biasê°€ ë‚®ê¸° ë•Œë¬¸ì— Training Errorê°€ Case 3ë³´ë‹¤ ë‚®ì§€ë§Œ, Varianceê°€ ë†’ê³  êµ¬ê°„ ì¶”ì • ë²”ìœ„ê°€ ë„“ì–´ Testing Errorê°€ ë†’ë‹¤ëŠ” íŠ¹ì§•ì„ ê°€ì§‘ë‹ˆë‹¤. ì¦‰ **Overfitting**ì˜ ê²½í–¥ì„ ë³´ì¼ ìˆ˜ ìˆëŠ” ëª¨ë¸ì´ì£ .
    - ì´ë•Œ Case 2ì— ì†í•˜ëŠ” ëª¨ë¸ì€ ê¸°ë³¸ì ìœ¼ë¡œ **ëª¨ë¸ì˜ ë³µì¡ë„(VC Dimension)ê°€ ë†’ì•„ ê°œë³„ì ì¸ Training Error(Empirical Error)ê°€ ë‚®ìŠµë‹ˆë‹¤**.   

<br/>

- **<span style="color:purple">Case 3ì€ Varianceê°€ ë‚®ê³  Biasê°€ ë†’ì€ ëª¨ë¸**ì…ë‹ˆë‹¤.
    - Varianceê°€ ë‚®ê¸° ë•Œë¬¸ì— Training Errorë„ ë†’ê³ , Testingì— ëŒ€í•œ ì˜ˆì¸¡ë ¥ë„ ë‚®ì£ . ì´ëŠ” **Underfitting**ì´ ëœ ì „í˜•ì ì¸ ëª¨ë¸ì˜ ì˜ˆë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - Case 3ì— ì†í•˜ëŠ” ëª¨ë¸ì€ ê¸°ë³¸ì ìœ¼ë¡œ **ëª¨ë¸ì˜ ë³µì¡ë„(VC Dimension)ê°€ ë‚®ë‹¤ëŠ” íŠ¹ì§•**ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.

> <span style="color:gray">__[ì°¸ê³ ]__   
VC Dimensionê³¼ Empirical Errorì˜ ê´€ê³„ì— ëŒ€í•´ì„œëŠ” [Kernel Based Learning íŠœí† ë¦¬ì–¼](https://github.com/Im-JihyunKim/BusinessAnalytics/blob/main/Ch2_Kernel_Based_Learning(SVM)_Tutorial.ipynb)ì— ë³´ë‹¤ ìì„¸íˆ ê¸°ìˆ ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

<br/>

## <span style="color:darkblue">Bagging</span> vs. <span style="color:purple">Boosting
ê·¸ë ‡ë‹¤ë©´ Ensemble Learningì—ì„œëŠ” ì–´ë–»ê²Œ ë‹¨ì¼ëª¨ë¸ë³´ë‹¤ ì˜ˆì¸¡ ì˜¤ë¥˜ë¥¼ ê°ì†Œì‹œí‚¬ ìˆ˜ ìˆì„ê¹Œìš”?   
ë³¸ íŠœí† ë¦¬ì–¼ì—ì„œ ë‹¤ë£° Ensemble Learningì˜ ë°©ë²•ë¡ ì€ **<span style="color:darkblue">Bagging**ê³¼ **<span style="color:purple">Boosting**ì´ë©°, ê° ë°©ë²•ë¡ ì€ ë‹¤ìŒê³¼ ê°™ì€ íŠ¹ì§•ì„ ê°€ì§‘ë‹ˆë‹¤.   

<p align="center">
    <img src="Img/bagging_boosting_1.PNG" width="650"/>
</p>

**<span style="color:darkblue">Bagging (Bootstrap Aggregating)</span>**:   
- **<span style="color:darkblue">í•™ìŠµ ë°ì´í„°ì…‹ì„ Randomí•˜ê²Œ ì¶”ì¶œí•˜ì—¬ ëª¨ë¸ì„ ê°ê° ë‹¤ë¥´ê²Œ í•™ìŠµì‹œí‚¤ëŠ” ë°©ë²•ë¡ **ì…ë‹ˆë‹¤.
- ë”°ë¼ì„œ **<span style="color:darkblue">ê°œë³„ ëª¨ë¸ë“¤ì€ ì„œë¡œ ë…ë¦½ì **ì´ë©° ì˜í–¥ì„ ì£¼ê³  ë°›ì§€ ì•ŠëŠ”ë‹¤ëŠ” íŠ¹ì§•ì´ ìˆìŠµë‹ˆë‹¤. ì¦‰, ê° ëª¨ë¸ë“¤ì˜ í•™ìŠµ ë° ì¶”ë¡ ì´ **<span style="color:darkblue">ë³‘ë ¬ì (Parallel)** ìœ¼ë¡œ ì´ë£¨ì–´ì§€ëŠ” ê²ƒì´ì£ .
- ì´ë•Œ ë°ì´í„°ë¥¼ ë¬´ì‘ìœ„ë¡œ ì¶”ì¶œí•  ë•Œ, **ì¤‘ë³µì„ í—ˆìš©í•˜ëŠ” ë³µì› ì¶”ì¶œì´ë©´ Bootstrapping**, ì¤‘ë³µì„ í—ˆìš©í•˜ì§€ ì•Šìœ¼ë©´ Pastingì´ë¼ í•©ë‹ˆë‹¤.
- ì¼ë°˜ì ìœ¼ë¡œëŠ” **<span style="color:darkblue">Bootstrappingì„ í†µí•´ ëª¨ë¸ ë³„ë¡œ í•™ìŠµ ë°ì´í„°ë¥¼ ë§Œë“¤ê³ , ì´ë“¤ì„ aggregationí•˜ì—¬ ì˜ˆì¸¡ ê°’ì„ ëª¨ìœ¼ëŠ” ë°©ì‹**ì„ íƒí•©ë‹ˆë‹¤. ê·¸ë˜ì„œ "Bootstrap Aggregating"ì´ë¼ ì´ë¦„ ë¶™ì—¬ì§„ ê²ƒì´ì£ .

**<span style="color:purple">Boosting**:   
- Boostingì€ **<span style="color:purple">ì„±ëŠ¥ì´ ì•½í•œ Weak Learnerë¥¼ ì—¬ëŸ¬ ê°œ ì—°ê²°í•˜ì—¬ Strong Learnerë¥¼ ë§Œë“œëŠ” ë°©ë²•ë¡ **ì…ë‹ˆë‹¤.
- ì•ì—ì„œ í•™ìŠµëœ ëª¨ë¸ì˜ ì•½ì ì„ ë³´ì™„í•´ ë‚˜ê°€ë©´ì„œ ë” ë‚˜ì€ ëª¨ë¸ë¡œ í•™ìŠµì‹œí‚¤ëŠ” ë°©ì‹ì´ì£ . ì´ë¥¼í…Œë©´ **<span style="color:purple">ì´ì „ ëª¨ë¸ì´ ì˜ëª» ì˜ˆì¸¡í•œ ë°ì´í„°ì— ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ê³ , ë‹¤ìŒ ëª¨ë¸ì´ ì´ì— ëŒ€í•œ ì˜¤ë¥˜ë¥¼ ê°œì„ í•´ ë‚˜ê°€ë©° í•™ìŠµ íˆëŠ” ë°©ì‹**ì…ë‹ˆë‹¤.
- ë”°ë¼ì„œ **<span style="color:purple">ì„ í–‰ ëª¨ë¸ì˜ ì„±ê³¼ì— ì˜ì¡´ì **ì´ë©°, ì„ í–‰ ëª¨ë¸ì˜ ê°€ì´ë“œê°€ í•„ìš”í•˜ê¸°ì— í•™ìŠµ ë° ì¶”ë¡ ì´ **<span style="color:purple">ìˆœì°¨ì (Sequential)** ì´ë¼ëŠ” íŠ¹ì§•ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.

<br/>


<p align="center">
    <img src="Img/bagging_boosting.PNG" width="650"/>
</p>

**<span style="color:darkblue">Baggingì€ Case 2ì™€ ê°™ì´ Biasê°€ ë‚®ì€ ëª¨ë¸ë“¤ì„ ì´ìš©í•´ì„œ Varianceë¥¼ ì¤„ì—¬ë‚˜ê°€ëŠ” ë°©ì‹ìœ¼ë¡œ ì˜ˆì¸¡ ì˜¤ë¥˜ë¥¼ ê°ì†Œ**ì‹œí‚¤ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ê° ê°œë³„ ëª¨ë¸ì˜ ì„±ëŠ¥ì€ ì¢‹ì§€ë§Œ, ê·¸ í¸ì°¨ê°€ ìˆë‹¤ë©´ ì´ë¥¼ ì¤„ì¼ ìˆ˜ ìˆëŠ” ë°©ë²•ë¡ ì´ì£ .   
- Case 2ì˜ ìƒí™©ì—ì„œ ì•™ìƒë¸”ì„ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ì„ ë•Œ **<span style="color:darkblue">Overfittingì´ ë¬¸ì œê°€ ëœë‹¤ë©´, ì´ë¥¼ í•´ê²°í•˜ëŠ” ë° Bagging ë°©ì‹ì„ ì´ìš©**í•  ìˆ˜ ìˆëŠ” ê²ƒì…ë‹ˆë‹¤.

<br/>

**<span style="color:purple">Boostingì€ Case 3ê³¼ ê°™ì´ Varianceê°€ ë‚®ì€ ëª¨ë¸ë“¤ì„ í•©ì³ì„œ Biasë¥¼ ì¤„ì´ëŠ” ë°©ì‹ìœ¼ë¡œ ì˜ˆì¸¡ ì˜¤ë¥˜ë¥¼ ê°ì†Œì‹œí‚¤ëŠ” ë°©ë²•**ì…ë‹ˆë‹¤. Sequentialí•˜ê²Œ ëª¨ë¸ì˜ ì•½ì ë“¤ì„ ë³´ì™„í•´ë‚˜ê°€ëŠ” ê²ƒì´ì£ .   
- ì¦‰, ì˜ˆì¸¡ ì„±ëŠ¥ì„ í–¥ìƒì„ ê¾€í•˜ëŠ” ë°©ì‹ì´ê¸° ë•Œë¬¸ì—, **<span style="color:purple">ì•™ìƒë¸” ëª¨ë¸ì„ ì‚¬ìš©í•˜ì§€ ì•Šì„ ë•Œ ì˜ˆì¸¡ ì„±ëŠ¥ì´ ë¬¸ì œ ë˜ë©´ Boosting ë°©ì‹ì„ í†µí•´ ì„±ëŠ¥ì„ ë†’ì´ëŠ” ê²ƒ**ì´ ì¼ë°˜ì ì…ë‹ˆë‹¤.

<br/>
ì ì´ì œ Ensemble Learningì˜ ì´ë¡ ì  ë°°ê²½ë¿ ì•„ë‹ˆë¼ Baggingê³¼ Boostingì˜ ì°¨ì´ë¥¼ ì•Œì•„ ë³´ì•˜ìœ¼ë‹ˆ, ë³¸ê²©ì ìœ¼ë¡œ êµ¬ì²´ì ì¸ ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•œ ì„¤ëª… ë° ì½”ë“œë¥¼ í†µí•´ ì´í•´ë¥¼ ë†’ì—¬ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

<br/>

# Set up for Python Tutorial
```python
# Import Libraries
import time
import numpy as np
np.random.seed(2022)
import pandas as pd

# For visualiation
%matplotlib inline
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_context("talk")
sns.set_style("white")
sns.set_palette("Pastel1")
plt.rcParams['axes.labelsize'] = 14
plt.rcParams['xtick.labelsize'] = 14
plt.rcParams['ytick.labelsize'] = 14
plt.rcParams['axes.unicode_minus'] = False

from sklearn.datasets import make_classification, make_regression
from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score

import warnings
warnings.filterwarnings('ignore')
```

<br/>

# <span style="color:darkblue">Bagging 1: Bagging with Decision Tree
`Scikit-Learn`ì—ì„œëŠ” Baggingì„ ê°„í¸í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤. í’€ê³ ì í•˜ëŠ” taskê°€ Classificationì¸ ê²½ìš°ì—ëŠ” `BaggingClassifier`ë¥¼, Regressionì¸ ê²½ìš°ì—ëŠ” `BaggingRegressor`ë¥¼ ì œê³µí•˜ì£ .   

ë³¸ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” Classification ë¬¸ì œë¥¼ í’€ë©° ê° ì•Œê³ ë¦¬ì¦˜ì˜ íŠ¹ì§•ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.
```python
from sklearn.ensemble import BaggingClassifier
```
ë°ì´í„°ì…‹ìœ¼ë¡œëŠ” [make_classification() í•¨ìˆ˜](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html)ë¥¼ ì´ìš©í•˜ì—¬ Binary Classification Taskë¥¼ í’€ê¸° ìœ„í•œ ê°€ìƒ ë°ì´í„°ë¥¼ ë§Œë“¤ì–´ëƒ…ë‹ˆë‹¤. í•´ë‹¹ ë°ì´í„°ì…‹ì—ëŠ” 20ê°œì˜ ì„¤ëª… ë³€ìˆ˜ì™€ 1,000ê°œì˜ ê´€ì¸¡ì¹˜ê°€ í¬í•¨ë˜ì–´ ìˆë„ë¡ ì„¤ì •í•˜ì˜€ìŠµë‹ˆë‹¤.   

```python
# Define and Get Dataset
def get_dataset():
    X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, 
                               n_redundant=5, random_state=2022)
    return X, y
```

<br/>

## <span style="background-color:#fff5b1"> [ì‹¤í—˜ 1] Decision Tree vs. Bagging with Decision Tree
ì•ì„œ VC Dimensionì´ ë†’ì€ ë‹¨ì¼ ëª¨ë¸ë¡œ ê³¼ì í•©ì´ ë°œìƒí•˜ëŠ” ê²½í–¥ì´ ìˆì„ ë•Œ, Baggingì„ í†µí•´ì„œ ì´ë¥¼ ì™„í™”í•  ìˆ˜ ìˆë‹¤ê³  í•˜ì˜€ìŠµë‹ˆë‹¤. ê·¸ ëŒ€í‘œì ì¸ Base Learnerë¡œëŠ” (1) Decision Tree, (2) Artificial Neural Network, (3) k-NN with small K ë“±ì´ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ëª¨ë¸ì€ ë˜í•œ Varianceê°€ ë†’ì•„ ì˜¤ë¥˜ë¥¼ ë°œìƒì‹œí‚¤ëŠ” íŠ¹ì§•ì„ ê°€ì§€ëŠ”ë°, **<span style="color:maroon">ê³¼ì—° Baggingì„ í†µí•´ì„œ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆì„ê¹Œìš”?**   

**<span style="color:maroon">ë¨¼ì € Decision Treeë¥¼ í™œìš©í•˜ì—¬, ë‹¨ì¼ ëª¨ë¸ì˜ ì„±ëŠ¥ê³¼ Baggingì˜ ì„±ëŠ¥ ì°¨ì´ë¥¼ ë¹„êµ**í•´ë³´ê² ìŠµë‹ˆë‹¤. ì´ë•Œ **Baggingì˜ ì—¬ëŸ¬ Decision Treeì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì·¨í•©í•˜ëŠ” ë° ìˆì–´ì„œëŠ”, ê° ëª¨ë¸ì˜ Accuracy Scoreì™€ AUROC Scoreì˜ í‰ê·  ê°’ì„ ì´ìš©**í•©ë‹ˆë‹¤.

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier
```
```python
# Define Data
X, y = get_dataset()

# Single Decision Tree
tree_clf = DecisionTreeClassifier(random_state=2022)

# Bagging with Decision Tree
bag_clf_tree = BaggingClassifier(
    DecisionTreeClassifier(random_state=2022), n_estimators=500, bootstrap=True, n_jobs=-1, random_state=2022
)
```
- ë¨¼ì € ìœ„ì™€ ê°™ì´ Datasetê³¼ Modelì„ ì •ì˜í•©ë‹ˆë‹¤.
```python
def evaluate_model(model, X, y):
    # Define the evaluation procedure
    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=2022)
    # Evaluate model and collect the results
    acc = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)
    roc_auc = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)
    
    # Report Performance
    print("Accuracy: %.2f (%.2f)" % (np.mean(acc), np.std(acc)))
    print("AUROC: %.2f (%.2f)" % (np.mean(roc_auc), np.std(roc_auc)))
```
- **<span style="color:maroon">ëª¨ë¸ì˜ ê²°ê³¼ ê°’ì€ Cross Valitaionì„ í†µí•´ ë½‘ì•„ëƒ…ë‹ˆë‹¤.** 
- ì´ë•Œ `RepeatedStratifiedKFold`ëŠ” êµì°¨ ê²€ì¦ì„ ë°˜ë³µì ìœ¼ë¡œ ì—¬ëŸ¬ ë²ˆ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” Classì…ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ì§€ì •í•œ íšŸìˆ˜ (`n_repeats`) ë§Œí¼ ë°˜ë³µí•´ì„œ Foldë¥¼ ë‚˜ëˆ„ê³ , Cross Validationì— ëŒ€í•œ Scoreë„ ë°˜ë³µ íšŸìˆ˜ë§Œí¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. `n_repeats`ì˜ ê¸°ë³¸ ê°’ì€ 10ì´ë©° ë³¸ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” 3ì„ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.
- ì´ë•Œ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ ë° Evaluation Procedureì˜ Stochastic Natureì— ì˜í•´ì„œ, ê²°ê³¼ ê°’ì€ ê·¸ë•Œ ê·¸ë•Œ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ ê°ì•ˆí•˜ì—¬ í‘œì¤€í¸ì°¨ë¥¼ í•¨ê»˜ ì‚°ì¶œí•©ë‹ˆë‹¤.

```python
# Decision Tree
evaluate_model(model=tree_clf, X=X, y=y)

# Bagging with Decision Tree
evaluate_model(model=bag_clf_tree, X=X, y=y)
```

### Results
|                 |__Decision Tree__|__Bagging with Decision Tree__|  
|-----------------|:-----------------:|:------------------:|
|__Mean Accuracy (std)__| 0.82 (0.04) | __0.90 (0.03)__ |
|__Mean AUROC (std)__| 0.82 (0.04) | __0.97 (0.01)__ |

<br/>

### <span style="background-color:#fff5b1"> [ì‹¤í—˜ 1] ê²°ê³¼ í•´ì„  
ìœ„ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆë“¯ì´, **<span style="color:maroon">Baggingì„ ì´ìš©í•œ ë°©ë²•ë¡ ì´ Accuracy ì¸¡ë©´ì—ì„œëŠ” 8%, AUROC ì¸¡ë©´ì—ì„œëŠ” 15%ë‚˜ ë” ìš°ìˆ˜í•˜ê²Œ ë‚˜ì˜¨ ê²ƒì„ í™•ì¸**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Classification Taskì—ì„œ Decision TreeëŠ” ì–´ë–»ê²Œë“  ë°ì´í„° ìƒ˜í”Œì„ í•˜ë‚˜ì˜ Classë¡œ í• ë‹¹í•˜ê¸° ìœ„í•´ ê³„ì†í•´ì„œ ë¶„ê¸°í•´ë‚˜ê°€ëŠ” ì„±ì§ˆì´ ìˆê³ , ë”°ë¼ì„œ ê³¼ì í•©ì˜ ìš°ë ¤ê°€ ìˆëŠ” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤. ë”°ë¼ì„œ **<span style="color:maroon">Baggingì„ ì‚¬ìš©í•˜ë©´ ë³´ë‹¤ ì¼ë°˜í™” ì„±ëŠ¥ì´ ì¢‹ì€ Decision boundaryë¥¼ ì–»ëŠ” ë™ì‹œì—, Varainceë¥¼ ë‚®ì¶° ì˜ˆì¸¡ ì˜¤ë¥˜ë¥¼ ê°ì†Œ**ì‹œí‚¬ ìˆ˜ë„ ìˆëŠ” ê²ƒì´ì£ .

---

<br/>

## <span style="background-color:#fff5b1"> [ì‹¤í—˜ 2] Change Base Learner in Bagging (k-NN with small k)
ì•ì„  ì‹¤í—˜ì„ í†µí•´ì„œ Ensemble Learningì˜ ìš°ìˆ˜ì„±ì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ì´ë•Œ Decision TreeëŠ” Baggingì˜ Base Learnerë¡œì„œ ì£¼ë¡œ í™œìš©ë˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤. ì´ëŠ” Varianceê°€ í¬ë„ë¡ êµ¬ì„±í•˜ê¸°ê°€ ì‰½ê³ , ì¼ë°˜ì ìœ¼ë¡œ Biasê°€ ë‚®ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. **<span style="color:maroon">kê°’ì´ ë‚®ì€ k-NN ì—­ì‹œ ë§ˆì°¬ê°€ì§€ë¡œ Varianceê°€ ë†’ê³  Biasê°€ ë‚®ì€ ëŒ€í‘œì ì¸ ì˜ˆ**ì…ë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ë§Œì¼ **<span style="color:maroon">Base Learnerë¥¼ ë°”ê¾¸ì—ˆì„ ë•Œë„ Baggingì˜ íš¨ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆì„ê¹Œìš”?** ì‹¤í—˜ì„ í†µí•´ì„œ í™•ì¸í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

```python
from sklearn.neighbors import KNeighborsClassifier
```
```python
# Single kNN with k=5


# Bagging with kNN with k=5
bag_clf_kNN = BaggingClassifier(base_estimator=KNeighborsClassifier())
```
```python
# Evaluate 
evaluate_model(model=bag_clf_kNN, X=X, y=y)
```
- `BaggingClassifier` Classì—ì„œ Base Learnerë¥¼ ë°”ê¾¸ë ¤ë©´, `base_estimator` ì¸ìë¥¼ `KNeighborsClassifier()`ë¡œ ë°”ê¾¸ì–´ì£¼ë©´ ë©ë‹ˆë‹¤. ì´ë•Œ **kê°’ì˜ defaultëŠ” 5ì´ë©°, ë³¸ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” k=3ì„ ì‚¬ìš©**í•˜ì˜€ìŠµë‹ˆë‹¤.

### Results
|      |__Decision Tree__|__Bagging with DT__| __kNN(k=3)__ | __Bagging with kNN(k=3)__ |
|------|:---------------:|:----------------:|:----------------:|:------------------:|
|__Mean Accuracy (std)__| 0.82 (0.04) | 0.90 (0.03) | __0.93 (0.03)__ | __0.93 (0.03)__ |
|__Mean AUROC (std)__| 0.82 (0.04) | 0.97 (0.01) | 0.96 (0.02) | __0.97 (0.01)__ |

<br/>

### <span style="background-color:#fff5b1"> [ì‹¤í—˜ 2] ê²°ê³¼ í•´ì„
kNNì˜ ê²½ìš°, AUROC Score ì¸¡ë©´ì—ì„œëŠ” Bagging ë°©ì‹ì´ 1% ê°€ëŸ‰ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ê²ƒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. **<span style="color:maroon"> ê·¸ë ‡ë‹¤ë©´ ë‹¤ë¥¸ kê°’ì— ë”°ë¼ì„œëŠ” ì„±ëŠ¥ì´ ë‹¬ë¼ì§ˆê¹Œìš”? 'kê°’ì´ ì‘ë‹¤'ëŠ” ê¸°ì¤€ì€ ë¬´ì—‡ì¼ê¹Œìš”?** ì´ëŠ” [ì‹¤í—˜ 3]ì—ì„œ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤.

<br/>

## <span style="background-color:#fff5b1"> [ì‹¤í—˜ 3] Bagging with kNN with k values from 1 to 20
```python
def get_models():
    base_learner, bagging = dict(), dict()
    # Evaluate k values from 1 to 20
    for i in range(1, 20+1):
        # Define Base learner
        base_learner[str(i)] = KNeighborsClassifier(n_neighbors=i)
        # Define Ensemble Model
        bagging[str(i)] = BaggingClassifier(base_estimator=base_learner[str(i)])
    
    return base_learner, bagging
```
```python
kNN, bag_kNN = get_models()
```
- ë¨¼ì € Single kNNê³¼ kNNì„ ê¸°ë³¸ ë² ì´ìŠ¤ë¡œ Bagging ë°©ì‹ì˜ Ensemble Learningì„ ìˆ˜í–‰í•˜ëŠ” ëª¨ë¸ì„ `get_models()`ë¥¼ í†µí•´ ì •ì˜í•©ë‹ˆë‹¤. ì´ë•Œ kê°’ì„ 1ë¶€í„° 20ê¹Œì§€ ë‹¤ë¥´ê²Œ í•˜ì—¬ ê° ëª¨ë¸ë“¤ì„ dict ì•ˆì— ë„£ì–´ ë¶ˆëŸ¬ì˜¬ ê²ƒì…ë‹ˆë‹¤.
```python
# Evaluate the models and store results

def print_results(models: dict, X, y):
    acc_score, auroc_score, k_list = [], [], []
    for k, model in models.items():
        # Evaluate Model
        acc, auroc = evaluate_model(model, X, y)
        # Store the Results
        acc_score.append(acc)
        auroc_score.append(auroc)
        k_list.append(k)
        
        # Print the performance along the way
        print('k=%s, Accuracy: %.3f (%.3f)' % (k, np.mean(acc), np.std(acc)))
        print('k=%s, AUROC: %.3f (%.3f)' % (k, np.mean(auroc), np.std(auroc)))
    
    return k_list, acc_score, auroc_score
```
```python
kNN_k_list, kNN_acc_list, kNN_auroc_list = print_results(kNN, X, y)
```
```
k=1, Accuracy: 0.915 (0.026)
k=1, AUROC: 0.915 (0.026)
k=2, Accuracy: 0.896 (0.029)
k=2, AUROC: 0.951 (0.021)
k=3, Accuracy: 0.931 (0.026)
k=3, AUROC: 0.962 (0.016)
...
k=20, Accuracy: 0.922 (0.025)
k=20, AUROC: 0.979 (0.010)
```
- ë¨¼ì € Single kNNì—ì„œ kì˜ ê°’ì´ 1ë¶€í„° 20ê¹Œì§€ ë³€í™”í•  ë•Œ, ê° ëª¨ë¸ì˜ Accuracyì™€ AUROCë¥¼ ì‚°ì¶œí•©ë‹ˆë‹¤.
```python
bag_kNN_k_list, bag_kNN_acc_list, bag_kNN_auroc_list = print_results(bag_kNN, X, y)
```
```
k=1, Accuracy: 0.915 (0.028)
k=1, AUROC: 0.960 (0.019)
k=2, Accuracy: 0.922 (0.025)
k=2, AUROC: 0.971 (0.013)
k=3, Accuracy: 0.927 (0.027)
k=3, AUROC: 0.975 (0.013)
...
k=20, Accuracy: 0.924 (0.026)
k=20, AUROC: 0.979 (0.011)
```
- ë‹¤ìŒìœ¼ë¡œëŠ” Bagging with kNNìœ¼ë¡œ k ê°’ì´ 1ë¶€í„° 20ê¹Œì§€ ë³€í™”í•  ë•Œ ê° Bagging ëª¨ë¸ì˜  Accuracyì™€ AUROCë¥¼ ì‚°ì¶œí•©ë‹ˆë‹¤.

<br/>

### Results
### <span style="color:maroon">1. Single kNN
```python
plt.figure(figsize=(10, 5))
plt.title("Single kNN num of neighbors vs. Classification accuracy")
plt.boxplot(x=kNN_acc_list, labels=kNN_k_list, showmeans=True);
```
<p align="center">
    <img src="Img/knn_k_acc.png" width="500"/>
</p>

```python
plt.figure(figsize=(10, 5))
plt.title("Single kNN num of neighbors vs. Classification AUROC")
plt.boxplot(x=kNN_auroc_list, labels=kNN_k_list, showmeans=True);
```
<p align="center">
    <img src="Img/knn_k_auroc.png" width="500"/>
</p>

### <span style="color:maroon"> 2. Bagging with kNN
```python
plt.figure(figsize=(10, 5))
plt.title("Bagging kNN num of neighbors vs. Classification accuracy")
plt.boxplot(x=bag_kNN_acc_list, labels=bag_kNN_k_list, showmeans=True);
```
<p align="center">
    <img src="Img/bag_knn_k.png" width="500"/>
</p>

```python
plt.figure(figsize=(10, 5))
plt.title("Bagging kNN num of neighbors vs. Classification AUROC")
plt.boxplot(x=bag_kNN_auroc_list, labels=bag_kNN_k_list, showmeans=True);
```
<p align="center">
    <img src="Img/bag_knn_k_auroc.png" width="500"/>
</p>

<br/>

### <span style="background-color:#fff5b1"> [ì‹¤í—˜ 3] ê²°ê³¼ í•´ì„
- **<span style="color:maroon">Accuracyë¥¼ ê¸°ì¤€ìœ¼ë¡œ, Baggingì„ ë³´ë©´, kê°’ì´ 7ë³´ë‹¤ ì‘ì„ ë•ŒëŠ” Accuracyê°€ ì¦ê°€í•˜ë‹¤ê°€, k ê°’ì´ ì»¤ì§ˆ ìˆ˜ë¡ ì˜¤íˆë ¤ ì„±ëŠ¥ì´ í•˜ë½í•˜ê±°ë‚˜ ë³€ë™ì„±ì´ í° ê²½í–¥**ì„ ì‹œê°ì ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.
    - **<span style="color:maroon">íŠ¹íˆ kê°€ 10ì„ ë„˜ì–´ê°€ë©´, ì˜¤íˆë ¤ Single kNNì—ì„œ ì¢€ ë” ì•ˆì •ëœ ì„±ëŠ¥ì„ í™•ì¸**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - ì´ëŠ” kê°’ì´ ì»¤ì§ˆ ìˆ˜ë¡ Varianceê°€ ì‘ì€ ëª¨ë¸ì´ ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
- **<span style="color:maroon">AUROC ê¸°ì¤€ìœ¼ë¡œëŠ” Single kNNì€ kê°€ 7ì´ ë„˜ì–´ê°€ë©° í° ê°’ì„ ê°€ì§ˆ ìˆ˜ë¡ ë§¤ìš° ë†’ì€ ì„±ëŠ¥ìœ¼ë¡œ ìˆ˜ë ´í•˜ëŠ” ëª¨ìŠµ**ì„ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤. 
    - ë°˜ë©´ **<span style="color:maroon">Baggingì˜ ê²½ìš° ì´ˆë°˜ì—ëŠ” ì„±ëŠ¥ì´ ì˜¤ë¥´ë‹¤ê°€, ì´í›„ì—ëŠ” ì„±ëŠ¥ì´ ì•½ê°„ ë–¨ì–´ì§€ê±°ë‚˜ ë“¤ì­‰ë‚ ì­‰í•œ ê²½í–¥**ì´ ìˆìŠµë‹ˆë‹¤.
- ë‹¤ì‹œ ë§í•´, **<span style="color:maroon">k ê°’ì´ ì»¤ì§€ë©´ Baggingì˜ íš¨ê³¼ê°€ í¬ê²Œ ë‚˜ì˜¤ì§€ ì•ŠëŠ” ê²ƒ**ì´ì£ . ì´ëŠ” íŠ¹íˆ AUROCì—ì„œ í™•ì‹¤í•œ ê²½í–¥ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- **<span style="color:maroon">ê²°ë¡ ì ìœ¼ë¡œ, ì´ ë°ì´í„°ì—ì„œëŠ” k=6~7 ì •ë„ë¡œ ì¡ì„ ë•Œ Baggingì˜ íš¨ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆë‹¤ê³  ê²°ë¡ ** ì§€ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<br/>

# <span style="color:darkblue">Bagging 2: <span style="color:green">Random Forest ğŸŒ²ğŸŒ³ğŸŒ´

<p align="center">
    <img src="Img/RF.png" width="500"/>
</p>

<p align="center">
    <em>Random Forest: General Framework </em>
</p>
<p align="center">
    <em> Image source: https://ai-pool.com/a/s/random-forests-understanding </em>
</p>

Bagging ë°©ë²•ë¡ ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ëŒ€í‘œì ì¸ ì•Œê³ ë¦¬ì¦˜ì€ Random Forestì…ë‹ˆë‹¤. Decision Treeë¥¼ ì—¬ëŸ¬ ê°œ ëª¨ì•„ ë†“ìœ¼ë©´ ìˆ²ì´ ë˜ëŠ”ë°, ì´ ìˆ²ì„ êµ¬ì„±í•˜ëŠ” ë°©ë²•ì„ Randomìœ¼ë¡œ í•œë‹¤ê³  í•˜ì—¬ "Random Forest"ë¡œ ë¶ˆë¦¬ëŠ” ëª¨ë¸ì´ì£ .   

ë³´ë‹¤ êµ¬ì²´ì ìœ¼ë¡œëŠ” **ì—¬ëŸ¬ ê°œì˜ Decision Treeë¥¼ ìƒì„±í•œ ë’¤, ê° ê°œë³„ Treeì˜ ì˜ˆì¸¡ ê°’ë“¤ ì¤‘ ê°€ì¥ ë§ì€ ì„ íƒì„ ë°›ì€ ë³€ìˆ˜ë“¤ë¡œ ì˜ˆì¸¡ì„ ì§„í–‰í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë™ì‘**í•©ë‹ˆë‹¤. Decision Treeì˜ ì¤‘ì‹¬ ê·¹í•œ ì •ë¦¬ ë²„ì „ì´ë¼ í•  ìˆ˜ ìˆì£ .   

ì´ëŸ¬í•œ ë°©ì‹ì˜ ì¥ì ì€, **ì˜ˆì¸¡ ê°’ì— ëŒ€í•œ Varianceê°€ ë†’ë‹¤ í•˜ë”ë¼ë„, ì´ë¥¼ í‰ê· ë‚´ì„œ ë¶„ì‚°ì„ ì¤„ì¼ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒ**ì…ë‹ˆë‹¤.   

ê·¸ë¦¬ê³  ê° Decision Treeë§ˆë‹¤ ë…ë¦½ë³€ìˆ˜ì˜ ì‚¬ìš© ê°œìˆ˜ë¥¼ ì œí•œí•˜ëŠ”ë°, ì´ë•Œ Bagging ê¸°ë²•ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ íŠ¹ì§•ì ì´ì£ . Random ForestëŠ” ê¸°ë³¸ì ìœ¼ë¡œëŠ” Baggingì˜ ë°©ì‹ì„ ë”°ë¥´ê¸° ë•Œë¬¸ì—, **<span style="color:green">ê° Decision Tree ë§ˆë‹¤ ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ì…‹(Bootstrap)ì€ ë‹¤ë¥´ì§€ë§Œ, Baggingì²˜ëŸ¼ ëª¨ë“  ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ëŒ€ì‹ ì— Tree ë³„ë¡œ í™œìš©í•˜ëŠ” ë…ë¦½ë³€ìˆ˜ë¥¼ ë‹¤ë¥´ê²Œ í•˜ëŠ” ê¸°ë²•**ì…ë‹ˆë‹¤.   

ì´ë•Œ í™œìš©í•˜ëŠ” ë…ë¦½ë³€ìˆ˜ì˜ ìˆ˜ëŠ” ì›ë˜ ë³€ìˆ˜ì˜ ìˆ˜ $D$ë³´ë‹¤ ì ì€ ìˆ˜ì˜ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ê³ , ë³´í†µ $\sqrt D$ê°œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

`Scikit-Learn`ì—ì„œëŠ” Classification Taskì— ìˆì–´ Random Forestë¥¼ ê°„í¸í•˜ê²Œ ì´ìš©í•  ìˆ˜ ìˆë„ë¡, `RandomForestClassifier`ë¥¼ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤.
```python
from sklearn.ensemble import RandomForestClassifier
```

<br/>

## <span style="background-color:#fff5b1"> [ì‹¤í—˜ 3] Bagging with Decision Tree vs. RandomForest
ê·¸ë ‡ë‹¤ë©´ ì—¬ê¸°ì„œ ì§ˆë¬¸ì´ í•˜ë‚˜ ìƒê¹ë‹ˆë‹¤. **<span style="color:darkblue">Decision Treeë¥¼ Base Learnerë¡œ ì‚¬ìš©í•˜ëŠ” Bagging ë°©ì‹</span>ê³¼ <span style="color:green">Random Forest</span> ê°„ì˜ ì„±ëŠ¥ ì°¨ì´ëŠ” ì–¼ë§ˆë‚˜ ë‚ ê¹Œìš”?**   

ëª¨ë“  ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ ë” ì •ë³´ëŸ‰ì´ ë§ìœ¼ë‹ˆ, Baggingì´ ë” ë†’ì€ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆì„ê¹Œìš”? ì•„ë‹ˆë©´ ë‹¤ì–‘í•œ ì…ë ¥ ë³€ìˆ˜ ì¡°í•©ì— ëŒ€í•œ ì„±ëŠ¥ì„ í™•ì¸í•  ìˆ˜ ìˆìœ¼ë‹ˆ, Random Forestì˜ ì„±ëŠ¥ì´ ë” ì¢‹ê²Œ ë‚˜ì˜¬ê¹Œìš”? ì´ë¥¼ ì‹¤í—˜ì ìœ¼ë¡œ í™•ì¸í•´ ë³´ê² ìŠµë‹ˆë‹¤.
```python
# Random Forest
rf_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=15, n_jobs=-1, random_state=2022)

# Bagging with Decision Tree
bag_clf_rf = BaggingClassifier(DecisionTreeClassifier(splitter="random", max_leaf_nodes=15, 
            random_state=2022), n_estimators=500, bootstrap=True, n_jobs=-1, random_state=2022)
```

- Baggingê³¼ RandomForest ëª¨ë‘ Treeì˜ ê°œìˆ˜ëŠ” 500ê°œ, max_leaf_nodes 15ê°œë¡œ ë™ì¼í•œ ì¡°ê±´ì„ ì£¼ì—ˆìŠµë‹ˆë‹¤. ë‹¤ë§Œ ë‹¤ë¥¸ ê²ƒì€ ì‚¬ìš©í•˜ëŠ” ì…ë ¥ ë³€ìˆ˜ì˜ ê°œìˆ˜ê°€ ë‹¤ë¥´ê² ì£ .   

```python
# Evaluate Random Forest
acc_RF, acc_RF = evaluate_model(model=rf_clf, X=X, y=y)

# Evaluate Bagging with Decision Tree
acc_bag_DT, auroc_bag_DT = evaluate_model(model=bag_clf_rf, X=X, y=y)
```

### Results
- Random Forest: 0.88(0.03), 0.96(0.02)
- Bagging: 0.87(0.03), 0.95(0.02)

### <span style="background-color:#fff5b1"> [ì‹¤í—˜ 3] ê²°ê³¼ í•´ì„
- Decision Treeë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” Baggingë³´ë‹¤ëŠ”, **<span style="color:maroon">Random Forestì˜ ì„±ëŠ¥ì´ ë” ìš°ìˆ˜í•œ ê²ƒì„ ì‹¤í—˜ì ìœ¼ë¡œ í™•ì¸** í•˜ì˜€ìŠµë‹ˆë‹¤. ë‘˜ì˜ ì°¨ì´ëŠ” Bootstrap ë§ˆë‹¤ í™œìš©í•˜ëŠ” ì…ë ¥ ë³€ìˆ˜ì˜ ê°œìˆ˜ì˜€ëŠ”ë°, **<span style="color:maroon">Random ForestëŠ” ì„œë¡œ ë‹¤ë¥¸ ì…ë ¥ ë³€ìˆ˜ ì¡°í•©ì„ í™œìš©í•œë‹¤ëŠ” íŠ¹ì§•**ì„ ê°€ì§€ê³  ìˆì—ˆì£ .
- ì´ë¡œì¨ ì•Œ ìˆ˜ ìˆëŠ” ê²ƒì€, Ensembel Learningì— ìˆì–´ **<span style="color:maroon">ê°œë³„ ëª¨ë¸ì˜ ë‹¤ì–‘ì„± í™•ë³´**ì…ë‹ˆë‹¤. Ensemble Learnigì— ìˆì–´ ê°€ì¥ ì¤‘ìš”í•œ í•µì‹¬ ì•„ì´ë””ì–´ëŠ”, ê°œë³„ ëª¨ë¸ì˜ "ë‹¤ì–‘ì„±"ì„ ì–´ë–»ê²Œ í™•ë³´í•  ê²ƒì¸ê°€?ì— ê¸°ë°˜í•˜ê¸° ë•Œë¬¸ì´ì£ . ë™ì¼í•œ ëª¨ë¸ì„ ì—¬ëŸ¬ ê°œ ì·¨í•©í•´ë´¤ì í° ì„±ëŠ¥ í–¥ìƒì´ ì—†ì„ í…Œë‹ˆê¹Œìš”. ì—¬ê¸°ì„œ **<span style="color:maroon">"ë‹¤ì–‘í•œ ëª¨ë¸"ì´ë¼ í•¨ì€, ì•Œê³ ë¦¬ì¦˜ì´ ë‹¤ë¥´ê±°ë‚˜, ë™ì¼ ì•Œê³ ë¦¬ì¦˜ì´ì–´ë„ ë°ì´í„°ì˜ êµ¬ì„±ì´ë‚˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ë‹¬ë¦¬ í•˜ì—¬ í•™ìŠµí•œ ìƒí™©ì„ ì˜ë¯¸**í•©ë‹ˆë‹¤. (ë°ì´í„° ë‚´ì˜ ë…¸ì´ì¦ˆê°€ ì„œë¡œ ë‹¤ë¥´ê³ , í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ ì•Œê³ ë¦¬ì¦˜ì˜ êµ¬ì¡°ì— ì˜í–¥ì„ ì£¼ê¸° ë•Œë¬¸ì´ì£ .) ì´ëŸ¬í•œ ìƒí™©ì—ì„œ ê°œë³„ ëª¨ë¸ì€ ì„œë¡œ ì ì ˆí•˜ê²Œ ë‹¬ë¼ì•¼ ì•™ìƒë¸”ì˜ íš¨ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ë”°ë¼ì„œ Ensemble Learningì˜ íš¨ê³¼ë¥¼ ë³´ê¸° ìœ„í•´ì„œëŠ” **<span style="color:maroon">ê°œë³„ì ìœ¼ë¡œëŠ” ì–´ëŠ ì •ë„ì˜ ì¢‹ì€ ì„±ëŠ¥(Random Modelë³´ë‹¤ëŠ” ì¢‹ì€ ì„±ëŠ¥)ì„ ê°€ì§€ê³ , ì•™ìƒë¸” ë‚´ì—ì„œ ê°ê°ì˜ ëª¨ë¸ì´ ì„œë¡œ ë‹¤ì–‘í•œ í˜•íƒœë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê²ƒì´ ê°€ì¥ ì´ìƒì **ì…ë‹ˆë‹¤.
- ê·¸ëŸ°ë° **<span style="color:maroon">Baggingì€ ë°ì´í„°ëŠ” ë‹¤ë¥´ì§€ë§Œ ì…ë ¥ ë³€ìˆ˜ê°€ ëª¨ë‘ ë™ì¼í•œ ë°˜ë©´, Random ForestëŠ” ì…ë ¥ ë³€ìˆ˜ê°€ ëª¨ë¸ ë³„ë¡œ ëª¨ë‘ ë‹¬ë¼ ë‹¤ì–‘ì„± ì¸¡ë©´ì—ì„œ ìš°ìˆ˜í•˜ë‹¤ê³  í•  ìˆ˜ ìˆì£ . ì´ê²ƒì´ ë°”ë¡œ Random Forestê°€ ë‹¨ìˆœ Decision Treeì˜ Baggingë³´ë‹¤ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ëŠ” ì´ìœ **ë¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<br/>

## <span style="background-color:#fff5b1"> [ì‹¤í—˜ 4] Random Forest ì‚¬ìš© ë³€ìˆ˜ ê°œìˆ˜ ë³„ ì„±ëŠ¥ ì°¨ì´ê°€ ìˆì„ê¹Œ?
ê·¸ëŸ¬ë‚˜ ìœ„ ì‹¤í—˜ì—ì„œ Baggingê³¼ Random Forest ê°„ ì„±ëŠ¥ ì°¨ì´ê°€ í¬ì§€ëŠ” ì•Šì•˜ìŠµë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ì´ëŸ° ì§ˆë¬¸ì´ ìƒê¸¸ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. **ì •ë§ Random Forestì˜ ì‚¬ìš© ë³€ìˆ˜ ê°œìˆ˜ëŠ” $\sqrt D$ê°œì´ë©´ ë ê¹Œìš”?** ì´ê²ƒì´ ìµœì ì˜ ì„±ëŠ¥ì„ ë‚´ëŠ” ê²ƒì¼ê¹Œìš”?   
ì´ëŸ° ë¬¼ìŒì„ ê°€ì§€ê³  [ì‹¤í—˜ 4]ì—ì„œëŠ” Random Forestì—ì„œ Bootstrapë§ˆë‹¤ ì‚¬ìš©ë˜ëŠ” ì…ë ¥ë³€ìˆ˜ì˜ ìˆ˜ë¥¼ 1ë¶€í„° ì…ë ¥ ë³€ìˆ˜ì˜ ê°œìˆ˜ë§Œí¼ ëŠ˜ë ¤ê°€ë©° ì„±ëŠ¥ì„ ë¹„êµí•´ë³´ì•˜ìŠµë‹ˆë‹¤.

```python
def get_RF():
    random_forest = dict()
    # Evaluate 'max_features' = from 1 to 20
    for i in range(1, 20+1):
        # Define Random Forest
        random_forest[str(i)] = RandomForestClassifier(n_estimators=500, n_jobs=-1, max_features=i, random_state=2022)
    return random_forest
```
```python
rf_clf_dict = get_RF()
```
```python
def print_results_per_max_features(models:dict, X, y):
    acc_score, auroc_score, n_feature_list = [], [], []
    for k, model in models.items():
        # Evaluate Random Forest
        acc, auroc = evaluate_model(model, X, y)
        # Store the Results
        acc_score.append(acc)
        auroc_score.append(auroc)
        n_feature_list.append(k)
        
        # Print the performance along the way
        print('n_feature=%s, Accuracy: %.3f (%.3f)' % (k, np.mean(acc), np.std(acc)))
        print('n_feature=%s, AUROC: %.3f (%.3f)' % (k, np.mean(auroc), np.std(auroc)))
        
    return acc_score, auroc_score, n_feature_list
```
```python
rf_acc_list, rf_auroc_list, rf_n_feature_list = print_results_per_max_features(rf_clf_dict, X, y)
```
```
Accuracy: 0.93 (0.03)
AUROC: 0.98 (0.01)
n_feature=1, Accuracy: 0.932 (0.027)
n_feature=1, AUROC: 0.983 (0.008)
Accuracy: 0.94 (0.02)
AUROC: 0.98 (0.01)
n_feature=2, Accuracy: 0.936 (0.023)
n_feature=2, AUROC: 0.984 (0.008)
...
Accuracy: 0.90 (0.03)
AUROC: 0.97 (0.01)
n_feature=20, Accuracy: 0.905 (0.032)
n_feature=20, AUROC: 0.970 (0.013)
```
```python
Best_Acc_idx = np.where([np.mean(acc) for acc in rf_acc_list] == np.max([np.mean(acc) for acc in rf_acc_list]))[0][0]+1
print('Best "max_features" is', Best_Acc_idx,
      'in terms of Accuracy:', '%.3f' % (np.mean(rf_acc_list[Best_Acc_idx-1])))
```
```
Best "max_features" is 2 in terms of Accuracy: 0.936
```
```python
Best_AUROC_idx = np.where([np.mean(auroc) for auroc in rf_auroc_list] == np.max([np.mean(auroc) for auroc in rf_auroc_list]))[0][0]+1
print('Best "max_features" is', Best_AUROC_idx,
      'in terms of AUROC:', '%.3f' % (np.mean(rf_auroc_list[Best_AUROC_idx-1])))
```
```
Best "max_features" is 3 in terms of AUROC: 0.985
```

### Result
```python
plt.figure(figsize=(10, 5))
plt.title("Random Forest num of features vs. Classification accuracy")
plt.boxplot(x=rf_acc_list, labels=rf_n_feature_list, showmeans=True);
```
<p align="center">
    <img src="Img/RF_acc_features.png" width="500"/>
</p>

```python
plt.figure(figsize=(10, 5))
plt.title("Random Forest num of features vs. Classification AUROC")
plt.boxplot(x=rf_auroc_list, labels=rf_n_feature_list, showmeans=True);
```

<p align="center">
    <img src="Img/RF_auroc_features.png" width="500"/>
</p>

### <span style="background-color:#fff5b1"> [ì‹¤í—˜ 4] ê²°ê³¼ í•´ì„
- ì´ 20ê°œì˜ ë³€ìˆ˜ ì¤‘, Random Forest ë‚´ Subsetì—ì„œ ëª‡ ê°œì˜ ë³€ìˆ˜ë¥¼ ì…ë ¥ ë°›ì„ ê²ƒì¸ì§€(`max_features`)ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™”ë¥¼ Boxplotìœ¼ë¡œ í™•ì¸í•´ ë³´ì•˜ìŠµë‹ˆë‹¤.
- ì „ì²´ì ì¸ ê²½í–¥ì„ í™•ì¸í•´ë³´ë‹ˆ, **<span style="color:maroon">ì‚¬ìš©í•˜ëŠ” ë³€ìˆ˜ì˜ ìˆ˜ê°€ ë§ì•„ì§ˆ ìˆ˜ë¡ ì˜¤íˆë ¤ ì„±ëŠ¥ì´ í•˜ë½í•˜ëŠ” ì¼ë°˜ì ì¸ ê²½í–¥ì„ Accuracyì™€ AUROC ê´€ì ì—ì„œ ëª¨ë‘ í™•ì¸**í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.
- íŠ¹íˆ Acuuracy ê¸°ì¤€ì—ì„œëŠ” ì…ë ¥ ë³€ìˆ˜ë¥¼ 2ê°œë§Œ ì‚¬ìš©í•  ë•Œ, ê·¸ë¦¬ê³  AUROC ê¸°ì¤€ì—ì„œëŠ” ì…ë ¥ ë³€ìˆ˜ë¥¼ ì˜¤ì§ 3ê°œë§Œ ì‚¬ìš©í•  ë•Œ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ëª¨ë‘ $\sqrt D$ ($D=20 in this case$) ë³´ë‹¤ëŠ” ì‘ì€ ê°’ì…ë‹ˆë‹¤.   
- ì´ë ‡ê²Œ **<span style="color:maroon">ì ì€ ë³€ìˆ˜ë§Œì„ í™œìš©í•´ë„ ì¢‹ì€ ì„±ëŠ¥ì´ ë‚˜ì˜¤ëŠ” ì´ìœ ëŠ”, [Dimensionality Reduction Tutorial](https://github.com/Im-JihyunKim/BusinessAnalytics_Topic1)ë•Œ ë‹¤ë£¨ì—ˆë“¯ì´, í™œìš©í•˜ëŠ” ë³€ìˆ˜ ì°¨ì›ì´ ëŠ˜ì–´ë‚  ìˆ˜ë¡ ì°¨ì›ì˜ ì €ì£¼ì— ë¹ ì ¸ë“¤ê¸° ì‰½ê³ , ë˜í•œ ê°œë³„ ëª¨ë¸ì˜ ë‹¤ì–‘ì„±ì„ í™•ë³´í•˜ê¸° ìš©ì´** ë‹¤ëŠ” ì¥ì  ë•Œë¬¸ì…ë‹ˆë‹¤.

<br/>

-------

<br/>

# <span style="color:purple">Boosting 1: <span style="color:darkviolet">Gradient Boosting
<p align="center">
    <img src="Img/gradientboosting.png" width="500"/>
</p>

<p align="center">
    <em>Gradient Boosting: General Framework   </em>
</p>
<p align="center">
    <em> Image source: https://www.geeksforgeeks.org/ml-gradient-boosting/ </em>
</p>

Gradient Boostingì€ ëŒ€í‘œì ì¸ Boosting ê³„ì—´ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œì„œ, XGBoost, LightGBM, CatBoostì˜ ê·¼ê°„ì´ ë˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤. **<span style="color:darkviolet">í•™ìŠµì˜ ì „ ë‹¨ê³„ì—ì„œ ëª¨ë¸ ë³„ ì”ì—¬ ì˜¤ì°¨(residual error)ë¥¼ ê³„ì‚°í•˜ê³ , ì´ ì´ ì˜¤ì°¨ë¥¼ ë¯¸ë¶„í•œ gradientë¥¼ í†µí•´ ëª¨ë¸ì„ ë³´ì™„í•˜ëŠ” ë°©ì‹**ì„ ì·¨í•©ë‹ˆë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— "Gradient" Boostingì´ë¼ëŠ” ì´ë¦„ì´ ë¶™ì—ˆìŠµë‹ˆë‹¤.   

Gradient Boostingì—ì„œ ì‚¬ìš©ë˜ëŠ” ê°€ì¥ í•µì‹¬ì ì¸ ë°©ë²•ì€ **<span style="color:darkviolet">Gradient Descent, ì¦‰ ê²½ì‚¬ í•˜ê°•ë²•**ì…ë‹ˆë‹¤. Gradient DescentëŠ” **<span style="color:darkviolet">Loss functionì„ ì •ì˜í•˜ê³ , ì´ ë¯¸ë¶„ê°’ì´ ìµœì†Œí™”ë˜ëŠ” ë°©í–¥ì„ ì°¾ì•„ë‚˜ê°€ëŠ” ë°©ì‹**ì…ë‹ˆë‹¤. ë§Œì¼ Loss Functionì„ Squared Errorë¡œ ì •ì˜í•œë‹¤ë©´, ì•„ë˜ì™€ ê°™ì€ ì‹ìœ¼ë¡œ Lossì™€ Lossì˜ ë¯¸ë¶„ ê°’ì„ í‘œí˜„í•  ìˆ˜ ìˆê² ì£ .   

$$L(y_i, F(x_i)) = \frac{1}{2}(y_i - F(x_i))^2$$
$$\frac{\partial L(y_i, F(x_i))}{\partial F(x_i)} = y_i - F(x_i)$$

ì´ë•Œ í¸ë¯¸ë¶„ì„ í†µí•´ ì–»ì€ gradientê°€ ê²°êµ­ $f(x)$ê°€ Lossë¥¼ ì¤„ì´ê¸° ìœ„í•´ ê°€ì•¼í•˜ëŠ” ë°©í–¥ì¸ë°, ì´ê²ƒì´ ê²°êµ­ ì”ì—¬ ì˜¤ì°¨(residual error)ì™€ ê°™ìŠµë‹ˆë‹¤. ì´ë•Œ íšŒê·€ ëª¨í˜•ì˜ ì”ì°¨ëŠ” Squared Loss Functionì˜ Negative gradient $y_i-F(x_i)=-\frac{\partial L(y_i, F(x_i))}{\partial F(x_i)}$ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.   

ê·¸ë ‡ë‹¤ë©´ Gradient Boostingì—ì„œ ì‚¬ìš©ë˜ëŠ” ìˆ˜ì‹ì€ ë¬´ì—‡ì¼ê¹Œìš”?   

$$train \ set: \begin{Bmatrix}
(x_i, y_i)
\end{Bmatrix}^N_{i=1} \quad loss \ funtion: L(y, F(x))$$
$$F_0(x) = arg \ \underset{\gamma }{min}\sum_{i=1}^{N}L(y_i, \gamma )$$

nê°œì˜ í•™ìŠµ ë°ì´í„°ê°€ ìˆì„ ë•Œ, Gradient Boostingì—ì„œëŠ” ì´ˆê¸° ê°’ìœ¼ë¡œ ìƒìˆ˜ í•¨ìˆ˜ë¥¼ í™œìš©í•©ë‹ˆë‹¤.  ê·¸ë¦¬ê³  ì•„ë˜ì™€ ê°™ì´ pseudo-residual, ì¦‰ gradientë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ì´ë¥¼ pythonìœ¼ë¡œ êµ¬í˜„í•˜ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.   

ê°„ë‹¨í•œ ì˜ˆì‹œë¥¼ ìœ„í•´ MSE Lossë¥¼ ìµœì†Œí™” í•˜ëŠ” Regression Taskë¥¼ í‘¼ë‹¤ê³  ê°€ì •í•˜ê² ìŠµë‹ˆë‹¤.
```python
from sklearn.datasets import make_regression
from sklearn.tree import DecisionTreeRegressor
```
```python
# Define dataset
X, y = make_regression(n_samples=100, n_features=1, random_state=2022)
```
ì„ì˜ì˜ Regressionì„ ìœ„í•œ ê°€ìƒ ë°ì´í„°ì…‹ì„ ë§Œë“¤ê³ , ì´ë¥¼ Single Decision Treeì— í•™ìŠµì‹œí‚µë‹ˆë‹¤. max_depthë¥¼ 2ë¡œ ë‘ì–´ Biasê°€ ë†’ì€ Weak Learnerë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

```python
F0 = np.mean(y)
print(F0)
```
```
-0.2068375978691931
```
ì²« ë²ˆì§¸ UpdateëŠ” ë¨¼ì € $y$ ê°’ì˜ í‰ê· ìœ¼ë¡œ ëª¨í˜•ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
$$F_0(x) = -0.207$$

```python
# 1st residual error
r1 = y - F0

# First Single Decision Tree with 'max_depth' = 2
tree_1 = DecisionTreeRegressor(max_depth=2, random_state=2022, criterion='mse')
r1_fit = tree_1.fit(X, r1)
gamma1 = r1_fit.predict(X)
```
ì´í›„ ì²« ë²ˆì§¸ residual error $r_1 = y-F_0(x)$ë¥¼ êµ¬í•˜ê³ , ì´ ì”ì°¨ì— max_depth=2ì¸ Single Decision Treeë¥¼ í•™ìŠµì‹œì¼œ ì”ì°¨ì˜ ì˜ˆì¸¡ ê°’ì¸ $\gamma$ë¥¼ êµ¬í•©ë‹ˆë‹¤.
```python
print(f"Prediction of residual: {np.unique(gamma1)}")
```
```
Prediction of residual: [-42.66613158 -12.51579569  10.53423901  30.43568584]
```
Residualì´ ì¼ì¢…ì˜ pseudo target valueê°€ ë˜ëŠ” ê²ƒì´ì£ .
```python
lr = 0.1
F1 = F0 + lr * gamma1 
```
ì´ì œ ì˜ˆì¸¡ ê°’ì„ ì—…ë°ì´íŠ¸ í•´ì¤ë‹ˆë‹¤. ì—¬ê¸°ì„œ `lr`ì€ learning rate, ì¦‰ gradient ê³„ì‚°ì— ìˆì–´ í•™ìŠµë¥ ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. `gamma1`ì€ ì²« ë²ˆì§¸ ì”ì°¨ë¥¼ ì˜ˆì¸¡í•œ ê°’, `F1`ì€ ìƒˆë¡œ ì˜ˆì¸¡ëœ $y$ ê°’ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
```python
print(f"1st Prediction: {F1}")
```
```
1st Prediction: [-1.45841717 -4.47345076 -4.47345076  0.8465863  -1.45841717  0.8465863
  2.83673099  2.83673099 -1.45841717 -1.45841717  0.8465863   0.8465863
 -1.45841717 -1.45841717  0.8465863   0.8465863   2.83673099  0.8465863
 -1.45841717  2.83673099  2.83673099  2.83673099  0.8465863   0.8465863
  2.83673099 -1.45841717  0.8465863  -4.47345076  2.83673099 -4.47345076
...
 -1.45841717 -1.45841717  0.8465863   0.8465863   2.83673099 -4.47345076
 -4.47345076  2.83673099  0.8465863   0.8465863  -1.45841717 -1.45841717
 -1.45841717  2.83673099  2.83673099  2.83673099]
```
ì˜ˆì¸¡ ê°’ì„ ë³´ë©´ ê±°ì˜ ë™ì¼í•œ ê°’ìœ¼ë¡œ ì˜ˆì¸¡ì„ í•˜ë©°, ê·¸ ì„±ëŠ¥ì´ í˜•í¸ ì—†ìŒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Single Weak Learnerë¡œëŠ” í•œ ë²ˆë§Œìœ¼ë¡œëŠ” ì¢‹ì€ ì˜ˆì¸¡ ì„œëŠ¥ì„ ë‚´ì§€ ëª»í•˜ëŠ” ê²ƒì´ì£ . í•˜ì§€ë§Œ $T$ë²ˆ ë§Œí¼ í•™ìŠµì„ ë°˜ë³µí•œë‹¤ë©´ ì–´ë–»ê²Œ ë ê¹Œìš”? ìœ„ ê³¼ì •ì„ $T$ë²ˆ ë°˜ë³µí•  ìˆ˜ ìˆëŠ” í•¨ìˆ˜ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.
```python
def GBM_Regression(X, y, T: int, lr):
    F_t = np.mean(y)
    tree = DecisionTreeRegressor(max_depth=2, random_state=2022)  # Weak Learner
    
    for t in range(1, T+1): # t=100
        print("Current update: ", str(t))
        F_before_t = F_t
        residual = y - F_before_t
        residual_fit = tree.fit(X, residual)
        gamma = residual_fit.predict(X)
        print(f"Prediction of Residual: {np.round(np.unique(gamma), 2)}")
        
        F_t = F_before_t + lr * gamma
        
        print(f"Prediction: {np.round(F_t, 2)}")
```
```python
GBM_Regression(X, y, T=100, lr=1e-2)
```
```
Current update:  1
Prediction of Residual: [-42.67 -12.52  10.53  30.44]
Prediction: [-0.33 -0.63 -0.63 -0.1  -0.33 -0.1   0.1   0.1  -0.33 -0.33 -0.1  -0.1
 -0.33 -0.33 -0.1  -0.1   0.1  -0.1  -0.33  0.1   0.1   0.1  -0.1  -0.1
  0.1  -0.33 -0.1  -0.63  0.1  -0.63 -0.33 -0.1  -0.33 -0.33 -0.33  0.1
 -0.1  -0.33 -0.63 -0.1  -0.33 -0.33 -0.33 -0.1  -0.1  -0.33 -0.33 -0.1
 -0.1   0.1  -0.63 -0.1  -0.33  0.1  -0.33 -0.1   0.1  -0.1  -0.63 -0.33
 -0.33 -0.33 -0.63 -0.63  0.1  -0.33 -0.1  -0.33 -0.1   0.1  -0.33  0.1
 -0.1  -0.1  -0.33 -0.33  0.1  -0.33 -0.63 -0.33 -0.33  0.1  -0.33 -0.1
 -0.33 -0.33 -0.1  -0.1   0.1  -0.63 -0.63  0.1  -0.1  -0.1  -0.33 -0.33
 -0.33  0.1   0.1   0.1 ]
Current update:  2
Prediction of Residual: [-43.55 -13.47   8.68  28.84]
Prediction: [-0.47 -1.07 -1.07 -0.01 -0.47 -0.01  0.39  0.39 -0.47 -0.47 -0.01 -0.01
 -0.47 -0.47 -0.01 -0.01  0.39 -0.01 -0.47  0.39  0.39  0.39 -0.01 -0.01
  0.39 -0.47 -0.01 -1.07  0.39 -1.07 -0.47 -0.01 -0.47 -0.47 -0.47  0.39
 -0.01 -0.47 -1.07  0.19 -0.47 -0.47 -0.25 -0.01 -0.01 -0.47 -0.47 -0.01
 -0.01  0.39 -0.77  0.19 -0.47  0.39 -0.47 -0.01  0.39 -0.01 -1.07 -0.47
 -0.47 -0.47 -1.07 -1.07  0.39 -0.47 -0.01 -0.47 -0.01  0.39 -0.47  0.39
 -0.01 -0.01 -0.47 -0.47  0.39 -0.47 -1.07 -0.47 -0.25  0.39 -0.47 -0.01
 -0.47 -0.47 -0.01 -0.01  0.39 -1.07 -1.07  0.39  0.19 -0.01 -0.47 -0.47
 -0.47  0.39  0.39  0.39]

 ...

Current update:  100
Prediction of Residual: [-18.42  -4.15   6.89  25.6 ]
Prediction: [-2.060e+00 -2.353e+01 -2.353e+01  6.080e+00 -1.040e+01  8.420e+00
  2.618e+01  1.555e+01 -9.810e+00 -1.170e+01 -2.000e-02  6.080e+00
 -7.800e+00 -1.040e+01  3.670e+00  6.080e+00  1.526e+01  4.160e+00
 -1.442e+01  1.479e+01  2.618e+01  1.622e+01  6.080e+00  1.029e+01
  1.622e+01 -1.442e+01  7.360e+00 -2.353e+01  1.555e+01 -2.336e+01
 -7.020e+00  2.880e+00 -1.040e+01 -8.390e+00 -1.544e+01  1.375e+01
  6.080e+00 -1.930e+00 -2.452e+01  1.244e+01 -2.060e+00 -1.626e+01
 -6.500e-01  3.700e-01  9.830e+00 -4.140e+00 -1.301e+01  1.960e+00
  6.080e+00  1.622e+01 -1.740e+01  1.244e+01 -3.790e+00  1.479e+01
 -4.840e+00  6.080e+00  1.526e+01  9.830e+00 -3.320e+01 -3.790e+00
 -1.205e+01 -6.080e+00 -2.883e+01 -2.374e+01  1.555e+01 -5.360e+00
  8.580e+00 -2.480e+00  9.370e+00  1.526e+01 -2.060e+00  2.618e+01
  6.080e+00  8.580e+00 -2.200e+00 -1.442e+01  1.622e+01 -1.040e+01
 -3.320e+01 -1.110e+01 -8.300e-01  1.555e+01 -1.780e+00  6.080e+00
 -1.170e+01 -1.442e+01  9.830e+00  6.080e+00  1.685e+01 -2.353e+01
 -2.452e+01  2.618e+01  1.244e+01  6.080e+00 -8.840e+00 -1.040e+01
 -1.222e+01  1.339e+01  1.555e+01  1.526e+01]
```
ì´ˆë°˜ì˜ Update ê°’ì€ ì”ì°¨ë„ ë§¤ìš° í¬ê³ , ì˜ˆì¸¡ ê°’ë„ ì¼ì •í•œ ìˆ˜ì¤€ìœ¼ë¡œ ë°€ì–´ë²„ë¦¬ì§€ë§Œ, 100ë²ˆ ê°€ëŸ‰ì˜ Update í›„ì—ëŠ” ì”ì°¨ ê°’ì´ ìƒëŒ€ì ìœ¼ë¡œ ë‚®ì•„ì¡Œìœ¼ë©°, data point í•˜ë‚˜í•˜ë‚˜ì— ëŒ€í•œ ì‹¤ì œ prediction ê°’ì„ ë‚´ë±‰ëŠ” ê²½í–¥ì´ ê°•í•´ì§‘ë‹ˆë‹¤.

$$g_{im} = \begin{bmatrix}
\frac {\partial L(y_i, f(x_i))}{\partial f(x_i)}
\end{bmatrix}_{f(x_i) = f_{m-1}(x_i)}$$
<br/>

ìœ„ì—ì„œ ë³¸ ê³¼ì •ê³¼ ê°™ì´, **<span style="color:darkviolet">Gradient Boostingì€ í•™ìŠµ ë°ì´í„°ì— y ëŒ€ì‹  gradientë¥¼ ì ìš©í•˜ê³ (targetì— pseudo residualì„ ì ìš©í•˜ëŠ” ê²ƒì´ì£ ), Loss functionì— ë„£ìœ¼ë©´ì„œ ê³„ì†í•´ì„œ ì”ì°¨ë¥¼ ì¤„ì´ëŠ” ë°©ì‹**ì„ íƒí•©ë‹ˆë‹¤.   

<br/>

$$h_t(x): base \ model(tree) \quad train \ set: \begin{Bmatrix}
(x_i, g_{im}) \\
\end{Bmatrix}^N_{i=1} \\ 
F_t(x) = F_{t-1}(x) + \alpha h_t(x)$$

ë”°ë¼ì„œ **<span style="color:darkviolet">ì²˜ìŒ $F_0(x)$ëŠ” ìƒìˆ˜í•¨ìˆ˜ì˜€ì§€ë§Œ (In this case, $F_0 = mean\ of\ y\ values$), í˜„ì¬ ì‹œì  tì— ëŒ€í•œ ëª¨ë¸ $h_t(x)$ê°€ ë“¤ì–´ê°€ë©° gradientë¥¼ ê³ ë ¤í•œ í•™ìŠµì´ ê°€ëŠ¥**ì¼€ ë˜ì—ˆì£ . ì°¸ê³ ë¡œ $\alpha$ëŠ” Learning rate (lr) ì…ë‹ˆë‹¤. ì˜ˆì œì²˜ëŸ¼ ì§ì ‘ ë„£ê±°ë‚˜ ìµœì í™” ì‹ì„ ë„£ì–´ ì‚¬ìš©í•˜ê¸°ë„ í•©ë‹ˆë‹¤.   

ì´ ê³¼ì •ì„ pythonì„ í†µí•´ ì‹œê°í™”í•˜ìë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.
```python
# First Update
F0 = np.mean(y)
r1 = y - F0

# Second Update
tree_1 = DecisionTreeRegressor(max_depth=2, random_state=2022, criterion='mse')
tree_1.fit(X, r1)

# Third Update
y2 = y - tree_1.predict(X)  # residual errors

tree_2 = DecisionTreeRegressor(max_depth=2, random_state=2022, criterion='mse')
tree_2.fit(X, y2)

# Final Update
y3 = y2 - tree_2.predict(X)

tree_3 = DecisionTreeRegressor(max_depth=2, random_state=2022)
tree_3.fit(X, y3)

# Prediction
y_pred = sum(tree.predict(X_test) for tree in (tree_1, tree_2, tree_3))
```
```python
def plot_predictions(regressors, X, y, axes, label=None, style="r-", data_style="b.", data_label=None):
    x1 = np.linspace(axes[0], axes[1], 500)
    y_pred = sum(regressor.predict(x1.reshape(-1, 1)) for regressor in regressors)
    plt.plot(X[:, 0], y, data_style, label=data_label)
    plt.plot(x1, y_pred, style, linewidth=2, label=label)
    if label or data_label:
        plt.legend(loc="upper center", fontsize=16)
    plt.axis(axes)
```
```python
plt.figure(figsize=(11,11))

axes = [X.min()-1, X.max()+1, y.min()-1, y.max()+1]

plt.subplot(321)
plot_predictions([tree_1], X, y, axes=axes, label="$h_1(x_1)$", style="g-", data_label="Train Data")
plt.ylabel("$y$", fontsize=12, rotation=0)
plt.title("Residual Error & Prediction of Single DT", fontsize=16)

plt.subplot(322)
plot_predictions([tree_1], X, y, axes=axes, label="$h(x_1) = h_1(x_1)$", data_label="Train Data")
plt.ylabel("$y$", fontsize=12, rotation=0)
plt.title("Prediction of Ensemble", fontsize=16)

plt.subplot(323)
plot_predictions([tree_2], X, y2, axes=axes, label="$h_2(x_1)$", style="g-", data_style="k+", data_label="Residual Error")
plt.ylabel("$y - h_1(x_1)$", fontsize=16)

plt.subplot(324)
plot_predictions([tree_1, tree_2], X, y, axes=axes, label="$h(x_1) = h_1(x_1) + h_2(x_1)$")
plt.ylabel("$y$", fontsize=12, rotation=0)

plt.subplot(325)
plot_predictions([tree_3], X, y3, axes=axes, label="$h_3(x_1)$", style="g-", data_style="k+")
plt.ylabel("$y - h_1(x_1) - h_2(x_1)$", fontsize=12)
plt.xlabel("$x_1$", fontsize=12)

plt.subplot(326)
plot_predictions([tree_1, tree_2, tree_3], X, y, axes=axes, label="$h(x_1) = h_1(x_1) + h_2(x_1) + h_3(x_1)$")
plt.xlabel("$x_1$", fontsize=12)
plt.ylabel("$y$", fontsize=12, rotation=0)

plt.show()
```
<p align="center">
    <img src="Img/GBM.png" width="650"/>
</p>

ì‹œê°í™” ê²°ê³¼ë¥¼ ë³´ë©´, ì‹œì ì´ 1ì—ì„œ 3ìœ¼ë¡œ ì¦ê°€í•  ìˆ˜ë¡ Graident Boosting Modelì˜ ì˜ˆì¸¡ ê°’ì´ ì ì  ì‹¤ì œ ë°ì´í„°ì— ì˜ fittingë˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**<span style="color:darkviolet">ì •ë¦¬í•˜ìë©´ Gradient Boostingì˜ í•™ìŠµ ë°©ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.**   
1. ì´ˆê¸° ê°’ìœ¼ë¡œ ìƒìˆ˜ í•¨ìˆ˜ ì ìš©
2. Loss functionì„ ìµœì†Œí™” í•˜ëŠ” gradientë¥¼ êµ¬í•¨
3. Gradientë¥¼ $h^t$ì˜ targetìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ gradientë¥¼ ê³ ë ¤í•œ í•™ìŠµ ì§„í–‰
4. ì ì ˆí•œ Learning rate $\alpha$ë¥¼ ê³ ë ¤í•œ ìµœì¢… ëª¨í˜• ìƒì„±
5. 2~4 ê³¼ì • ë°˜ë³µ

<br/>

## <span style="background-color:#fff5b1"> [ì‹¤í—˜ 6] Learning Rateì— ë”°ë¥¸ Gradient Boostingì˜ ì„±ëŠ¥ ë³€í™”
ì´ë•Œ í•™ìŠµ ë°©ì‹ì—ì„œ **<span style="color:maroon">"ì ì ˆí•œ Learning rate"ë¥¼ ì„¤ì •í•˜ëŠ” ê¸°ì¤€ì€ ë¬´ì—‡ì¼ê¹Œìš”?** ì˜ˆì œì—ì„œëŠ” ì„ì˜ë¡œ 1e-2 ê°’ì„ ì‚¬ìš©í•˜ì˜€ìœ¼ë‚˜, **<span style="color:maroon">Lossë¥¼ ìµœì†Œí™” í•˜ëŠ” gradientë¥¼ ì°¾ê¸° ìœ„í•´ì„œëŠ” learning rate ê°’ì„ ì˜ ì„¤ì •í•´ì£¼ëŠ” ê²ƒì´ ë§¤ìš° ì¤‘ìš”í•´ë³´ì…ë‹ˆë‹¤.** ì´ë¥¼ ì‹¤í—˜ìœ¼ë¡œ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤.   
```python
from sklearn.ensemble import GradientBoostingClassifier
```
```python
# Get a list of models to evaluate
def get_models():
    models = dict()
    # Define learning rates to explore
    for i in [1e-4, 1e-3, 1e-2, 1e-1, 1e-0]:
        lr = '%.4f' % i
        models[lr] = GradientBoostingClassifier(learning_rate=i)
    return models


def print_results_per_lr(models:dict, X, y):
    acc_score, auroc_score, lr_list = [], [], []
    for k, model in models.items():
        # Evaluate Gradient Boosting Ensemble Models per learning rate
        acc, auroc = evaluate_model(model, X, y)
        # Store the Results
        acc_score.append(acc)
        auroc_score.append(auroc)
        lr_list.append(k)
        
        # Print the performance along the way
        print('lr=%s' % (k))
        
    return acc_score, auroc_score, lr_list
```
```python
# Define Dataset
X, y = get_dataset()

# Define Models
GBM = get_models()

# Evaluate Models
gbm_acc, gbm_auroc, gbm_lr = print_results_per_lr(GBM, X, y)
```

### Results
```
Accuracy: 0.74 (0.05)   
AUROC: 0.80 (0.05)   
lr=0.0001    

Accuracy: 0.75 (0.05)    
AUROC: 0.82 (0.05)    
lr=0.0010   

Accuracy: 0.81 (0.04)   
AUROC: 0.90 (0.03)   
lr=0.0100   

Accuracy: 0.91 (0.02)   
AUROC: 0.97 (0.01)   
lr=0.1000   

Accuracy: 0.92 (0.02)   
AUROC: 0.97 (0.01)   
lr=1.0000   
```
<br/>

## <span style="background-color:#fff5b1"> [ì‹¤í—˜ 6] ê²°ê³¼ í•´ì„
```python
plt.figure(figsize=(10, 5))
plt.title("Gradient Boosting Learning Rate vs. Classification Accuracy")
plt.boxplot(gbm_acc, labels=gbm_lr, showmeans=True);
```

<p align="center">
    <img src="Img/gbm_lr_acc.png" width="500"/>
</p>

```python
plt.figure(figsize=(10, 5))
plt.title("Gradient Boosting Learning Rate vs. Classification AUROC")
plt.boxplot(gbm_auroc, labels=gbm_lr, showmeans=True);
```

<p align="center">
    <img src="Img/gbm_lr_auroc.png" width="500"/>
</p>

- ê²°ê³¼ë¥¼ ì‹œê°í™”í•œ Plotì„ ë³´ë©´, **<span style="color:maroon">Learning Rateê°€ ì˜¬ë¼ê°ˆ ìˆ˜ë¡ Gradient Boostingì˜ ì„±ëŠ¥ì´ í–¥ìƒí•˜ëŠ” ê²½í–¥ì„ í™•ì¸**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ì´ëŠ” **<span style="color:maroon">Learning Rateê°€ ë†’ì„ ìˆ˜ë¡ ë¹ ë¥´ê²Œ ëª¨ë¸ì˜ Biasë¥¼ ì¤„ì—¬ë‚˜ê°€ê¸° ë•Œë¬¸**ì…ë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ learning rateì€ ë†’ì„ ìˆ˜ë¡ ë¬´ì¡°ê±´ ì¢‹ì€ ê²ƒì¼ê¹Œìš”? ê·¸ë ‡ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤.
- ì‚¬ì‹¤ **<span style="color:maroon">ì¼ë°˜ì ìœ¼ë¡œëŠ” 1e-3(0.001) ~ 1e-2(0.01) ìˆ˜ì¤€ì˜ ë‚®ì€ ê°’ì„ ì„¤ì •í•˜ëŠ” ê²ƒì´ ë³´í†µ**ì…ë‹ˆë‹¤. ê·¸ ì´ìœ ëŠ” **ì‘ì€ ê°’ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ì•¼ ì„¸ë°€í•œ Modelì„ ì–»ì„ ìˆ˜ ìˆê¸° ë•Œë¬¸**ì…ë‹ˆë‹¤. **<span style="color:maroon">Learning Rateê°€ ë†’ìœ¼ë©´ ë¹ ë¥´ê²Œ ëª¨ë¸ì˜ Biasë¥¼ ì¤„ì¼ ìˆ˜ ìˆëŠ” í•œí¸ Fitting ê³¼ì •ì—ì„œ detailí•œ ë¶€ë¶„ì„ ë†“ì¹  ìˆ˜ ìˆë‹¤ëŠ” Trade-offë¥¼ ê°ì•ˆ**í•´ì•¼ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

<br/>

# <span style="color:purple">Boosting 2: <span style="color:crimson">CatBoost ğŸ˜»
CatBoostëŠ” "Gradient Boosting with Categorical Features Suppeort", ì¦‰, ì„¤ëª… ë³€ìˆ˜ì— Category íƒ€ì…ì˜ ë°ì´í„°ê°€ í¬í•¨ë˜ì–´ ìˆì„ ë•Œ ìœ ìš©í•˜ê²Œ ì‚¬ìš©ë˜ëŠ” Gradient Boosting ê³„ì—´ ë°©ë²•ë¡ ì…ë‹ˆë‹¤. CatBoostì˜ ì €ìë“¤ì€ ê¸°ì¡´ì˜ Gradient Boosting ëª¨ë¸ì˜ ë¬¸ì œì ì„ ì§šìœ¼ë©´ì„œ ë…¼ë¬¸ì˜ ì„œë¡ ì„ ì‹œì‘í•˜ëŠ”ë°, ëª¨ë¸ì„ ìˆœì°¨ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ í•˜ëŠ” ë° ìˆì–´ **Greedy Manner**ë¥¼ ì´ìš©í•œë‹¤ëŠ” ë¬¸ì œë¥¼ ì§€ì í•©ë‹ˆë‹¤. ì´ê²ƒì´ **<span style="color:crimson">Inferenceì—ì„œ í™œìš©ë˜ì–´ì•¼ í•  ë°ì´í„°ë¥¼ Train ë‹¨ê³„ì—ì„œ ì´ìš©í•˜ê³  ìˆë‹¤ëŠ” ë¬¸ì œë¥¼ ì§€ì **í•œ ê²ƒì´ì£ .   

ì•ì„œ Gradient Boostingì—ì„œ të²ˆì§¸ Boosting ëª¨í˜• $F_t$ë¥¼ ë§Œë“¤ ë•Œì—ëŠ”, t-1ë²ˆì§¸ê¹Œì§€ ëˆ„ì ëœ ëª¨í˜• $F_{t-1}$ì— í˜„ì¬ ì‹œì ì˜ ëª¨í˜• $h_t$ë¥¼ ë”í•´ì£¼ëŠ” ë°©ì‹ì„ ì´ìš©í•œë‹¤ê³  í•˜ì˜€ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì•„ë˜ì™€ ê°™ì´ t ì‹œì ì˜ Boosting ëª¨í˜•ì„ êµ¬í•  ìˆ˜ ìˆëŠ” ê²ƒì´ì£ . 
$$F_t(x) = F_{t-1}(x) + \alpha h_t(x)$$
ì´ë•Œ $h_t$ëŠ”, t-1 ì‹œì ê¹Œì§€ì˜ ëˆ„ì  ëª¨ë¸ $F_{t-1}$ì— $h_t$ë¥¼ ë”í•˜ì˜€ì„ ë•Œ ì¶”ì •ë˜ëŠ” ì˜ˆì¸¡ ê°’ì„ ì‹¤ì œ ê°’ê³¼ ë¹„êµí–ˆì„ ë•Œ ê·¸ Lossê°€ ìµœì†Œí™” ë˜ëŠ” í•¨ìˆ˜ë¥¼ ì°¾ìŠµë‹ˆë‹¤. ì´ëŠ” ì•„ë˜ì™€ ê°™ì€ ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
$$h_t = arg\ \underset{h \in H}{min}\mathcal{L}(F_{t-1}+h) = arg \ \underset {h \in H}{min} \ \mathbb{E}L(y)F_{t-1}(x)+h(x)$$   

ì´ë•Œ $h_t$ë¥¼ ê·¼ì‚¬í•˜ëŠ” ë°ëŠ”, t ì‹œì ì—ì„œì˜ gradientì˜ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ì´ë¥¼ ì¶”ì •í•©ë‹ˆë‹¤. ì£¼ë¡œ Least Square Approximationì„ ì‚¬ìš©í•˜ëŠ”ë°, ì´ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

$$h_t = arg \ \underset{h\in H}{min} \ \mathbb{E}(-g_t(x, y)-h(x))^2 \\
g_t(x, y) := \frac{\partial L(y,s)}{\partial F_{t-1}(x)}$$

ë‹¤ì‹œ ë§í•´, negative gradient = $-g_t(x, y)$ì—ì„œ í˜„ì¬ h(x)ë¼ëŠ” í•¨ìˆ˜ë¥¼ ì¶”ì •í–ˆì„ ë•Œ, ê·¸ ì°¨ì´ì— ëŒ€í•œ Expectationì´ ìµœì†Œí™” ë˜ëŠ” të²ˆì§¸ ì‹œì ì—ì„œì˜ tree ëª¨í˜•ì„ ë§Œë“œëŠ” ê²ƒì´ Gradient Boosting ëª¨í˜•ì´ì£ .   

ì´ì— CaBoostì˜ ì €ìë“¤ì€, **<span style="color:crimson">ê¸°ë³¸ì ì¸ Gradient Boosting ê³„ì—´ì˜ ë°©ë²•ë¡ ë“¤ì´ ê°€ì§€ëŠ” 2ê°€ì§€ ë¬¸ì œë¥¼ ì œê¸°**í•©ë‹ˆë‹¤.  

## Problems of Gradient Boosting 1: <span style="color:crimson">Prediction Shift
ë¨¼ì € $h_t$ë¥¼ ì¶”ì •í•˜ëŠ” ë° ìˆì–´, ëª¨ë“  ë°ì´í„°ì…‹ì— ëŒ€í•œ ê¸°ëŒ“ê°’ì„ ìµœì†Œí™” í•˜ëŠ” ê²ƒì€ ìœ í•œí•œ ê´€ì¸¡ì¹˜ ê°œìˆ˜ë¥¼ ê°€ì§€ëŠ” ë°ì´í„°ì—ì„œëŠ” ë¶ˆê°€ëŠ¥í•œ ì¼ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ì•„ë˜ì™€ ê°™ì´ Training Datasetì— ëŒ€í•œ í‰ê· ì²™ ì°¨ì´ë¡œ ê·¼ì‚¬í•˜ê²Œ ë˜ì£ .  
    
$$h_t = arg \underset{h \in H}{min}\mathbb{E}(-g_t(x, y)-h(x))^2 \approx \frac{1}{n}\sum^{n}{k=1}(-g_t(x,y)-h(x))^2 \\  Training \ Dataset: \mathcal{D}=(x_k, y_k)_{k=1,...,n} \ where \ x_k=(x^1_k, ..., x^m_k), \quad y_k \in \mathbb{R}$$
    
ë°”ë¡œ ì´ëŸ¬í•œ ì§€ì ì—ì„œ, í•™ìŠµ ë°ì´í„°ì—ì„œ $x_k$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ì§€ê¸ˆê¹Œì§€ ìš°ë¦¬ê°€ ë§Œë“¤ì—ˆë˜ ëˆ„ì ëœ Boosting ëª¨í˜•ì˜ ê°’ê³¼ Test Example $x$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ëˆ„ì ëœ Boosting ëª¨í˜•ì˜ ê°’ì´ ë‹¤ë¥´ë‹¤ëŠ” ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤. ë‹¤ì‹œ ë§í•´, **<span style="color:crimson">train example $x_k$ê°€ ì£¼ì–´ì¡Œì„ ë•Œì˜ gradientì™€, $x$ê°€ ì£¼ì–´ì¡Œì„ ë•Œì˜ test exampleì—ì„œì˜ gradientì˜ conditional distributionì´ ë‹¤ë¥¸ ê²ƒ**ì…ë‹ˆë‹¤.   
$$F_{t-1}(x_k)|x_k â‰  F_{t-1}(x)|x$$
ì´ ë‘ ê°€ì§€ê°€ ê°™ì•„ì•¼ ëª¨ë¸ë§ì˜ ì •í•©ì„±ì´ í™•ë³´ ë˜ëŠ”ë°, í•™ìŠµ ë°ì´í„°ì— ëŒ€í•œ ëˆ„ì  í•¨ìˆ˜ì˜ ì¡°ê±´ë¶€ í™•ë¥ ê³¼ ê²€ì¦ìš© ë°ì´í„°ì— ëŒ€í•œ ì¡°ê±´ë¶€ í™•ë¥ ì´ ë‹¤ë¥¸ ê²ƒì´ ë°”ë¡œ ì²« ë²ˆì§¸ Issueì…ë‹ˆë‹¤. ì´ë¥¼ Prediction Shiftë¼ í•˜ê³ , ì´ë ‡ê²Œ **<span style="color:crimson">í¸í–¥ëœ $h_t$ë¥¼ $F_t$ë¥¼ ë§Œë“œëŠ” ë° ì‚¬ìš©í•˜ë©´, ê²°êµ­ $F_t$ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì´ ë¬¸ì œê°€ ëœë‹¤ëŠ” ì ì„ ì§šì€ ê²ƒì´ì£ . ì‹¤ì œë¡œ Gradient Boosting ëª¨ë¸ì€ Overfittingì˜ ë¬¸ì œ**ë¥¼ ì•ˆê³  ìˆìŠµë‹ˆë‹¤.   

<br/>

## <span style="color:crimson">Solution 1: Ordered Boosting
ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ CatBoostì—ì„œ ì œì•ˆí•œ ë°©ë²•ì´ ë°”ë¡œ **<span style="color:crimson">Ordered Boosting**ì…ë‹ˆë‹¤. 'Ordered'ë¼ëŠ” ë§ì´ ë“¤ì–´ê°„ ì´ìœ ëŠ”, ë³€ìˆ˜ì— ëŒ€í•´ ë¬´ì‘ìœ„ permutationë¥¼ ìˆ˜í–‰í•˜ì—¬ ìˆœì—´ì„ ë§Œë“¤ê³ , ìˆœì°¨ì ìœ¼ë¡œ ì”ì°¨ë¥¼ ê³„ì‚°í•˜ë©° treeë¥¼ í•™ìŠµì‹œí‚¤ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.   

ë§Œì¼ 9ê°œì˜ ë³€ìˆ˜ê°€ ìˆë‹¤ê³  ê°€ì •í•˜ìë©´, ë°©ë²•ë¡ ì„ ì•„ë˜ì™€ ê°™ì´ ë„ì‹í™” í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.   
<p align="center">
    <img src="Img/ordered_boosting.PNG" width="650"/>
</p>

$M_5^{t-1}$ì€ 5ë²ˆì§¸ ë°ì´í„°ê¹Œì§€ë§Œì„ ì‚¬ìš©í•´ ë§Œë“¤ì–´ë‚¸ ëª¨ë¸ì´ê³ , $M_6^{t-1}$ì€ 6ë²ˆì§¸ ë°ì´í„°ê¹Œì§€ë§Œì„ ì‚¬ìš©í•´ ë§Œë“¤ì–´ë‚¸ ëª¨ë¸ì´ê² ì£ . <span style="color:crimson">ì¤‘ìš”í•œ ê²ƒì€ **<span style="color:crimson">7ë²ˆì§¸ ë°ì´í„°ì— ëŒ€í•´ ì”ì°¨(residual)ë¥¼ êµ¬í•  ë•Œ, $M_7^{t-1}$ë¥¼ ì´ìš©í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, $M_6^{t-1}$ë¥¼ ì´ìš©í•´ì•¼ í•œë‹¤ëŠ” ê²ƒ**</span>ì…ë‹ˆë‹¤. ì™œ ê·¸ëŸ´ê¹Œìš”? **<span style="color:crimson">$M_6^{t-1}$ë¥¼ ë§Œë“¤ ë•Œ 7ë²ˆì§¸ ë°ì´í„°, $x_3$ì€ í•œ ë²ˆë„ ì‚¬ìš©ëœ ì ì´ ì—†ì£ . ê·¸ë ‡ê¸°ì— inference ë•Œì™€ ë™ì¼í•œ í™˜ê²½ì„ ì¡°ì„±**í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ì„œ Prediction Shift ë¬¸ì œë¥¼ í•´ê²°í•˜ê³  ìˆìŠµë‹ˆë‹¤.   

<br/>

## <span style="background-color:#fff5b1"> [ì‹¤í—˜ 7] CatBoost vs. Gradient Boosting: in terms of performance
ì‚¬ì‹¤ Ensemble Learningì€ ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê°’ì„ í•©ì¹˜ëŠ” ì·¨í•©í•˜ëŠ” ë°©ì‹ì´ê¸° ë•Œë¬¸ì—, Single Modelë³´ë‹¤ëŠ” Overfittingì„ ë°©ì§€í•  ìˆ˜ ìˆëŠ” ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤. ê·¸ëŸ¼ì—ë„ Deisicion Treeë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì˜ ê²½ìš°ì—ëŠ” ì—¬ì „íˆ Overfittingì˜ ë¬¸ì œë¥¼ í”¼í•´ê°€ê¸° í˜ë“¤ë©°, Gradient Boosting ê³„ì—´ ëª¨ë¸ë“¤ì˜ ê²½ìš° ì•ì„  ì´ìœ ì—ì„œ ê³¼ì í•©ì˜ ê²½í–¥ì„ ë³´ì´ê³  ìˆìŠ¤ë‹ˆë‹¤.   

**<span style="color:maroon">CatBoostëŠ” ì •ë§ GBMê³¼ëŠ” ë‹¤ë¥´ê²Œ Overfittingì„ ì™„í™”í•˜ì˜€ì„ê¹Œìš”? ê·¸ë ‡ë‹¤ë©´ Test ê²°ê³¼ CatBoostì˜ ì„±ëŠ¥ì´ ë” ë†’ì„ ê²ƒì…ë‹ˆë‹¤.** CatBoostì˜ ê¸°ë³¸ì  ì„±ëŠ¥ì„ ì‹¤í—˜ìœ¼ë¡œ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤.   

```python
from catboost import CatBoostClassifier
```
```python
# Define Dataset
X, y = get_dataset()

# Gradient Boosting Classifier
gbm_clf = GradientBoostingClassifier(random_state=2022, learning_rate=1e-2)

# CatBoost Classifier
cat_clf = CatBoostClassifier(random_seed=2022, learning_rate=1e-2)
```
- GBMê³¼ CatBoostëŠ” ë™ì¼í•œ Hyperparameterë¥¼ ì‚¬ìš©í•˜ë©°, learning_rateëŠ” 1e-2ë¡œ ì¼ë°˜ì ìœ¼ë¡œ ìì£¼ ì‚¬ìš©ë˜ëŠ” ì‘ì€ ê°’ì„ ì„ì˜ë¡œ ì„¤ì •í•´ì¤ë‹ˆë‹¤. learning rateê°€ 1e-2ì¸ ì´ìœ ëŠ” **<span style="background-color:#fff5b1">[ì‹¤í—˜ 6]** ì—ì„œ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤.   

```python
# Evaluate GBM
acc_gbm, auroc_gbm = evaluate_model(model=gbm_clf, X=X, y=y)

# Evaluate CatBoost
acc_cat, auroc_cat = evaluate_model(model=cat_clf, X=X, y=y)
```

### Results
|                       | __Gradient Boosting__| __CatBoost__ |
|:---------------------:|:--------------------:|:------------:|
|__Mean Accuracy (std)__| 0.81 (0.04)          | **0.95 (0.02)**  |
|   __Mean AUROC (std)__|   0.90 (0.03)        |  **0.99 (0.01)** |

<br/>

### <span style="background-color:#fff5b1"> [ì‹¤í—˜ 7] ê²°ê³¼ í•´ì„
- ì‹¤í—˜ ê²°ê³¼, **<span style="color:maroon">CatBoostê°€ ê´„ëª©í•  ë§Œí•œ ì„±ëŠ¥ì„ ë‚´ë©°, ì„±ëŠ¥ ì¸¡ë©´ì—ì„œ ìš°ìˆ˜ì„±ì„ ì…ì¦**í•˜ì˜€ìŠµë‹ˆë‹¤. Accuracy ì¸¡ë©´ì—ì„œ 14%, AUROC ì¸¡ë©´ì—ì„œëŠ” 9% ê°€ëŸ‰ ë†’ì€ ì„±ëŠ¥ì„ ê¸°ë¡í•œ ê²ƒì…ë‹ˆë‹¤.
- ë”ë¶ˆì–´ **<span style="color:maroon">ëª¨ë¸ë“¤ì˜ ì˜ˆì¸¡ ê°’ í¸ì°¨ë„ Auuracyì™€ AUROC ì¸¡ë©´ì—ì„œ ëª¨ë‘ ë” ë‚®ì€ ê°’ì„ ê¸°ë¡í•˜ë©°, Biasë¿ ì•„ë‹ˆë¼ Varianceë¥¼ ë‚®ì¶˜ ì´ìƒì ì¸ ëª¨ë¸ë¡œì„œ ì„±ëŠ¥ì„ ì…ì¦**í–ˆìŠµë‹ˆë‹¤.
- ì´ëŠ” Inference ì‹œ í™œìš©ë˜ì–´ì•¼ í•  ë°ì´í„°ë¥¼ í•™ìŠµ ì‹œ ëª¨ë¸ ì—…ë°ì´íŠ¸ ê³¼ì •ì—ì„œ ì§€ì†ì ìœ¼ë¡œ ì‚¬ìš©í–ˆë˜ Gradient Boostingê³¼ëŠ” ë‹¤ë¥´ê²Œ, **<span style="color:maroon">CatBoostì—ì„œëŠ” t ì‹œì ì˜ ì”ì°¨ë¥¼ ê³„ì‚°í•˜ëŠ” ê³¼ì •ì—ì„œ t-1 ì‹œì ì˜ ëˆ„ì  Boosting ëª¨í˜•ì„ í™œìš©í•˜ë©´ì„œ, Inference ë•Œì™€ ë™ì¼í•œ í™˜ê²½ì„ ì¡°ì„±í•˜ì˜€ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì¦‰, Generalization ì„±ëŠ¥ì´ í™•ë³´**ëœ ê²ƒì´ì£ .

<br/>

## <span style="background-color:#fff5b1"> [ì‹¤í—˜ 8] CatBoost vs. Gradient Boosting: in terms of training time
ì•ì„œ CatBoostëŠ” random permutationì„ í•™ìŠµ ê³¼ì •ë§ˆë‹¤ ì‹¤ì‹œí•˜ê³  ìˆœì°¨ì ìœ¼ë¡œ residualì„ ê³„ì‚°í•˜ë©° ì§€ì†ì ì¸ updateë¥¼ ìˆ˜í–‰í•œë‹¤ê³  í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ë¡œì¨ Overfittingì„ ë°©ì§€í•˜ê³  Testing ì„±ëŠ¥ì„ ë†’ì¼ ìˆ˜ ìˆì§€ë§Œ, **<span style="color:maroon">í•™ìŠµ ì‹œ ì†Œìš” ì‹œê°„ì´ ë§¤ìš° ì˜¤ë˜ ê±¸ë¦¬ì§€ëŠ” ì•Šì„ê¹Œìš”?** ì´ë¥¼ ì§ì ‘ ì‹¤í—˜ì„ í†µí•´ ë¹„êµí•´ë³´ê² ìŠµë‹ˆë‹¤.   

```python
def evaluate_model_time(model, X, y, task: str):
    training_time = dict()
    
    start = time.time()
    
    # Define the evaluation procedure
    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=2022)
    # Evaluate model and collect the results
    acc = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)
    roc_auc = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)
    training_time[task] = np.round(time.time() - start, 3)
    
    # Report Performance
    print("Accuracy: %.2f (%.2f)" % (np.mean(acc), np.std(acc)))
    print("AUROC: %.2f (%.2f)" % (np.mean(roc_auc), np.std(roc_auc)))
    
    # Report Training Time
    print(f"Training Time: {training_time[task]}s")
    
    return acc, roc_auc
```
```python
acc_gbm, auroc_gbm = evaluate_model_time(model=gbm_clf, X=X, y=y, task="Gradient Boosting")

acc_cat, auroc_cat = evaluate_model_time(model=cat_clf, X=X, y=y, task="CatBoost")
```

### Results
|                       | __Gradient Boosting__| __CatBoost__ |
|:---------------------:|:--------------------:|:------------:|
| __Training Time__     |  **2.192 sec**           |   73.247 sec   |
|__Mean Accuracy (std)__| 0.81 (0.04)          | **0.95 (0.02)**  |
|   __Mean AUROC (std)__|   0.90 (0.03)        |  **0.99 (0.01**) |

<br/>

### <span style="background-color:#fff5b1"> [ì‹¤í—˜ 8] ê²°ê³¼ í•´ì„
- ì‹¤í—˜ ê²°ê³¼ **<span style="color:maroon">CatBoostì˜ í•™ìŠµ ì‹œê°„ì´ ì•½ 33% ì´ìƒ ì˜¤ë˜ ê±¸ë¦° ê²ƒì„ í™•ì¸**í•˜ì˜€ìŠµë‹ˆë‹¤. 
- ì´ëŠ” ì•ì„œ ì–¸ê¸‰í•œ ë°”ì™€ ê°™ì´ **<span style="color:maroon">CatBoostê°€ í•™ìŠµ ë‹¨ê³„ë§ˆë‹¤ Random Permutationì„ ìˆ˜í–‰ í›„ ìˆœì°¨ì ìœ¼ë¡œ ì”ì°¨ë¥¼ ê³„ì‚°í•˜ëŠ” í•™ìŠµ ë°©ì‹ì„ í†µí•´ ëª¨ë¸ì„ ì—…ë°ì´íŠ¸í•˜ê¸° ë•Œë¬¸**ì…ë‹ˆë‹¤.
- ì¦‰, CatBoostëŠ” ì„±ëŠ¥ì€ ë†’ì§€ë§Œ í•™ìŠµ ì‹œê°„ì´ ë§¤ìš° ì˜¤ë˜ ê±¸ë¦¬ê¸°ì—, ë‘˜ ì‚¬ì´ì˜ Trade Offë¥¼ ê³ ë ¤í•´ì•¼ í•˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤.


<br/>

## Problems of Gradient Boosting 2: <span style="color:crimson">Target Leakage (in Target Statistics)   
> **Target Statistics (TS)**   
> Categorical ë³€ìˆ˜ë“¤ì„ Numerical Valueë¡œ ë°”ê¾¸ëŠ” ë°©ë²•ë¡ ì…ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ê°€ì¥ ë²”ìš©ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ê²ƒì€ Mean-Encoding/Target-Encodingì´ë¼ ë¶ˆë¦¬ëŠ” ë°©ë²•ë¡ ìœ¼ë¡œ, í•™ìŠµ ë°ì´í„°ì…‹ì˜ **Categorical Featuresë¥¼ Target Valueì˜ í‰ê· ìœ¼ë¡œ ëŒ€ì²´**í•˜ëŠ” ë°©ë²•ë¡ ì„ ì¼ì»«ìŠµë‹ˆë‹¤.   

ê¸°ì¡´ Gradient Boosting ëª¨ë¸ì˜ ë‘ ë²ˆì§¸ ì´ìŠˆëŠ” Target Leakageì…ë‹ˆë‹¤. **<span style="color:crimson">í•™ìŠµ ë°ì´í„°ì˜ Target(ì •ë‹µ) ì •ë³´ë¥¼ ì…ë ¥ ë³€ìˆ˜, ì¦‰ ê°ì²´ì˜ ì†ì„±ì„ ì •ì˜í•˜ëŠ” ë° ì´ë¯¸ ì‚¬ìš©ë˜ê³  ìˆë‹¤ëŠ” ë¬¸ì œë¥¼ ì œê¸°**í•œ ê²ƒì…ë‹ˆë‹¤. ì›ë˜ ë¨¸ì‹ ëŸ¬ë‹ì˜ ëª©ì ì€ ì…ë ¥ ë³€ìˆ˜ $x$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ $y$ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆëŠ” í•¨ìˆ˜ $f(\cdot)$ë¥¼ ì°¾ëŠ” ê²ƒì´ì£ . ê·¸ëŸ°ë° ì¼ë°˜ì ì¸ Boositngì—ì„œì˜ Target Statistics ë°©ì‹ì—ì„œëŠ” ì‹¤ì œ $y$ ê°’ì´ $\hat y$ì„ ê³„ì‚°í•˜ëŠ” ë° ì‚¬ìš©ë˜ê³  ìˆë‹¤ëŠ” ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì¦‰, $x$ë¡œ $y$ë¥¼ ì¶”ì •í•œë‹¤ëŠ” ê¸°ë³¸ ê°€ì •ì„ ìœ„ë°°í•œ ì±„ target ì •ë³´ê°€ ìƒˆì–´ë‚˜ê°€ê³  ìˆë‹¤ëŠ” ê²ƒì´ì£ .  

ë”°ë¼ì„œ CatBoostì˜ ì €ìë“¤ì€ **Target Statisticsì—ì„œ í•„ìš”í•œ ë°”ëŒì§í•œ ì†ì„±**ì— ëŒ€í•˜ì—¬ ì§ˆë¬¸ì„ ë˜ì§€ê³ , ë‚˜ë¦„ì˜ 2ê°€ì§€ ì†ì„±ì„ ì œì•ˆí•©ë‹ˆë‹¤.
$$Property\ 1 \quad \mathbb{E}(\hat x^i | y=v) = \mathbb{E}(\hat x^i_k | y_k=v), where \ (x_k, y_k) \ is \ the \ k-th \ training \ example.$$
$$Property\ 2 \quad Effective\ usage\ of\ all\ training\ data\ for\ calculating\ TS\ features\ and\ for\ learning\ a\ model$$   

**Property 1**ì€ $y$ê°€ $v$ë¼ëŠ” ê°’ì„ ê°€ì§ˆ ë•Œ, $i$ë²ˆì§¸ ì†ì„±ì— í•´ë‹¹í•˜ëŠ” Expectationê³¼, kë²ˆì§¸ training exampleì— ëŒ€í•´ì„œ $y_k$ê°€ $v$ì¼ ë•Œì˜ Expectationì´ ê°™ì•„ì•¼ í•œë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤. ì‰½ê²Œ ë§í•´, **$y$ê°€ $v$ë¼ëŠ” ê°’ì„ ê°€ì§ˆ ë•Œ Train ë°ì´í„°ì™€ Test ë°ì´í„°ì— ëŒ€í•´ì„œ Target Statisticsì˜ ê¸°ëŒ“ê°’ì´ ëª¨ë‘ ë™ì¼í•´ì•¼ í•œë‹¤ëŠ” ê²ƒ**ì…ë‹ˆë‹¤.   

<br/>

**Property 2**ëŠ” ê°€ê¸‰ì ì´ë©´ **Target Statisticsë¥¼ ê³„ì‚°í•˜ëŠ” ë° ìˆì–´ ëª¨ë“  Datasetì„ íš¨ê³¼ì ìœ¼ë¡œ ì‚¬ìš©í•˜ë¼ëŠ” ì˜ë¯¸**ì…ë‹ˆë‹¤. ê²°êµ­ ê°€ëŠ¥í•œ ë§ì€ í•™ìŠµ ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ì„œ TSë¥¼ ê³„ì‚°í•˜ë¼ëŠ” ê²ƒì´ì£ .

<br/>

## <span style="color:crimson">Solution 2: Ordered Target Statistics
ìœ„ì™€ ê°™ì€ ì†ì„±ì„ ë§Œì¡±ì‹œí‚¤ê¸° ìœ„í•´ì„œ, ì €ìë“¤ì´ ì œì•ˆí•œ Solutionì´ ë°”ë¡œ Ordered Target Statisticsì…ë‹ˆë‹¤. ì—¬ê¸°ì„œ 'Ordered'ë¼ í•¨ì€, ì•ì„œ ì–¸ê¸‰í•œ ê²ƒê³¼ ë§ˆì°¬ê°€ì§€ë¡œ, **<span style="color:crimson">ê°ì²´ë¥¼ ë¬´ì‘ìœ„ë¡œ permutation ì‹œí‚¨ í›„, Artificial Time, ì¦‰ ê°€ìƒì˜ ì‹œê°„ ê°œë…ì„ ë¶€ì—¬í•˜ê¸° ë•Œë¬¸**ì…ë‹ˆë‹¤. ì´ë•Œ Ordered Boostingê³¼ ë§ˆì°¬ê°€ì§€ë¡œ, **$x_k$ì— ëŒ€í•œ TSë¥¼ ê³„ì‚°í•  ë•ŒëŠ” $x_k$ì˜ ì´ì „ ì •ë³´ë§Œì„ í™œìš©**í•©ë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ TSë¥¼ ìœ„í•´ í•„ìš”í•œ subset $\mathcal{D_k}$ëŠ” ì•„ë˜ì™€ ê°™ì´ í‘œê¸°ë˜ê² ì£ .
    
$$\mathcal{D}_k \subset \mathcal{D} \ \backslash \begin{Bmatrix}
x_k
\end{Bmatrix} \ excluding \ x_k$$

$$
\mathcal{D}_k = \begin{Bmatrix}
x_j : \sigma (j) < \sigma (k)
\end{Bmatrix}
$$

                       
ì—¬ê¸°ì„œ $\sigma$ëŠ” random permutationì„ ì˜ë¯¸í•˜ëŠ” parameterì…ë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ Categorical ë³€ìˆ˜ $x^i$ì—ì„œ kë²ˆì§¸ ê°ì²´ $x^i_k$ëŠ” ë‹¤ìŒê³¼ ê°™ì´ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.   
$$\hat{x}^i_k = \frac{\sum_{x_j \in \mathcal{D}_k} \mathbb{1} \begin{Bmatrix} x^i_j = x^i_k \end{Bmatrix} \cdot y_j + ap}{\sum{x_j \in \mathcal{D}_k} \mathbb{1} \begin{Bmatrix} x^i_j = x^i_k \end{Bmatrix} + a}$$

- ë¨¼ì € ë¶„ìì™€ ë¶„ëª¨ì— ê³µí†µì ìœ¼ë¡œ ë“¤ì–´ê°„ termì¸ $\sum_{x_j \in \mathcal{D}_k}\mathbb{1}\begin{Bmatrix} x^i_j = x^i_k \end{Bmatirx}$ëŠ” ë¬´ìŠ¨ ì˜ë¯¸ì¼ê¹Œìš”? **kë²ˆì§¸ ê´€ì¸¡ì¹˜ $x_k$ ì§ì „ê¹Œì§€ì˜ ëª¨ë“  ë°ì´í„°ì— ëŒ€í•˜ì—¬, $x_k$ì™€ ë™ì¼í•œ ì¹´í…Œê³ ë¦¬ ê°’ì„ ê°€ì§€ëŠ” ê´€ì¸¡ì¹˜ì˜ ê°œìˆ˜ë¥¼ ì˜ë¯¸**í•©ë‹ˆë‹¤.
                         
- $\sum_{x_j \in \mathcal{D}_k}\mathbb{1}\begin{Bmatrix} x^i_j = x^i_k \end{Bmatirx} \cdot y_j$ëŠ”, **kë²ˆì§¸ ê´€ì¸¡ì¹˜ ì§ì „ê¹Œì§€ì˜ ëª¨ë“  ë°ì´í„°ì— ëŒ€í•˜ì—¬, $x_k$ì™€ ë™ì¼í•œ ì¹´í…Œê³ ë¦¬ ê°’ì„ ê°€ì§€ëŠ” ê´€ì¸¡ì¹˜ì˜ Target ê°’**ì„ ì˜ë¯¸í•˜ê² ì£ .
- ì´ë•Œ **$a$ëŠ” Permutationì— ëŒ€í•œ Hyperparameter**ì´ê³ , Ordered Boostingì—ì„œë„ í•¨ê»˜ ì‚¬ìš©ë˜ëŠ” ê°’ì…ë‹ˆë‹¤.
- pëŠ” **kë²ˆì§¸ ê´€ì¸¡ì¹˜ ì§ì „ê¹Œì§€ì˜ ëª¨ë“  ë°ì´í„°ì— ëŒ€í•˜ì—¬, íŠ¹ì • Targetì´ ë‚˜íƒ€ë‚  ì„ í–‰ í™•ë¥ **ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

<br/>

## <span style="background-color:#fff5b1">  [ì‹¤í—˜ 9] Mean Target Encoding vs. Ordered Target Statistics
CatBoostëŠ” ì´ë¦„ì—ì„œë„ ì•Œ ìˆ˜ ìˆë“¯ì´ Categorical ë³€ìˆ˜ë¥¼ ë‹¤ë£¸ì— ìˆì–´ ìµœì í™”ëœ ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤. ì´ë•Œ ì•ì„œ ì–¸ê¸‰í•œ Target Mean ë°©ì‹ì€ Target Leakage ë¬¸ì œë¡œ ì¸í•œ Overfitting í˜„ìƒì´ ì¼ì–´ë‚œë‹¤ê³  ì§šì€ ë°” ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ CatBoostëŠ” Ordered TSë¥¼ í†µí•´ Target Leakageë¥¼ í•´ê²°í•˜ê³ ì í•©ë‹ˆë‹¤.   

ì´ë•Œ ë³¸ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” **<span style="color:maroon">Binary Classification Taskì— ìˆì–´, Ordered Target Statisticsì™€ Target Mean Encoding ëª¨ë‘ ì§ì ‘ Scratchë¡œ êµ¬í˜„í•´ë³´ê³  ê²°ê³¼ë¥¼ ë¹„êµ**í•´ë³´ê³ ì í•©ë‹ˆë‹¤.   

í™œìš©í•˜ëŠ” ë°ì´í„°ì…‹ì€ titanic dataset ì¼ë¶€ì´ë©°, Categorical ë³€ìˆ˜ë¥¼ ë‹¤ë£¨ê¸° ìœ„í•´ 'Embarked' Columnë§Œì„ í™œìš©í•©ë‹ˆë‹¤. **<span style="color:maroon"> ë³¸ íŠœí† ë¦¬ì–¼ì˜ ëª©ì **ì€ titanic ë°ì´í„°ì…‹ì—ì„œ ë†’ì€ ë¶„ë¥˜ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•œ ë¶„ì„ì´ ì•„ë‹ˆë¼, **<span style="color:maroon"> Categorical ë³€ìˆ˜ ì²˜ë¦¬ ë°©ì‹ì„ ë¹„êµí•˜ê¸° ìœ„í•¨**ì´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

```python
from catboost.datasets import titanic

# Loade Dataset
df, _ = titanic()
df = df[['Embarked', 'Survived']].dropna().reset_index(drop=True)

# Split Dataset into train/valid/test
train_idx, test_idx = train_test_split(np.arange(df.shape[0]), train_size=.8, random_state=2022)

df_train = df.iloc[train_idx].reset_index(drop=True)
df_test = df.iloc[test_idx].reset_index(drop=True)
```
- í™œìš©í•˜ëŠ” ë°ì´í„°ì…‹ì€ titanic dataset ì¼ë¶€ì´ë©°, Categorical ë³€ìˆ˜ë¥¼ ë‹¤ë£¨ê¸° ìœ„í•´ 'Embarked' Columnë§Œì„ í™œìš©í•©ë‹ˆë‹¤. **ë³¸ íŠœí† ë¦¬ì–¼ì˜ ëª©ì **ì€ titanic ë°ì´í„°ì…‹ì—ì„œ ë†’ì€ ë¶„ë¥˜ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•œ ë¶„ì„ì´ ì•„ë‹ˆë¼, **Categorical ë³€ìˆ˜ ì²˜ë¦¬ ë°©ì‹ì„ ë¹„êµí•˜ê¸° ìœ„í•¨**ì´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
- yëŠ” 'Survived'ì´ë©°, ìƒì¡´ ì—¬ë¶€ë¥¼ êµ¬ë¶„í•˜ëŠ” Binary Classification Taskì…ë‹ˆë‹¤.
- ì´ë•Œ ë³¸ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” **Binary classë¥¼ ê°€ì§€ëŠ” Targetì˜ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ Target Statistics ê°’ì„ ë§Œë“œëŠ” ë°©ë²•ì„ ì§ì ‘ êµ¬í˜„**í•´ë³¼ ê²ƒì…ë‹ˆë‹¤.


### <span style="color:maroon">1. Target Mean Encoding
```python
def Mean_TS_for_binary_clf(df: pd.DataFrame, X_col: str, y_col:str):
    Mean_TS_encode = df[y_col].groupby(df[X_col]).agg(['mean'])['mean']
    
    df.loc[:, 'TS'] = df[X_col].map(Mean_TS_encode)
    
    return df
```
- Binary Classification Taskë¥¼ í’€ê³ ì í•  ë•Œ, Target Mean Encoding ë°©ì‹ì€ ìœ„ì™€ ê°™ì´ êµ¬í˜„ë  ê²ƒì…ë‹ˆë‹¤.

#### <span style="color:maroon"> Target Mean Encoding: TS in Train Data
```python
df_train_TS = Mean_TS_for_binary_clf(df=df_train, X_col='Embarked', y_col='Survived')
df_train_TS
```
```
    Embarked	Survived	TS
761	    C	        1	0.550725
546	    C	        1	0.550725
450	    S	        0	0.349421
135	    S	        1	0.349421
58	    S	        1	0.349421
...	    ...	       ...	    ...
240	    Q	        1	0.327273
689	    S	        1	0.349421
624	    S	        0	0.349421
173	    C	        0	0.550725
220	    S	        0	0.349421
```
```python
df_test_TS = Mean_TS_for_binary_clf(df=df_test, X_col='Embarked', y_col='Survived')
df_test_TS
```
```
    Embarked	Survived	TS
747	    S	        0	0.285714
178	    S	        0	0.285714
784	    S	        0	0.285714
159	    S	        0	0.285714
337	    S	        1	0.285714
...	    ...	       ...	...
233	    S	        0	0.285714
80	    S	        1	0.285714
453	    S	        0	0.285714
136	    S	        0	0.285714
627	    S	        0	0.285714
```
- Mean Target Encodingì„ ì ìš©í•  ì‹œ Train dataì—ì„œ `Embarked` Columnì´ ê°€ì§€ëŠ” Target Statistics ê°’ì€ ìœ„ì™€ ê°™ìŠµë‹ˆë‹¤.
- ëª¨ë“  í•™ìŠµ ë°ì´í„°ì— ëŒ€í•´ì„œ, $encoding = (count\ of\ target\ 1)/(total\ occurence)$ì˜ ë°©ì‹ìœ¼ë¡œ ê°’ì´ ê³„ì‚°ë˜ê¸°ì—, í•˜ë‚˜ì˜ ì¹´í…Œê³ ë¦¬ëŠ” Target ê°’ê³¼ ìƒê´€ ì—†ì´ ëª¨ë‘ ë™ì¼í•œ TSë¡œ ì¹˜í™˜ë©ë‹ˆë‹¤.
- ì—¬ê¸°ì„œ ë¬¸ì œì ì´ ë³´ì…ë‹ˆë‹¤. **<span style="color:maroon"> Target Mean Encoding ë°©ì‹ì„ ì‚¬ìš©í•˜ë©´, ê°™ì€ ì¹´í…Œê³ ë¦¬ ë°ì´í„°ë¼ë„, Train ë°ì´í„°ì™€ Test ë°ì´í„° ê°„ì˜ TSê°€ ë‹¬ë¼ì§€ëŠ” ê²ƒ**ì…ë‹ˆë‹¤.
- ê·¸ë ‡ë‹¤ë©´ Ordered TS ë°©ì‹ì€ ë‹¤ë¥¼ê¹Œìš”?


<br/>

### <span style="background-color:#fff5b1"><span style="color:maroon">2. Ordered Target Statistics
```python
def Random_Permutation(x):
    perm = np.random.permutation(len(x)) 
    x = x.iloc[perm].reset_index(drop=True) 
    return x

def Ordered_TS_for_binary_clf(X: np.ndarray, y: np.ndarray, a: float=0.1):
    x_list, y_list, ts_list = [], [], []
    
    for k, (x, y) in enumerate(zip(X, y)):
        if k == 0:
            ts_list.append(0)
        else:
            p = np.sum(y_list)/len(y_list)
            ts = (sum(np.array(y_list)[[i for i, x_ in enumerate(x_list) if x_ == x]]) + a*p) / (x_list.count(x) + a)
            ts_list.append(ts)
        
        x_list.append(x)
        y_list.append(y)
    
    return pd.DataFrame(ts_list)
```
- Binary Classification Taskë¥¼ í’€ê³ ì í•  ë•Œ, Ordered TS ë°©ì‹ì„ êµ¬í˜„í•œ ê²ƒì…ë‹ˆë‹¤. 
- ë¨¼ì € Random Permutationì„ ìˆ˜í–‰í•´ì¤€ í›„, Ordered TSë¥¼ êµ¬í•´ì¤„ ê²ƒì…ë‹ˆë‹¤.

```python
permuted_train = Random_Permutation(df_train)
permuted_train['Ordered_TS_Embarked'] = Ordered_TS_for_binary_clf(permuted_train.Embarked, permuted_train.Survived, a=0.1)
permuted_train
```
```
    Embarked	Survived	Ordered_TS_Embarked
0	    S	        0	        0.000000
1	    S	        0	        0.000000
2	    S	        0	        0.000000
3	    S	        0	        0.000000
4	    S	        1	        0.000000
...	    ...	       ...	            ...
706	    S	        0	        0.350202
707	    Q	        0	        0.333434
708	    S	        0	        0.349522
709	    S	        1	        0.348844
710	    S	        0	        0.350104
```

```python
permuted_test = Random_Permutation(df_test)
permuted_test['Ordered_TS_Embarked'] = Ordered_TS_for_binary_clf(permuted_test.Embarked, permuted_test.Survived, a=0.1)
permuted_test
```
```
    Embarked	Survived	Ordered_TS_Embarked
0	    S	        0	        0.000000
1	    S	        0	        0.000000
2	    S	        1	        0.000000
3	    S	        1	        0.333333
4	    Q	        1	        0.500000
...	    ...	       ...	            ...
173	    S	        1	        0.284613
174	    S	        0	        0.290380
175	    C	        1	        0.551065
176	    Q	        1	        0.523050
177	    S	        0	        0.288063
```
- **<span style="color:maroon"> Ordered TSëŠ” ê°™ì€ ì¹´í…Œê³ ë¦¬ ê°’ì´ë¼ í•˜ë”ë¼ë„ ë™ì¼í•œ TSë¡œ ì¹˜í™˜ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.**
- ë¨¼ì € **<span style="color:maroon"> ë°ì´í„°ì— artificial timeì„ ë¶€ì—¬í•˜ê³ , TSë¥¼ ê³„ì‚°í•˜ëŠ” ë° ìˆì–´ì„œ ì´ì „ì— ë‚˜íƒ€ë‚œ ë°ì´í„°ì…‹ë§Œì„ ì´ìš©í•˜ì—¬ ê³„ì‚°í•˜ê¸° ë•Œë¬¸**ì…ë‹ˆë‹¤. ë”°ë¼ì„œ Random Permutationì„ ì—¬ëŸ¬ ë²ˆ ìˆ˜í–‰í•´ì•¼ í•œë‹¤ëŠ” íŠ¹ì§•ì„ ê°€ì§‘ë‹ˆë‹¤.
- ê·¸ëŸ¬ë‚˜ <span style="color:maroon">  íŠœí† ë¦¬ì–¼ì—ì„œëŠ” 1ë²ˆì˜ Permutationë§Œì„ ì„ì˜ë¡œ ìˆ˜í–‰í•˜ì˜€ìŠµë‹ˆë‹¤.


### <span style="background-color:#fff5b1"><span style="color:maroon"> Expectation of TS
ê·¸ë ‡ë‹¤ë©´ **<span style="color:maroon">Train ë°ì´í„°ì˜ TSì˜ ê¸°ëŒ“ê°’ê³¼, Test ë°ì´í„°ì˜ TSì˜ ê¸°ëŒ“ê°’ì€ ì„œë¡œ ë™ì¼í• ê¹Œìš”?** ë‘ ë°©ì‹ì´ Property 1ì„ ë§Œì¡±í•˜ëŠ”ì§€ ê·¸ ì—¬ë¶€ë¥¼ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.
```python
def ts_expectation_binary_clf(df_train: pd.DataFrame, df_test: pd.DataFrame, TS_col:str, y_col:str):
    exp1_train = np.mean(df_train[df_train[y_col] == 0][TS_col])
    exp2_train = np.mean(df_train[df_train[y_col] == 1][TS_col])
    
    exp1_test = np.mean(df_test[df_test[y_col] == 0][TS_col])
    exp2_test = np.mean(df_test[df_test[y_col] == 1][TS_col])
    
    print(f"Expectation of '{TS_col}' when y = 0: ", "[train]", np.round(exp1_train, 3), "[test]", np.round(exp1_test, 3))
    print(f"Expectation of '{TS_col}' when y = 1: ", "[train]", np.round(exp2_train, 3), "[test]", np.round(exp2_test, 3))
```
- ìœ„ëŠ” Binary Classification Taskì— ìˆì–´ TSì˜ ê¸°ëŒ“ê°’ì„ êµ¬í•˜ëŠ” ë°©ì‹ì„ êµ¬í˜„í•œ ê²ƒì…ë‹ˆë‹¤.

ë¨¼ì € **Mean Target Encodingì˜ TS ê¸°ëŒ“ê°’ì„ í™•ì¸**í•´ë³´ê² ìŠµë‹ˆë‹¤.
```python
ts_expectation_binary_clf(df_train_TS, df_test_TS, TS_col='TS', y_col='Survived')
```
```
Expectation of 'TS' when y = 0:  [train] 0.376 [test] 0.341
Expectation of 'TS' when y = 1:  [train] 0.404 [test] 0.407
```
- í™•ì¸ ê²°ê³¼, trainê³¼ test ê°„ TS ê¸°ëŒ“ê°’ì´ ì•½ê°„ì˜ ì°¨ì´ê°€ ìˆìœ¼ë‚˜ ê±°ì˜ ìœ ì‚¬í•œ ê°’ì´ ë‚˜ì˜¤ëŠ” ê²ƒì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤.
- ê¸°ëŒ€ì™€ëŠ” ë‹¤ë¥´ê²Œ, Mean TSëŠ” Property 1ì„ ì–´ëŠ ì •ë„ ë§Œì¡±í•˜ëŠ” ë“¯ í•©ë‹ˆë‹¤.

ë‹¤ìŒìœ¼ë¡œëŠ” **Ordered TSì˜ ê¸°ëŒ“ê°’ì„ í™•ì¸**í—¤ë³´ê² ìŠµë‹ˆë‹¤.
```python
ts_expectation_binary_clf(permuted_train, permuted_test, TS_col='Ordered_TS_Embarked', y_col='Survived')
```
```
Expectation of 'Ordered_TS_Embarked' when y = 0:  [train] 0.369 [test] 0.348
Expectation of 'Ordered_TS_Embarked' when y = 1:  [train] 0.386 [test] 0.417
```
- í™•ì¸ ê²°ê³¼, trainê³¼ test ê°„ TS ê¸°ëŒ“ê°’ì´ ì•½ê°„ì˜ ì°¨ì´ê°€ ìˆê³ , y=0ì¼ ë•ŒëŠ” ê¸°ëŒ“ê°’ì´ ê±°ì˜ ë¹„ìŠ·í•˜ì§€ë§Œ, y=1ì¼ ë•Œì—ëŠ” ê¸°ëŒ“ê°’ì´ ì¡°ê¸ˆ ë‹¬ë¼ì§‘ë‹ˆë‹¤.
- ê¸°ëŒ€ì™€ëŠ” ë‹¤ë¥´ê²Œ, Ordered TSëŠ” Property 1ì„ ë§Œì¡±í•˜ì§€ ëª»í•˜ê³  ìˆìŠµë‹ˆë‹¤.


### <span style="background-color:#fff5b1"><span style="color:maroon"> Evaluate using single Decision Tree
ê·¸ë ‡ë‹¤ë©´ ìœ„ì—ì„œ ë§Œë“  TS ê°’ì„ ê°€ì§€ê³  ê°ê° ì„±ëŠ¥ì„ ë„ì¶œí•´ë³´ê² ìŠµë‹ˆë‹¤. Train ë°ì´í„°ì™€ Test ë°ì´í„° ê°„ **<span style="color:maroon"> TS Expectation ì°¨ì´ê°€ ì¢ì€ Mean Target Encoding ë°©ì‹ì´ Overfittingì„ ë°©ì§€í•˜ì—¬ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆì„ ê²ƒ**ì…ë‹ˆë‹¤.   

```python
# Define Dataset
mean_TS_df = pd.concat([df_train_TS, df_test_TS])
ordered_TS_df = pd.concat([permuted_train, permuted_test])

# Define Single Decision Tree
tree_clf = DecisionTreeClassifier(random_state=2022)
```
- Datasetì€ ì•ì„œ ë§Œë“¤ì—ˆë˜ ê²ƒì„ Train/Testë¥¼ concatí•˜ì—¬ ì‚¬ìš©í•˜ê³ , ì´ë¥¼ Cross Validationì„ í†µí•´ ê²€ì¦í•  ê²ƒì…ë‹ˆë‹¤.
- ëª¨ë¸ì€ Single Decision treeë¥¼ í™œìš©í•˜ë©°, hyperparameterëŠ” default ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

ë¨¼ì € **Mean Target Encoding ë°©ì‹ì—ì„œì˜ ì„±ëŠ¥ì„ í™•ì¸**í•´ë³´ê² ìŠµë‹ˆë‹¤.
```python
mean_acc, mean_auroc = evaluate_model(tree_clf, np.reshape(mean_TS_df.TS.values, (-1, 1)), mean_TS_df.Survived)
```
```
Accuracy: 0.63 (0.04)
AUROC: 0.58 (0.05)
```
ë‹¤ìŒìœ¼ë¡œëŠ” **Ordered Target Statistics ë°©ì‹ì—ì„œì˜ ì„±ëŠ¥ì„ í™•ì¸**í•´ë³´ê² ìŠµë‹ˆë‹¤.
```python
ordered_acc, ordered_auroc = evaluate_model(tree_clf, np.reshape(ordered_TS_df.Ordered_TS_Embarked.values, (-1, 1)), ordered_TS_df.Survived)
```
```
Accuracy: 0.54 (0.04)
AUROC: 0.51 (0.05)
```

<br/>

### Results
|                             | __Mean Target Encoding__| __Ordered Target Statistics__ |
|:---------------------------: |:-----------------------:|:-----------------------------:|
| __Diff in Expectation (y=0)__|  0.035           |   **0.021**   |
| __Diff in Expectation (y=1)__|  **0.003**           |   0.031   |
|      __Mean Accuracy (std)__| **0.63 (0.04)**       | 0.54 (0.04)  |
|         __Mean AUROC (std)__|   **0.58 (0.05)**        |  0.51 (0.05) |


<br/>

### <span style="background-color:#fff5b1">  [ì‹¤í—˜ 9] ê²°ê³¼ í•´ì„
- ë³¸ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” Mean Target Encodingê³¼ Ordered Target Statisticsë¥¼ êµ¬í˜„í•˜ì˜€ìœ¼ë©°, ê°ê°ì˜ TSì— ëŒ€í•œ Expectation ë° ê¸°ë³¸ì ì¸ ì„±ëŠ¥ì„ ë„ì¶œí•˜ì˜€ìŠµë‹ˆë‹¤.   
- ì´ë•Œ **ê¸°<span style="color:maroon">ëŒ€ì™€ëŠ” ë‹¬ë¦¬, Ordered TSì˜ TS ê¸°ëŒ“ê°’ì´ Property 1ì„ ë§Œì¡±í•˜ì§€ ëª»í•¨ê³¼ ë™ì‹œì—, Single Decision Treeë¥¼ í†µí•´ ì„±ëŠ¥ì„ ë„ì¶œí•œ ê²°ê³¼ Accuracyì™€ AUROC ê´€ì ì—ì„œ ëª¨ë‘ ë‚®ì€ ì„±ëŠ¥ì„ ê¸°ë¡**í•˜ì˜€ìŠµë‹ˆë‹¤.
- Property 1ì„ ë§Œì¡±í•˜ì§€ ëª»í•œ ê²ƒì€ Ordered TSì™€ Mean Target Encodingì´ ëª¨ë‘ ë§ˆì°¬ê°€ì§€ì¸ë°, ì´ë•Œ **<span style="color:maroon">ë³¸ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” Random Permutationì„ í•œ ë²ˆë§Œ ìˆ˜í–‰í–ˆìŒì„ ê°ì•ˆ**í•´ì•¼ í•©ë‹ˆë‹¤. **<span style="color:maroon">ì‹¤ì œ CatBoost ì•Œê³ ë¦¬ì¦˜ ë‚´ì—ì„œëŠ” itertation ë§ˆë‹¤, treeë¥¼ ìƒˆë¡œ ìƒì„±í•  ë•Œë§ˆë‹¤ permutationì„ ìˆ˜í–‰í•˜ê³ , ì§€ì†ì ì¸ Inference í™˜ê²½ì„ ì¡°ì„±**í•©ë‹ˆë‹¤.
- ë”ë¶ˆì–´ **<span style="color:maroon">ê¸°ë³¸ì ì¸ ì„±ëŠ¥ì´ ë‚®ì€ ì´ìœ ëŠ” Mean Target Encodingì€ TS ê³„ì‚° ì‹œ í•™ìŠµ ë°ì´í„°ì˜ ëª¨ë“  target ê°’ì„ í•¨ê»˜ í™œìš©í•˜ê³  ìˆê¸° ë•Œë¬¸**ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤. ë³¸ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” **<span style="color:maroon">ê²€ì¦ ì‹œ í•´ë‹¹ í•™ìŠµ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„°ë¥¼ ëª¨ë‘ í•©ì³ Cross Validationì„ ìˆ˜í–‰**í•˜ì˜€ê¸°ì— **<span style="color:maroon">Target ê°’ì„ ì´ë¯¸ ì¶©ë¶„íˆ í™œìš©í•œ Mean Target Encoding ë°©ì‹ì´ ë” ë†’ì€ ì„±ëŠ¥ì„ ê¸°ë¡í•  ìˆ˜ ìˆìœ¼ë¡œ ì¶”ì¸¡**ë©ë‹ˆë‹¤.
- ì¦‰, ë³¸ íŠœí† ë¦¬ì–¼ì—ì„œ í•œ ë²ˆë§Œ ìˆ˜í–‰í–ˆë˜ random permutationì„ ì—¬ëŸ¬ ë²ˆ ìˆ˜í–‰í•˜ê³ , Ordered Boostingì„ í†µí•´ Treeë¥¼ ë§Œë“¤ì–´ë‚˜ê°€ë©´, **<span style="color:maroon"><span style="background-color:#fff5b1">ì‹¤ì œ CatBoost ì•Œê³ ë¦¬ì¦˜ì„ Overfittingì„ ë°©ì§€í•¨ê³¼ ë™ì‹œì— Categorical ë³€ìˆ˜ë„ ì ì ˆíˆ ì²˜ë¦¬í•˜ëŠ” í›Œë¥­í•œ ëª¨ë¸ë¡œì„œ í™œìš©í•  ìˆ˜ ìˆì„ ê²ƒ**ì…ë‹ˆë‹¤.

<br/>

-----
# Appendix
ì§€ê¸ˆê¹Œì§€ëŠ” Baggingê³¼ Boosting ì•Œê³ ë¦¬ì¦˜ì˜ ê¸°ë³¸ê³¼ í•¨ê»˜, ê° ê¸°ë²•ì˜ ëŒ€í‘œ ì•Œê³ ë¦¬ì¦˜ì˜ ë™ì‘ ë°©ì‹ ë° íŠ¹ì§•ì„ ì‹¤í—˜ ë° Python codeë¥¼ í†µí•´ ì•Œì•„ë³´ì•˜ìŠµë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ **Baggingê³¼ Boosting ì¤‘ ì–´ë–¤ ê²ƒì„ ì‚¬ìš©í•´ì•¼ í• ê¹Œìš”?**    

ì¼ë°˜ì ìœ¼ë¡œ **<span style="color:darkblue">Baggingì€ ë‹¨ì¼ ëª¨ë¸ì˜ ì˜ˆì¸¡ ì„±ëŠ¥ì´ ì–´ëŠ ì •ë„ ë³´ì¥ëœ ìƒí™©ì—ì„œ, Overfittingì´ ë¬¸ì œê°€ ë˜ëŠ” ê²½ìš° ì´ë¥¼ í•´ê²°í•˜ëŠ” ë°©ë²•ë¡ ìœ¼ë¡œì„œ ì‚¬ìš©**í•©ë‹ˆë‹¤. ë°˜ë©´ **<span style="color:purple">Boostingì€ Biasë¥¼ ì¤„ì´ë©° ì˜ˆì¸¡ ì„±ëŠ¥ì„ ë†’ì´ê³ ì í•˜ëŠ” ë°©ë²•ë¡ ì´ê¸°ì—, Baggingë³´ë‹¤ëŠ” ì˜ˆì¸¡ ì„±ëŠ¥ì´ ë” ì¢‹ìŠµë‹ˆë‹¤.**   

ê·¸ëŸ¬ë‚˜ ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œëŠ” ì‹œê°í™”ê°€ ëª©ì ì´ ì•„ë‹ˆë¼ë©´, êµ³ì´ ë‹¨ì¼ ëª¨ë¸ì„ ë¨¼ì € ë§Œë“  í›„ Ensemble Learningì„ ìˆ˜í–‰í•˜ëŠ” ê²½ìš°ëŠ” ê±°ì˜ ì—†ê² ì£ . ë”°ë¼ì„œ **<span style="background-color:#fff5b1">Bagging ê¸°ë°˜ì˜ Random Forestë¡œ ë¨¼ì € í•™ìŠµì„ ì§„í–‰í•˜ì—¬ í˜„ì¬ ê°€ì§€ê³  ìˆëŠ” Datasetìœ¼ë¡œ ì–´ëŠ ì •ë„ì˜ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆì„ì§€ ê°€ëŠ í•´ë³´ê³ , ì´í›„ ì‹¤ì œ ëª¨ë¸ì€ Boosting ê³„ì—´ì˜ ëª¨ë¸ë¡œ ìµœì í™” ì‘ì—…ì„ ê±°ì³ ì„±ëŠ¥ì„ ëŒì–´ì˜¬ë¦¬ëŠ” ê²ƒì´ íš¨ìœ¨ì ì¸ í”„ë¡œì íŠ¸ ì§„í–‰ ë°©ë²•**ì…ë‹ˆë‹¤.  

íŠ¹íˆ **CatBoostëŠ” Ordered Boostingì„ í†µí•´ Overfitting ë°©ì§€ì™€ ë”ë¶ˆì–´ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•¨ê³¼ ë™ì‹œì—, Ordered TSë¥¼ í†µí•´ Categorical ë³€ìˆ˜ë¥¼ ë‹¤ë£¨ëŠ” ë°ì—ë„ í›Œë¥­í•œ ë°©ë²•ë¡ **ì…ë‹ˆë‹¤.   

ì´ëŸ¬í•œ ì•Œê³ ë¦¬ì¦˜ì˜ íŠ¹ì§•ì„ ê°ì•ˆí•˜ê³ , ë³¸ íŠœí† ë¦¬ì–¼ì„ í†µí•´ í”„ë¡œì íŠ¸ì—ì„œ ì–´ë–¤ Ensemble Modelë¥¼ íƒí•˜ì—¬ í”„ë¡œì íŠ¸ë¥¼ ìˆ˜í–‰í•  ì§€ ë„ì›€ì´ ë˜ì—ˆìœ¼ë©´ í•©ë‹ˆë‹¤. ğŸ²

----
<br/>

# References
https://ysyblog.tistory.com/220
https://machinelearningmastery.com/bagging-ensemble-with-python/
https://machinelearningmastery.com/gradient-boosting-machine-ensemble-in-python/
https://www.kaggle.com/code/faressayah/xgboost-vs-lightgbm-vs-catboost-vs-adaboost
https://towardsdatascience.com/dealing-with-categorical-variables-by-using-target-encoder-a0f1733a4c69
https://m.blog.naver.com/baek2sm/221771893509
