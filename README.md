
# 2022 Business Analytics Chapter 3: Anomaly DetectionğŸ§
### Python Tutorial: Anomaly Detection Using AutoEncoder with Time Series Data
### 2022010558 ê¹€ì§€í˜„ğŸ²

<br/>

## Table of Contents
- [Anomaly Detection: Overview](#anomaly-detection--overview)
  * [Definition of Anomaly(Novel Data)](#definition-of-anomaly-novel-data-)
  * [Binary Classification vs. Anomaly Detection](#binary-classification-vs-anomaly-detection)
  * [Assumption of Anomaly Detection](#assumption-of-anomaly-detection)
- [Dive into Anomaly Detection Using AutoEncoder with Time Series Data](#dive-into-anomaly-detection-using-autoencoder-with-time-series-data)
  * [AutoEncoder](#autoencoder)
  * [Data](#data)
    + [Description](#description)
    + [Download](#download)
    + [Prepare Data](#prepare-data)
- [Anomaly Detection Using AutoEncoder PyTorch Implementation](#anomaly-detection-using-autoencoder-pytorch-implementation)
  * [AutoEncoder](#autoencoder-1)
  * [Training](#training)
  * [Choosing a Threshold](#choosing-a-threshold)
  * [Evaluation](#evaluation)
- [Experiments](#experiments)
  * [Experiment 1](#experiment-1)
    + [Parameters](#parameters)
    + [LSTM AutoEncoder](#lstm-autoencoder)
    + [Convolution AutoEncoder](#convolution-autoencoder)
    + [Result of Experiment 1](#result-of-experiment-1)
    + [Conclusion of Experiment 1](#conclusion-of-experiment-1)
  * [Experiment 2](#experiment-2)
    + [Add Regularized Loss!](#add-regularized-loss-)
    + [MLP AutoEnocder for Regularized Loss](#mlp-autoenocder-for-regularized-loss)
    + [LSTM AutoEnocder for Regularized Loss](#lstm-autoenocder-for-regularized-loss)
    + [1D Convolution AutoEncoder for Regulaized Loss](#1d-convolution-autoencoder-for-regulaized-loss)
    + [Result of Experiment 2](#result-of-experiment-2)
    + [Conclusion of Experiment 2](#conclusion-of-experiment-2)
- [Appendix](#appendix)
- [References](#references)

<small><i><a href='http://ecotrust-canada.github.io/markdown-toc/'>Table of contents generated with markdown-toc</a></i></small>

------------------

# Anomaly Detection: Overview

## Definition of Anomaly(Novel Data)

<p align="center">
    <img src="Anomaly_Detection_Img/What_is_Anomaly.jpg" width="800"/>
</p>

ì´ë²ˆ íŠœí† ë¦¬ì–¼ì€ 'Anomaly Detection(ì´ìƒì¹˜ íƒì§€)'ì— ëŒ€í•œ ì£¼ì œë¡œ ì§„í–‰í•˜ë ¤ í•©ë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ 'ì´ìƒì¹˜(Anomaly, Novel Data)'ë€ ë¬´ì—‡ì¼ê¹Œìš”? ìœ„ ì‚¬ì§„ì„ ë³´ë©´, ì–‘ìƒì€ ë‹¤ë¥´ì§€ë§Œ ëŒ€ë¶€ë¶„ í˜¸ë‘ì´ ê°ì²´ë°–ì— ì—†ëŠ” ìƒí™©ì—ì„œ ë‹¨ í•˜ë‚˜ ìš©ì— ëŒ€í•œ ê°ì²´ê°€ ìˆë‹¤ë©´, ì´ëŠ” ì¼ë°˜ì ì¸ ê°ì²´ë“¤ë³´ë‹¤ëŠ” ì¡°ê¸ˆ ë‹¤ë¥¸ íŠ¹ì´í•œ ê°ì²´ê°€ ë  ê²ƒì…ë‹ˆë‹¤. ì´ì²˜ëŸ¼ __ì¼ë°˜ì ì¸ ë°ì´í„°ì™€ëŠ” ë‹¤ë¥¸ ì¡°ê¸ˆ íŠ¹ì´í•œ ê°ì²´ë¥¼ Anomaly__ ë¼ê³  í•˜ê³ , ì´ë¥¼ ì°¾ì•„ë‚´ëŠ” ê²ƒì´ ì´ìƒì¹˜ íƒì§€ì˜ ëª©ì ì´ ë©ë‹ˆë‹¤.

ë³´ë‹¤ í•™ìˆ ì ìœ¼ë¡œëŠ”, ì•„ë˜ì™€ ê°™ì´ 'ì´ìƒì¹˜(Anomaly, Novel Data)'ë¥¼ 2ê°€ì§€ ê´€ì ì—ì„œ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.   

1. __ë°ì´í„° ìƒì„± ë©”ì»¤ë‹ˆì¦˜__ ì—ì„œì˜ ê´€ì : ì¼ë°˜ì ì¸ ë°ì´í„°ì™€ëŠ” ë‹¤ë¥¸ ë©”ì»¤ë‹ˆì¦˜ì— ì˜í•´ ë°œìƒí•œ ë°ì´í„°    
"Observations that deviate so much from other observations as to arouse suspicions that they were generated by a different mechanism (Hawkins, 1980)"   

2. __ë°ì´í„° ë¶„í¬ì—ì„œì˜ ë°€ë„__ ì— ëŒ€í•œ ê´€ì : ê°ì²´ë“¤ì´ ë°œìƒí•  ìˆ˜ ìˆëŠ” í™•ë¥  ë°€ë„ê°€ ë‚®ì€ ë°ì´í„°   
â€œInstances that their true probability density is very low (Harmelinget al., 2006)"

<br/>

> __(ì°¸ê³ ) Anomaly vs. Noise__   
> ì´ìƒì¹˜ ë°ì´í„°ëŠ” ë…¸ì´ì¦ˆ ë°ì´í„°ì™€ëŠ” ë‹¤ë¦…ë‹ˆë‹¤. ë…¸ì´ì¦ˆëŠ” ë°ì´í„° ìˆ˜ì§‘ ê³¼ì •ì—ì„œì˜ ë¬´ì‘ìœ„ì„±(Randomness)ì— ê¸°ë°˜í•˜ì—¬ ìì—° ë°œìƒì ìœ¼ë¡œ ë“¤ì–´ê°€ëŠ” ë³€ë™ì„±ì„ ì˜ë¯¸í•˜ë©°, ë”°ë¼ì„œ ë°ì´í„° ë¶„ì„ ì‹œ ì •í™•íˆ ê°€ë ¤ ì§€ì›Œë‚´ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ë°ì´í„°ì— ë…¸ì´ì¦ˆê°€ ë‚´ì¬ë˜ì–´ ìˆë‹¤ëŠ” ê°€ì • í•˜ì— ë¶„ì„ì„ ì§„í–‰í•©ë‹ˆë‹¤.   
>
> ë°˜ë©´ ì´ìƒì¹˜ëŠ” __ë¶„ì„ ê³¼ì •ì—ì„œ ë°˜ë“œì‹œ ì°¾ì•„ë‚´ì•¼ í•˜ëŠ” "í¥ë¯¸ê°€ ìˆëŠ” ê°ì²´"__ ë¼ê³  ì´í•´í•´ì•¼ í•©ë‹ˆë‹¤. ì¼ë°˜ì ì¸ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ì„ ìœ„ë°°í•˜ì—¬ ë§Œë“¤ì–´ì§„ ë°ì´í„°ì´ê¸° ë•Œë¬¸ì—, ì´ë¥¼ ë°œê²¬í•˜ì—¬ __XAIì™€ ê°™ì´ ì‚¬í›„ì ì¸ ë¶„ì„ì„ ìˆ˜í–‰í•´ì•¼ í•  í•„ìš”ê°€ ìˆê³ , ê·¸ ê°ì²´ë¥¼ ì°¾ì•„ë‚´ëŠ” ê²ƒ ìì²´ë§Œìœ¼ë¡œë„ ë¶„ì„ì— ë§¤ìš° ë„ì›€ì´ ë˜ê¸° ë•Œë¬¸__ ì…ë‹ˆë‹¤.   

<br/>

ê·¸ë ‡ë‹¤ë©´ ì´ëŸ° ì˜ë¬¸ì´ ë“¤ ìˆ˜ ìˆê² ì£ . ì´ìƒì¹˜ íƒì§€ë¥¼ í•˜ê³  ì‹¶ì„ ë•Œ, ì •ìƒê³¼ ì´ìƒì„ ë¶„ë¥˜í•´ë‚¼ ìˆ˜ ìˆëŠ” Binary Classification ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í•˜ë©´ ì•ˆ ë˜ëŠ” ê±¸ê¹Œìš”? **ì™œ ìš°ë¦¬ëŠ” Anomaly Detectionì´ë¼ëŠ” ìƒˆë¡œìš´ ë¶„ì•¼ì˜ ì•Œê³ ë¦¬ì¦˜ì„ í•„ìš”ë¡œ í•˜ëŠ” ê±¸ê¹Œìš”?**   

<br/>

## Binary Classification vs. Anomaly Detection
<p align="center">
    <img src="Anomaly_Detection_Img/C_vs_A.jpg" width="800"/>
</p>

ì „í†µì ì¸ ì§€ë„í•™ìŠµ(Supervised Learning) ê´€ì ì—ì„œ Binary Classificationì€ ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ì˜ êµ¬ë¶„í•˜ëŠ” ë¶„ë¥˜ ê²½ê³„ë©´(Decision Boundary)ì„ ì˜ ì°¾ì•„ë‚´ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ í•©ë‹ˆë‹¤. ìœ„ ê·¸ë¦¼ì˜ ì˜ˆë¥¼ ë´…ì‹œë‹¤. ë§Œì¼ í˜¸ë‘ì´ğŸ¯ê°€ Normalì´ê³  ìš©ğŸ²ì´ Abnormal ë°ì´í„°ë¼ê³  í•œë‹¤ë©´, **Binary Classifier**ëŠ” **ìƒˆë¡œìš´ ë°ì´í„° "A"ë¥¼** ë¬´ì—‡ìœ¼ë¡œ íŒë‹¨í• ê¹Œìš”? Normal Classì˜ ê²½ê³„ë©´ ì•ˆìª½ì— ì†í•´ìˆê¸° ë•Œë¬¸ì— **Normalì´ë¼ê³  íŒë‹¨**í•  ê²ƒì…ë‹ˆë‹¤.    

ì´ë•Œ ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ ì§ˆë¬¸ì„ ë˜ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. **AbnormalğŸ² ê´€ì¸¡ì¹˜ê°€ ì ˆëŒ€ì ìœ¼ë¡œ ì ì€ ìƒí™©ì—ì„œ, í•´ë‹¹ ë°ì´í„°ê°€ Abnormal Classë¥¼ ëŒ€í‘œí•  ìˆ˜ ìˆëŠ”ê°€?"** ì— ëŒ€í•´ ë§ì…ë‹ˆë‹¤. 
ì™œëƒí•˜ë©´ ì ì€ ê´€ì¸¡ì¹˜ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒì„±ëœ ë¶„ë¥˜ ê²½ê³„ë©´ì„ ì˜¨ì „íˆ ì‹ ë¢°í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì´ì£ . ìƒê°í•´ë³´ë‹ˆ ìƒˆë¡œìš´ ë°ì´í„° "A"ëŠ” 'Normal'ì´ë¼ê³  íŒë‹¨í•˜ê¸°ì—ëŠ” ë‹¤ë¥¸ ì •ìƒ ê´€ì¸¡ì¹˜ë“¤(ğŸ¯)ê³¼ëŠ” Feature Space ìƒì˜ ê±°ë¦¬ê°€ ê½¤ ë¨¼ ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
 
ë°”ë¡œ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ ë°”íƒ•ìœ¼ë¡œ, Anomaly Detectionì€ **Normal ë°ì´í„°ëŠ” ì¶©ë¶„íˆ ë§ìœ¼ë‹ˆ, "ì •ìƒ"ì´ë€ ë¬´ì—‡ì¸ê°€ë¥¼ ì •ì˜í•˜ì—¬ ì´ì— í¬í•¨ë˜ì§€ ì•ŠëŠ” ê²ƒì„ ê±¸ëŸ¬ë‚´ë³´ì**ëŠ” ì•„ì´ë””ì–´ì—ì„œ ì¶œë°œí•˜ê²Œ ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê´€ì ì—ì„œ **ìƒˆë¡œìš´ ë°ì´í„° "A"** ëŠ” Anomaly Detectionì˜ ê´€ì ì—ì„œëŠ” **Normalì´ ì•„ë‹Œ ê²ƒ**ìœ¼ë¡œ ì¬ì •ì˜ë˜ê² ì£ .   

> **(ì°¸ê³ )**   
> Normalì´ ì•„ë‹ˆë¼ê³  í•´ì„œ ë°ì´í„°ê°€ ë°˜ë“œì‹œ "Abnormal"ì¸ ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤. í•˜ì§€ë§Œ í˜„ ê´€ì¸¡ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë³´ì•˜ì„ ë•Œ í•´ë‹¹ ë°ì´í„°ëŠ” ìš°ë¦¬ê°€ ê·œì •í•œ "Normal"ì´ë¼ í•˜ê¸°ì—ëŠ” ì–´ë ¤ì›€ì´ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì ‘ê·¼ë²•ì´ ìœ íš¨í•œ ì´ìœ ëŠ” "Abnormal"ì˜ ì¢…ë¥˜ê°€ 2ê°€ì§€ì¼ ë•Œì…ë‹ˆë‹¤. ê°€ìš©í•  ìˆ˜ ìˆëŠ” ë¶ˆëŸ‰ ê´€ì¸¡ì¹˜ê°€ í•˜ë‚˜ì˜ ìœ í˜•(ì´ë¥¼í…Œë©´ ğŸ²)ë°–ì— ì—†ëŠ” ìƒí™©ì—ì„œ, í•œ ë²ˆë„ ê´€ì¸¡ë˜ì§€ ëª»í–ˆë˜ ìƒˆë¡œìš´ ë¶ˆëŸ‰ ìœ í˜•(ì´ë¥¼í…Œë©´ ğŸ¦)ì„ ë°œê²¬í•  ìˆ˜ ìˆë‹¤ëŠ” ì´ì ì„ ê°€ì§€ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
> 
> ì´ì²˜ëŸ¼ Classificationì€ ë°ì´í„°ë¥¼ ë°˜ë“œì‹œ ì£¼ì–´ì§„ ë²”ì£¼ ì¤‘ í•˜ë‚˜ë¡œ í• ë‹¹í•˜ì§€ë§Œ, Anomaly Detectionì€ ì •ìƒì˜ Boundaryë¥¼ ì •í•˜ê³  'ì •ìƒ' 'ì •ìƒì´ ì•„ë‹˜'ìœ¼ë¡œ ë³´ë‹¤ ê´‘ë²”ìœ„í•œ ë¶„ë¥˜ê°€ ê°€ëŠ¥í•˜ë‹¤ëŠ” ì¸¡ë©´ì—ì„œ ë‘˜ì€ ì°¨ì´ë¥¼ ê°€ì§‘ë‹ˆë‹¤. ì¦‰, Anomaly Detectionì€ ì¼ë°˜ì ì¸ ë°ì´í„°ì˜ ë²”ì£¼ì™€ í™•ì—°íˆ êµ¬ë¶„ë˜ëŠ” ë‹¤ë¥¸ ì–‘ìƒì˜ ë°ì´í„°ë¥¼ ì°¾ëŠ” ê²ƒì´ì£ . ì´ëŸ¬í•œ ê´€ì ì—ì„œ Anomaly Detectionì„ í†µí•´ ì°¾ì•„ë‚¸ ë°ì´í„°ëŠ” ë¶ˆëŸ‰, ì˜¤ë¥˜, ì•…ì„±ì½”ë“œ, ê°€ì§œ ë°ì´í„°ì¼ ìˆ˜ë„ ìˆì§€ë§Œ, ì˜ˆì™¸, ìƒˆë¡œìš´ íŒ¨í„´ ë“±ì˜ ë°ì´í„°ì¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

<br/>

## Assumption of Anomaly Detection
ì´ìƒì¹˜ ë°ì´í„°ì˜ ê¸°ë³¸ ê°€ì •ì€ **ì •ìƒ ë°ì´í„°ê°€ ì •ìƒì´ ì•„ë‹Œ ë°ì´í„°ë³´ë‹¤ í›¨ì”¬ ë” ë§ë‹¤**(There are considerably more "normal" observations than abnormal observations in the data)ì…ë‹ˆë‹¤. ê·¸ë¦¬ê³  ëª¨ë¸ë§ ì‹œì—ëŠ” **ì˜¤ë¡œì§€ ì •ìƒ ë°ì´í„°ë§Œì„ í™œìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµ**ì‹œí‚¨ í›„, ì •ìƒ ë²”ìœ„ë¥¼ ì •ì˜í•˜ì—¬ ì´ìƒì„ íƒì§€í•´ë‚¸ë‹¤ëŠ” ê°€ì •ì„ ê¸°ì €ë¡œ í™œìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤.   

<br/>

-------

# Dive into Anomaly Detection Using AutoEncoder with Time Series Data
ì´ëŸ¬í•œ ì ì„ ë°”íƒ•ìœ¼ë¡œ, ë³¸ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” Anomaly Detectionì— ëŒ€í•´ì„œ, ê·¸ ì¤‘ì—ì„œë„ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ë°”íƒ•ìœ¼ë¡œ ìˆ˜í–‰ë˜ëŠ” **AutoEncoder Based Anomaly Detection**ì— ëŒ€í•œ ê°„ëµí•œ ì†Œê°œ ë° í™œìš© ë°©ì•ˆì— ëŒ€í•´ ì†Œê°œí•´ë³´ê³ ì í•©ë‹ˆë‹¤. ì´ë•Œ **ë”¥ëŸ¬ë‹ ê¸°ë²•ì„ í™œìš©í•œ ëª¨ë¸ ê¸°ë°˜ì˜ Anomaly Detectionì„ ì†Œê°œí•˜ëŠ” ì´ìœ ëŠ”, ë°ì´í„°ê°€ ë‚´í¬í•˜ëŠ” ì •ë³´ë¥¼ ìµœëŒ€í•œ í™œìš©í•˜ëŠ” ë™ì‹œì— êµ¬ì¡°ë¥¼ ìœ ì—°í•˜ê²Œ ë°”ê¾¸ì–´ê°€ë©° ëª¨ë¸ ì„±ëŠ¥ì„ ë†’ì¼ ìˆ˜ ìˆëŠ” ì¥ì ì„ ê°€ì§€ê¸° ë•Œë¬¸**ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ë§Œí¼ì˜ ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆë‹¤ë©´ ë³¸ íŠœí† ë¦¬ì–¼ì„ í†µí•´ ë”¥ëŸ¬ë‹, ê·¸ ì¤‘ì—ì„œë„ AutoEncoderë¥¼ ì´ìš©í•œ Anomaly Detectionì˜ ê¸°ë³¸ì„ ì•Œì•„ê°ˆ ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.

íŠ¹íˆ ë³¸ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” **ì‹œê³„ì—´ ë°ì´í„°ì— ìˆì–´ ì´ìƒ íƒì§€ì— ëŒ€í•œ íŠœí† ë¦¬ì–¼**ì„ ì‘ì„±í•˜ê³ ì í•˜ë©°, í™œìš©í•˜ê³ ì í•˜ëŠ” ë°ì´í„°ëŠ” **Real-World Time Series Dataset ì¤‘ Electrocardiogram(ECG) ì‹¬ì „ë„ ë°ì´í„°ì…‹**ì…ë‹ˆë‹¤. 

## AutoEncoder
<p align="center">
    <img src="Anomaly_Detection_Img/AE.png" width="800"/>
</p>

AutoEncoderëŠ” í†µìƒì ìœ¼ë¡œ ìœ„ì™€ ê°™ì€ êµ¬ì¡°ë¥¼ ê°€ì§€ëŠ” ëª¨ë¸ë¡œ, **ì…ë ¥ê³¼ ì¶œë ¥ì´ ë™ì¼í•œ ì¸ê³µ ì‹ ê²½ë§ êµ¬ì¡°**ë¥¼ ë°”íƒ•ìœ¼ë¡œ **ì…ë ¥ìœ¼ë¡œ ë°›ì€ ë°ì´í„°(x)ë¥¼ ì¶œë ¥ìœ¼ë¡œ ìµœëŒ€í•œ ê°€ê¹ê²Œ ì¬êµ¬ì¶•(x')í•˜ëŠ” ê²ƒì„ ëª©ì **ìœ¼ë¡œ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë¶„ì„ ë°ì´í„°ì…‹ì´ ì–´ë–¤ í˜•íƒœì¸ì§€(e.g. Time Series, Image, NLP, etc.)ì— ë”°ë¼ì„œ ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ ëª¨ë¸ì„ ì„¤ê³„í•  ìˆ˜ ìˆì§€ë§Œ, ê¸°ë³¸ì ì¸ êµ¬ì¡°ëŠ” ëª¨ë‘ ë™ì¼í•©ë‹ˆë‹¤.   

- **Encoder**: ì£¼ì–´ì§„ Inputì„ ì°¨ì› ì¶•ì†Œë¥¼ í†µí•´ Latent Vector(= Hidden Representation)ë¡œ ë³€í™˜í•˜ëŠ”ë°, ì´ë•Œ ì›ë³¸ ë°ì´í„°ì˜ ì •ë³´ë¥¼ ì˜ ë³´ì¡´í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì••ì¶•í•˜ëŠ” ê²ƒì´ Enocderì˜ ëª©ì ì…ë‹ˆë‹¤.   
- **Decoder**: ì¼ì¢…ì˜ Generatorë¡œì„œ, Inputìœ¼ë¡œ ë“¤ì–´ì˜¨ Latent Vectorë¥¼ ì´ìš©í•˜ì—¬ ë‹¤ì‹œ Inputê³¼ ìœ ì‚¬í•˜ê²Œ ë°ì´í„°ë¥¼ ì¬êµ¬ì¶•(Reconstruction)í•˜ëŠ” ì—­í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.   
- **Objective Function**: $l(f(x)) = \frac{1}{2}\sum_{k}(\hat{x_k}-x_k)^2$   
    - **ì‹¤ì œ Inputê³¼ ì¬êµ¬ì¶•í•œ Output ê°„ì˜ ì°¨ì´ë¥¼ ìµœì†Œí™” í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ëª¨ë¸ Parametersê°€ ì—…ë°ì´íŠ¸** ë˜ë„ë¡ í•¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì¦‰, ì¬êµ¬ì¶•ëœ Output(x')ì™€ Input(x) ê°„ì˜ ì°¨ì´ë¥¼ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµì´ ì´ë£¨ì–´ì§€ëŠ” ê²ƒì´ì£ .   
    - ì´ë¥¼ **Reconstruction Error**ë¼ê³ ë„ í•©ë‹ˆë‹¤.   

<br/>

**ê·¸ë ‡ë‹¤ë©´ ì–´ë–»ê²Œ ìê¸° ìì‹ ì„ ì…ë ¥ìœ¼ë¡œ ë„£ì–´ ë‹¤ì‹œ ì¬êµ¬ì¶•í•˜ëŠ” ê²ƒì´ ì´ìƒì¹˜ íƒì§€ê°€ ë˜ëŠ” ê±¸ê¹Œìš”?**   
- í•™ìŠµ ì‹œ Normal ë°ì´í„°ë§Œì„ í™œìš©í•œë‹¤ë©´, **AutoEncoderëŠ” Normal ê°’ì„ ê°€ì§€ëŠ” ë°ì´í„°ì˜ ì£¼ìš”í•œ íŠ¹ì§•ë“¤ì„ ì˜ íŒŒì•…í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµì´ ì´ë£¨ì–´ì§ˆ ê²ƒ**ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ì œëŒ€ë¡œ í•™ìŠµëœ ëª¨ë¸ì€ ì •ìƒì ì¸ ë°ì´í„°ì— ëŒ€í•´ì„œëŠ” Reconstructionì„ í›Œë¥­í•˜ê²Œ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ê·¸ëŸ¬ë‚˜ ë§Œì¼ Anomaly ë°ì´í„°ê°€ ë“¤ì–´ì˜¨ë‹¤ë©´, í•™ìŠµ ì‹œ í•œ ë²ˆë„ Anomlayë¥¼ ë³´ì§€ ëª»í–ˆë˜ AutoEncoderëŠ” Recosntructionì„ ì˜ ìˆ˜í–‰í•˜ì§€ ëª»í•  ê²ƒì…ë‹ˆë‹¤. ì˜¤ë¡œì§€ **Normalì˜ íŠ¹ì§•ë§Œì„ í•™ìŠµí–ˆê¸° ë•Œë¬¸ì—, ì •ìƒì´ ì•„ë‹Œ íŠ¹ì§•ì„ ê°€ì§„ ë°ì´í„°ì— ëŒ€í•´ì„œëŠ” Reconstructionì— ì–´ë ¤ì›€ì„ ê²ªëŠ” ê²ƒ**ì´ì£ .
- ë”°ë¼ì„œ ìœ„ ëª©ì  í•¨ìˆ˜ì—ì„œ Reconstruction Error ê°’ì´ ì¼ì • ìˆ˜ì¤€(Threshold) ì´ìƒ ë†’ì•„ì§„ë‹¤ë©´, ì´ë¥¼ Anomalyë¼ íŒë‹¨í•  ìˆ˜ ìˆëŠ” ê²ƒì…ë‹ˆë‹¤.
    - ê·¸ë ‡ë‹¤ë©´ ì´ 'Threshold'ëŠ” ì–´ë–»ê²Œ ì •ì˜í•´ì•¼ í• ê°€ìš”? ê·¸ ê¸°ì¤€ì€ ë°ì´í„°ì…‹ë§ˆë‹¤ ê·¸ë¦¬ê³  ë„ë©”ì¸ë§ˆë‹¤ ë‹¤ë¥´ë¯€ë¡œ, ë³¸ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” **ì‹¤í—˜ì„ í†µí•´ ì ì ˆí•œ Thresholdë¥¼ ì •í•˜ëŠ” ê³¼ì •ì„ ì•„ë˜ì— ì†Œê°œ**í•˜ê³ ì í•©ë‹ˆë‹¤.

<br/>

## Data
### Description
ë³¸ íŠœí† ë¦¬ì–¼ì—ì„œ ë‹¤ë£° ë°ì´í„°ëŠ” ECG(Electrocardiogram), ì¦‰ ì‹¬ì „ë„ ë°ì´í„°ì…ë‹ˆë‹¤. ì‹¬ì „ë„ë€ ì‹¬ì¥ì˜ ì „ê¸°ì  í™œë™ì„ ì‹ ì²´ í‘œë©´ì—ì„œ ì¸¡ì •í•œ ì „ê¸°ì  ì‹ í˜¸ë¥¼ ì˜ë¯¸í•˜ë©°, ì´ë¥¼ í†µí•´ ë¶€ì •ë§¥ ë“± ì‹¬ì¥ ì§ˆí™˜ ì—¬ë¶€ë¥¼ íŒŒì•…í•´ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë³¸ íŠœí† ë¦¬ì–¼ì—ì„œ ì‚¬ìš©í•œ ë°ì´í„°ì…‹ì€ ì´ 2ê°€ì§€ì´ë©°, [MIT-BIH Arrhythmia Database](https://physionet.org/content/mitdb/1.0.0/)ì™€ [PTB Diagnostic ECG Database](https://www.physionet.org/content/ptbdb/1.0.0/)ê°€ ê·¸ê²ƒì…ë‹ˆë‹¤. MIT-BIHëŠ” ì‹¬ë¶€ì „ì¦ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ê¸° ìœ„í•´ ì´ 47ëª…ì˜ í”¼í—˜ìë¡œë¶€í„° ìˆ˜ì§‘í•œ ì‹¬ì „ë„ ë°ì´í„°ì…‹ìœ¼ë¡œ, ì´ 5ê°œì˜ Classë¡œ êµ¬ë¶„ë©ë‹ˆë‹¤. PTBëŠ” 290ëª…ì˜ í”¼í—˜ìë¡œë¶€í„° ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì˜€ìœ¼ë©°, 290ëª… ì¤‘ 52ëª…ì˜ í”¼í—˜ìë§Œì´ Normal Classë¥¼ ê°€ì§‘ë‹ˆë‹¤.   
ë³¸ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” ë‘ ë°ì´í„°ì…‹ì„ í•©ì³ Normalê³¼ Abnormalë¡œ êµ¬ë¶„í•˜ì˜€ê³ , Normalì˜ ê²½ìš° PTB ë°ì´í„°ì—ì„œ 52ëª…ì˜ ê±´ê°•í•œ í”¼í—˜ìì˜ ê¸°ë¡ì„ ì‚¬ìš©í•˜ì˜€ìœ¼ë©°, MIT-BIHì˜ í™˜ì ë°ì´í„°ì™€ PTB ë°ì´í„°ì˜ 238ê°œì˜ í™˜ì ë°ì´í„° ê¸°ë¡ì„ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.

> __[(ì°¸ê³ )](https://www.heartandstroke.ca/heart-disease/tests/electrocardiogram)__   
> **ECG**: "An electrocardiogram (ECG or EKG) is a test that checks how your heart is functioning by measuring the electrical activity of the heart. With each heart beat, an electrical impulse (or wave) travels through your heart. This wave causes the muscle to squeeze and pump blood from the heart."

ECGì˜ Normal/Abnormal ì—¬ë¶€ëŠ” ì •ìƒì ì¸ ì‹¬ì¥ ë°•ë™ì¸ì§€, ë¶€ì •ë§¥ê³¼ ê´€ë ¨í•œ ì‹¬ì¥ ë°•ë™ì¸ì§€ë¥¼ êµ¬ë¶„í•˜ëŠ” ê²ƒìœ¼ë¡œ ì •ì˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë•Œ Abnormal ì¦‰ ë¶€ì •ë§¥ì˜ ì¢…ë¥˜ëŠ” ì—¬ëŸ¬ ê°œì´ì§€ë§Œ, 
**ë³¸ íŠœí† ë¦¬ì–¼ì´ Anomaly Detectionì— ëŒ€í•œ ê²ƒì„ì„ ê³ ë ¤í•˜ì—¬ì„œ Normalì´ ì•„ë‹Œ ClassëŠ” ëª¨ë‘  Anomalyë¡œ ì •ì˜**í•©ë‹ˆë‹¤. ì´í›„ Normal(N)ì´ ì•„ë‹Œ Abnormalë¥¼ ê²€ì¶œí•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì„ ì„¤ê³„í•´ë³´ê² ìŠµë‹ˆë‹¤.

### Download
[í•´ë‹¹ ë§í¬](https://github.com/ashukid/anomaly-detection-in-ecg-signal/tree/master/input/ecg-data-mit-arrhythmia-ptb)ë¥¼ í†µí•´ì„œ ë°ì´í„°ë¥¼ ë°”ë¡œ ë‹¤ìš´ ë°›ìœ¼ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### Prepare Data
ë°ì´í„°ë¥¼ ë‹¤ìš´ ë°›ê³  ì´ìƒì¹˜ íƒì§€ì— í•„ìš”í•œ ë°ì´í„°ë¥¼ ì¤€ë¹„í•´ë³´ê² ìŠµë‹ˆë‹¤. ì´ë•Œ í•„ìš”í•œ **Hyperparameter**ëŠ” **Train/Validation/Test ê³¼ì •ì—ì„œ í™œìš©í•  Normal ë°ì´í„°ì˜ ë¹„ìœ¨**ì…ë‹ˆë‹¤. ë”ë¶ˆì–´ **Validation ê³¼ì •ì—ì„œ í•„ìš”í•œ Abnormal ë°ì´í„°ì˜ ë¹„ìœ¨** ì—­ì‹œ ì •ì˜í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤. ë³´ë‹¤ ìì„¸í•œ HyperparameterëŠ” [Parameters](#parameters)ì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

__Example of Use__
```python
import os, glob, sys, time, random, shutil, pickle, tqdm
import numpy as np
import pandas as pd
from colorama import Fore
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score

import torch
from torch import nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

from Utils.saver import Saver
from Utils.logger import get_tqdm_config
from Utils.earlystopping import EarlyStopping

# For visualiation
%matplotlib inline
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_context("talk")
sns.set_style("white")
sns.set_palette("Pastel1")
plt.rcParams['axes.labelsize'] = 14
plt.rcParams['xtick.labelsize'] = 14
plt.rcParams['ytick.labelsize'] = 14
matplotlib.rc('font', family='Malgun Gothic')  # For Windows
plt.rcParams['axes.unicode_minus'] = False

import warnings
warnings.filterwarnings(action='ignore')

# Seed ê³ ì •
def set_seed(seed=2022):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
set_seed()
```
- í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  Seedë¥¼ ê³ ì •ì‹œì¼°ìœ¼ë‹ˆ, ì´ì œ ë¶„ì„ì—ì„œ ì‚¬ìš©í•  ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  ì •ì˜í•´ë³´ê² ìŠµë‹ˆë‹¤.

- `Utils/`ì— ë“¤ì–´ê°€ì‹œë©´ í•™ìŠµì— í•„ìš”í•œ ëª¨ë“ˆë“¤ì„ ì¶”ê°€ì ìœ¼ë¡œ ì •ì˜í•´ ë†“ì•˜ìŠµë‹ˆë‹¤.

```python
class ECG_TrainDataset(Dataset):
    
    def __init__(self, data_dir):
        self.data=pd.read_csv(data_dir, header=None)
        
    def __getitem__(self,idx):
        x = self.data.loc[idx,:186].values  # Label ì œê±°
        return x
    
    def __len__(self):
        return len(self.data)
```
```python
class ECG_TestDataset(Dataset):
    
    def __init__(self,path,disease_type):
        self.data=pd.read_csv(path,header=None)
        
        # All values of specific disease (ëª¨ë“  ë¶€ì •ë§¥ í™˜ìë¥¼ Anomalyë¡œ ì •ì˜)
        if(disease_type != -1):
            self.data = self.data[self.data[config.seq_len]==disease_type].values[:, :config.seq_len]
        
        # Equal values from all disease
        else:
            how_many=20
            temp=np.zeros(shape=(6 * how_many,config.seq_len))
            for i in range(6):
                temp[i*how_many:(i+1)*how_many,:]=self.data[self.data[config.seq_len]==i].values[:how_many,:config.seq_len]
                
            self.data=temp
          
        print(self.data.shape)
        
    def __getitem__(self,idx):
        x=self.data[idx,:config.seq_len] # Label ì œê±°
        return x
    
    def __len__(self):
        return len(self.data)
```
```python
data_dir = './Data/'
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

class dataset_config:
    # Dataloader
    batch_size=64
    workers=0
    lr=1e-3
    
    # Dataset
    normal_train_path = data_dir+"normal_train.csv"
    normal_valid_path = data_dir+"normal_valid.csv"
    patient_valid_path = data_dir+"patient_valid.csv"
```
```python
# Define train/valid/testloader
normal_train = ECG_TrainDataset(dataset_config.normal_train_path)
train_loader = DataLoader(normal_train, batch_size=dataset_config.batch_size, shuffle=True, num_workers=dataset_config.workers)

normal_test = ECG_TestDataset(dataset_config.normal_valid_path, 10)
nor_testloader = DataLoader(normal_test, batch_size=1, shuffle=False, num_workers=dataset_config.workers)

abnormal_test = ECG_TestDataset(dataset_config.patient_valid_path, -1)
ab_testloader = DataLoader(abnormal_test, batch_size=1, shuffle=False, num_workers=dataset_config.workers)
```

```python
for x in train_loader:
    print(x.shape, x.dtype)
    x = x.numpy()
    x = pd.DataFrame(x)
    x.iloc[:10,:].T.plot(figsize=(20,8))
    plt.show()
    break
```
<p align="center">
    <img src="Anomaly_Detection_Img/output.png" width="800"/>
</p>


- ìœ„ ê·¸ë¦¼ì€ Trainloader ë‚´ì˜ ì •ìƒ ECG ë°ì´í„° ì¤‘ ì¼ë¶€ ë°ì´í„°ì…‹ì„ ì‹œê°í™” í•œ ê²ƒì…ë‹ˆë‹¤. **ìš°ë¦¬ê°€ ì •ì˜í•œ Trainloaderê°€ ì œëŒ€ë¡œ ë™ì‘í•˜ëŠ” ê²ƒì„ í™•ì¸í•˜ì˜€ìœ¼ë‹ˆ, ì´ì œ ë³¸ê²©ì ìœ¼ë¡œ ëª¨ë¸ í•™ìŠµì„ ìˆ˜í–‰**í•´ë³´ê² ìŠµë‹ˆë‹¤.

<br/>

----

<br/>

# Anomaly Detection Using AutoEncoder PyTorch Implementation
ì´ì œ ë³¸ê²©ì ìœ¼ë¡œ PyTorchë¥¼ ì´ìš©í•˜ì—¬ AutoEncoderë¥¼ êµ¬ì¶•í•˜ê³ , Normal ë°ì´í„°ë§Œì„ í™œìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµ í›„ ì´ìƒì¹˜ íƒì§€ ê²°ê³¼ë¥¼ ì•Œì•„ë³´ë ¤í•©ë‹ˆë‹¤. **ë¨¼ì € ê°€ì¥ ê¸°ë³¸ì ì¸ Multi-Layer Perceptron(MLP)ë¥¼ ì´ìš©í•œ AutoEncoderë¥¼ êµ¬ì¶•**í•´ë³´ê² ìŠµë‹ˆë‹¤.

## AutoEncoder
```python
class model_config:
    seq_len = 187
    hidden_1 = 500
    hidden_2 = 100
```
- ëª¨ë¸ í•™ìŠµì— í•„ìš”í•œ Hyperparameter ëª©ë¡ì…ë‹ˆë‹¤. ì´ë•Œ seq_lenì€ í•˜ë‚˜ì˜ ECG signalì˜ ë°ì´í„°ê°€ ê°€ì§€ëŠ” pointì˜ ê°œìˆ˜ë¥¼ ì˜ë¯¸í•˜ë©°, hidden_# ì€ ê°ê° ëª¨ë¸ì˜ MLP ë…¸ë“œì˜ ê°œìˆ˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ì¦‰, AutoEncoder Bottelneck Layersë¥¼ ì •ì˜í•˜ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤.
```python
class MLPAutoEncoder(nn.Module):
    def __init__(self, config: object):
        super().__init__()
        self.seq_len = config.seq_len
        self.hidden_1 = config.hidden_1
        self.hidden_2 = config.hidden_2
        self.hidden_3 = config.hidden_3
        
        self.Encoder=nn.Sequential(
            nn.Linear(self.seq_len, self.hidden_1),
            nn.ReLU(inplace=True),
            
            nn.Linear(self.hidden_1, self.hidden_2),
            nn.Softmax(dim=1),
            
            nn.Linear(self.hidden_2, self.hidden_3),
            nn.Softmax(dim=1),
        )
        
        self.Decoder=nn.Sequential(
            
            nn.Linear(self.hidden_3, self.hidden_2),
            nn.ReLU(inplace=True),
            
            nn.Linear(self.hidden_2, self.hidden_1),
            nn.ReLU(inplace=True),
            
            nn.Linear(self.hidden_1, self.seq_len),
            nn.Sigmoid()
        )
        
    def forward(self, x):
        x = x.view(-1, self.seq_len)
        latent_vector=self.Encoder(x)
        pred = self.Decoder(latent_vector)
        
        return pred.view(-1, self.seq_len)       
```
```python
print(MLPAutoEncoder(model_config))
```
```
MLPAutoEncoder(
  (Encoder): Sequential(
    (0): Linear(in_features=187, out_features=500, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=500, out_features=100, bias=True)
    (3): Softmax(dim=1)
    (4): Linear(in_features=100, out_features=32, bias=True)
    (5): Softmax(dim=1)
  )
  (Decoder): Sequential(
    (0): Linear(in_features=32, out_features=100, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=100, out_features=500, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=500, out_features=187, bias=True)
    (5): Sigmoid()
  )
)
```
- êµ¬ì¶•í•œ ëª¨ë¸ì˜ êµ¬ì¡°ëŠ” ìœ„ì™€ ê°™ìŠµë‹ˆë‹¤. ì•ì„œ ì–¸ê¸‰í•œ ë°”ì™€ ê°™ì´ **Encoderì™€ DecoderëŠ” ëŒ€ì¹­ êµ¬ì¡°ë¥¼ ì´ë£¨ê³  ìˆìŠµë‹ˆë‹¤**. ì, ì´ì œ ëª¨ë¸ í•™ìŠµ í›„ í•™ìŠµ ê²°ê³¼ë¥¼ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤.

## Training
```python
class train_config:
    lr = 1e-3
    weight_decay = 1e-2
    epochs = 70
```
- í•™ìŠµì— í•„ìš”í•œ Hyperparameterë¥¼ ì •ì˜í•˜ì˜€ìŠµë‹ˆë‹¤. **ì¬êµ¬ì¶• ì˜¤ì°¨ëŠ” MSE Lossë¥¼ ì´ìš©**í•©ë‹ˆë‹¤.
```python
def loss_function(pred, real):
    mse = F.mse_loss(pred, real, reduction="sum")
    return mses
```

```python
class ADTrainer(object):
    def __init__(self, config: object, model, check_path: str):
        self.check_path = check_path
        
        # Define Dataloader
        self.train_loader, self.valid_loader = train_loader, nor_validloader
        
        # Define Saver
        self.saver = Saver(path = check_path)
        
        # Define logging train history
        self.history = {'loss': {'train': [], 'valid': []}}
        
        # Define Model
        self.model = model.to(config.device)
        
        # Define Optimizer
        self.optimizer = torch.optim.Adam(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)
        
        
    def save_and_plotting_training_history(self):
            
        with open(os.path.join(self.check_path,'Anomaly_Detection_History.pkl'), 'wb') as f:
            pickle.dump(self.history, f)

        train_mse_loss = self.history['loss']['train']
        plt.plot(train_mse_loss,label='train')
        plt.plot(self.history['loss']['valid'],label='valid')
        plt.title("Reconstruction Loss")
        plt.legend()
        plt.savefig(os.path.join(self.check_path, 'Reconstruction_loss.png'))
        plt.close('all')
        
        
    def run(self):
        
        # EarlyStopping moduler
        earlystopping = EarlyStopping(patience=config.early_stop_patience, path = self.check_path)
        
        with tqdm.tqdm(**get_tqdm_config(total=config.epochs, leave=True, color='red')) as pbar:
            
            valid_best_mse = float('inf')
            
            for epoch in range(1, config.epochs+1):
                
                # train & Valid
                self.history['loss']['train'].append(self.train(self.train_loader))
                self.history['loss']['valid'].append(self.evaluate(self.valid_loader))
                
                # Logging
                desc = f" Epoch [{epoch:>04}/{config.epochs:>04} |"
                for metric_name, metric_dict in self.history.items():
                    for k, v in metric_dict.items():
                        desc += f" {k}_{metric_name}: {v[-1]:.4f} |"
                    
                # Save model if it is the current best
                valid_mse = self.history['loss']['valid'][-1]
                if valid_mse < valid_best_mse:
                    valid_best_mse = valid_mse
                    self.saver.checkpoint('best_model', self.model, is_best=False)
                    
                earlystopping(self.history['loss']['valid'][-1], self.model)
                if earlystopping.early_stop:
                    break
                
                pbar.set_description_str(desc)
                pbar.update(1)
                
        self.save_and_plotting_training_history()
        
    def _set_learning_phase(self, train: bool = False):
        if train:
            self.model.train()
        else:
            self.model.eval()
        
        
    def train(self, train_loader):
        steps_per_epoch = len(train_loader)
        self._set_learning_phase(True)
        result = {'loss': torch.zeros(steps_per_epoch, device=config.device)}
        
        with tqdm.tqdm(**get_tqdm_config(total=steps_per_epoch, leave=False, color='cyan')) as pbar:
            for i, signal in enumerate(train_loader):
                signal = signal.to(config.device).float()
                
                preds = self.model(signal)
                loss = loss_function(preds, signal)
                
                self.optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), config.optim_clip_value)
                self.optimizer.step()
                
                # For Save Information
                result['loss'][i] = loss.item()
                
                desc = f" Batch [{i + 1:>04}/{len(self.train_loader):>04}"
                pbar.set_description_str(desc)
                pbar.update(1)
                
        return result['loss'].mean().item()
    
    
    @torch.no_grad()
    def evaluate(self, valid_loader):
        steps_per_epoch = len(valid_loader)
        self._set_learning_phase(False)
        result = {'loss': torch.zeros(steps_per_epoch, device=config.device)}
        
        with tqdm.tqdm(**get_tqdm_config(total=steps_per_epoch, leave=False, color='yellow')) as pbar:
            for i, signal in enumerate(valid_loader):
                signal = signal.to(config.device).float()
                
                preds = self.model(signal)
                loss = loss_function(preds, signal)
                result['loss'][i] = loss.item()
                
            desc = f" Batch [{i + 1:>04}/{len(self.valid_loader):>04}"
            pbar.set_description_str(desc)
            pbar.update(1)
                
        return result['loss'].mean().item()
```
- ëª¨ë¸ì„ í•™ìŠµí•  Trainerë¥¼ From scratchë¡œ ì •ì˜í•˜ì˜€ìŠµë‹ˆë‹¤. Valid Lossë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•˜ì—¬ ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ íŒŒë¼ë¯¸í„°ë¥¼ ì €ì¥í•˜ê³  Epoch ë³„ í•™ìŠµ ë° ê²€ì¦ ì„±ëŠ¥ì„ ì¶œë ¥í•©ë‹ˆë‹¤.


```python
model = MLPAutoEncoder(config)
Trainer = ADTrainer(config = config, model = model, check_path = './Results/MLP')
Trainer.run()
```
```
Epoch [0100/0100 | train_loss: 93.6023 | valid_loss: 1.4745 |: |          | [01:31<00:00,  1.10it/s] 
```
<p align="center">
    <img src="Results/MLP/Reconstruction_loss.png" width="400">
</p>

- Train Lossë¥¼ ì‹œê°ì ìœ¼ë¡œ í™•ì¸í•´ë³´ë‹ˆ Lossê°€ ì˜ ìˆ˜ë ´í•˜ë©° í•™ìŠµì´ ì´ë£¨ì–´ì§„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ë‹¤ë§Œ Valid Lossì™€ Train Loss ê°„ì˜ ì°¨ì´ê°€ ìˆê¸°ì—, ê²€ì¦ì´ ì œëŒ€ë¡œ ì´ë£¨ì–´ì¡ŒëŠ”ì§€ì— ëŒ€í•œ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤. ì´ì œ í•´ë‹¹ ëª¨ë¸ì„ ë°”íƒ•ìœ¼ë¡œ Anomaly Detecionì„ ìˆ˜í–‰í•´ë³´ë ¤í•©ë‹ˆë‹¤.   

- ê·¸ ì´ì „ì—, Anomalyë¥¼ ì •ì˜í•˜ê¸° ìœ„í•´ì„œëŠ” **Normalì´ ë¬´ì—‡ì¸ì§€ì— ëŒ€í•œ ì •ì˜**ë¥¼ ì„¸ì›Œì•¼ í•©ë‹ˆë‹¤. ê·¸ **Thresholdë¥¼ ì •í•´ì•¼ Normalì´ ì•„ë‹Œ ê²ƒì„ Anomalyë¼ê³  ë§í•  ìˆ˜ ìˆê¸° ë•Œë¬¸**ì…ë‹ˆë‹¤.

<br/>

## Choosing a Threshold
- ë³¸ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” Thresholdë¥¼ ì •í•˜ê¸° ìœ„í•´ì„œ **í•™ìŠµ ì‹œ ì‚¬ìš©ë˜ì§€ ì•Šì•˜ë˜ Normal ë°ì´í„°ì™€ Abnormal ë°ì´í„°ë¥¼ ì´ìš©**í•´ë³´ë ¤ í•©ë‹ˆë‹¤. ë‘ ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ **Lossì˜ Thresholdë¥¼ ì„¤ì •**í•˜ê³ , **ì‹¤ì œ Label ê°’ì„ ê°€ì¥ ì˜ ë§ì¶”ëŠ” Thresholdë¥¼ ì´ìš©í•˜ì—¬ ìµœì¢… cutoffë¥¼ ì§€ì •**í•  ê²ƒì…ë‹ˆë‹¤.

    - ì¦‰, MSEë¥¼ í†µí•œ ì¬êµ¬ì¶• ì˜¤ì°¨(Reconstruction Error) ê°’ì„ ë¹„êµí•˜ì—¬ ì‹¤ì œ Labelì´ ì˜ êµ¬ë¶„ë˜ëŠ” ì§€ì ì„ Thresholdë¡œ ì§€ì •í•©ë‹ˆë‹¤.

    - Classification ì„±ëŠ¥ì€ **F1 Scoreë¥¼ ì´ìš©**í•©ë‹ˆë‹¤.

```python
model = MLPAutoEncoder(model_config).to(device)
model.load_state_dict(torch.load('./MLP_AE_epoch_70.pt'))
```
```python
def plot_signal(loader, title: str):
    model.eval()
    for i, signal in enumerate(loader):
        signal = torch.FloatTensor(signal).to(device).float()
        preds = model(signal)
        loss=loss_function(preds, signal)
        break
    
    input = signal.detach().cpu().numpy()
    pred = preds[0].detach().cpu().numpy()
    
    plt.figure(figsize=(15, 8))
    plt.plot(list(range(model_config.seq_len)), input,label="Original")
    plt.plot(list(range(model_config.seq_len)), pred,label="Predicted")
    plt.title(f"{title}, Reconstrunction Error: {loss.item()}")
    plt.legend(prop={'size': 20})
    
plot_signal(normal_test, title="Normal")
plot_signal(abnormal_test, title="Abnormal")
```
<p align="center">
    <img src="Anomaly_Detection_Img/plot_signal_normal.png" width="800">
</p>
<p align="center">
    <img src="Anomaly_Detection_Img/plot_signal_abnormal.png" width="800">
</p>

- ë¨¼ì € ì •ìƒ ë°ì´í„°ë¡œ í•™ìŠµëœ ëª¨ë¸ì„ í†µí•´ì„œ í•™ìŠµ ì‹œ ë³´ì§€ ëª»í–ˆë˜ **Normal ë°ì´í„°ì™€ Abnormal ë°ì´í„° ê°„ì˜ MSE Loss ë° Input ì¬êµ¬ì¶• ê²°ê³¼ë¥¼ ì‹œê°ì ìœ¼ë¡œ ë¹„êµ**í•´ë³´ì•˜ìŠµë‹ˆë‹¤.

- í™•ì¸ ê²°ê³¼ ìš°ë¦¬ì˜ MLPAutoEncoderëŠ” **Normal ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œëŠ” ë¹„êµì  ì‹œê°ì ìœ¼ë¡œ ìœ ì‚¬í•˜ë©° MSE Lossë„ ë‚®ì€** ë°˜ë©´, **Abnormal ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œëŠ” ìƒëŒ€ì ìœ¼ë¡œ ë†’ì€ Loss ê°’ì„ ë³´ì´ëŠ” ê²ƒì„ í™•ì¸**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”ë¶ˆì–´ **ì¬êµ¬ì¶• ëœ ê²°ê³¼ë„ ì‹œê°ì ìœ¼ë¡œ ë‹¬ë¼, í•´ë‹¹ ëª¨ë¸ì„ í†µí•´ Thresholdë¥¼ ì •í•˜ê³  Anomaly íƒì§€ë¥¼ ìˆ˜í–‰**í•´ë³´ê³ ì í•©ë‹ˆë‹¤.

```python
def get_loss(loader):
    model.eval()
    total_loss=[]
    for i, signal in enumerate(loader):
        signal=signal.to(config.device).float()
        output=model(signal)
        loss=loss_function(output,signal)
        total_loss.append(loss.item())

    return np.array(total_loss)

def normalize_loss(loss, doit=True):
    if(not doit):
        return loss
    global max_loss, min_loss
    loss=(loss - min_loss)/(max_loss - min_loss)
    return loss
```
- ë¨¼ì € Lossë¥¼ ì–»ê³ , ì´ë¥¼ 0ê³¼ 1 ì‚¬ì´ë¡œ Normalize í•´ë³´ê² ìŠµë‹ˆë‹¤. ì´ë¡œì¨ Thresholdì˜ ë²”ìœ„ëŠ” 0ê³¼ 1 ì‚¬ì´ì—ì„œ ì°¾ì„ ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.

```python
nor_loss = get_loss(nor_validloader)
abnor_loss = get_loss(ab_testloader)

max_loss = max(nor_loss.max(), abnor_loss.max())
min_loss = min(nor_loss.min(), abnor_loss.min())

nor_loss = normalize_loss(nor_loss)
abnor_loss = normalize_loss(abnor_loss)
```

```python
def find_optimal_threshold(nor_loss, abnor_loss):
    
    optimal_threshold=0
    optimal_f1score=0
    
    for threshold in np.arange(0, 1, 0.01):
        normal_pred = []
        for x in nor_loss:
            pred=1 if x > threshold else 0
            normal_pred.append(pred)

        abnormal_pred = []
        for x in abnor_loss:
            pred=1 if x > threshold else 0
            abnormal_pred.append(pred)

        normal_pred=np.array(normal_pred)
        normal_true=np.zeros_like(normal_pred)
        abnormal_pred=np.array(abnormal_pred)
        abnormal_true=np.ones_like(abnormal_pred)

        tp=sum(abnormal_pred)
        fp=sum(normal_pred)
        tn=len(normal_pred)-fp
        fn=len(abnormal_pred)-tp
        
        
        precision=tp/(tp+fp)
        recall=tp/(tp+fn)
        f1score=(2*(precision*recall))/(precision+recall)
        
        if(f1score > optimal_f1score):
            optimal_fscore=f1score
            optimal_threshold=threshold
        
    return optimal_threshold
```
```python
threshold = find_optimal_threshold(nor_loss, abnor_loss)
print(threshold)
```
```
0.1
```
- ìµœì ì˜ Threshold íƒìƒ‰ ê²°ê³¼, 0.1ì—ì„œ F1 Scoreê°€ ê°€ì¥ ë†’ì€ ê²ƒìœ¼ë¡œ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ ì´ ê°’ì„ í†µí•´ Precision, Recall, F1 Score ê°’ì„ ê°ê° í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤.

- ì´ë•Œ F1 ScoreëŠ” Precisionê³¼ Recallì˜ ì¡°í™” í‰ê· ì„ ê³„ì‚°í•˜ëŠ” ë° ìˆì–´ ë‘ ê°’ì— ê°€ì¤‘ì¹˜ $\beta$ë¥¼ ë¶€ì—¬í•˜ì—¬ ê°’ì„ ì¡°ì ˆí•©ë‹ˆë‹¤.   

$$F_{\beta } = (1+\beta ^2)\frac{Precision\times Recall}{(\beta ^2 \times Precision)+Recall}$$

- $\beta $ ê°’ì´ 1ë³´ë‹¤ í¬ë©´ Recallì— í° ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ê³ , 1ë³´ë‹¤ ì‘ìœ¼ë©´ Precisionì— í° ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ì—¬ ê³„ì‚°ë©ë‹ˆë‹¤.

## Evaluation
```python
def print_metrics(nor_loss, abnor_loss, threshold, beta: float):
    normal_pred=[]
    for x in nor_loss:
        pred=1 if x > threshold else 0
        normal_pred.append(pred)

    abnormal_pred=[]
    for x in abnor_loss:
        pred=1 if x > threshold else 0
        abnormal_pred.append(pred)

    normal_pred=np.array(normal_pred)
    normal_true=np.zeros_like(normal_pred)
    abnormal_pred=np.array(abnormal_pred)
    abnormal_true=np.ones_like(abnormal_pred)

    tp=sum(abnormal_pred)
    fp=sum(normal_pred)
    tn=len(normal_pred)-fp
    fn=len(abnormal_pred)-tp

    accuracy_normal=tn/len(normal_pred)
    accuracy_abnormal=tp/len(abnormal_pred)
    accuracy_total=(tp+tn)/(len(normal_pred)+len(abnormal_pred))

    precision=tp/(tp+fp)
    recall=tp/(tp+fn)
    num=(1+beta**2)*(precision*recall)
    den=((beta**2)*precision)+recall
    f1score=num/den

    print(f"Accuracy Normal: {accuracy_normal*100:.2f}%")
    print(f"Accuracy Abnormal: {accuracy_abnormal*100:.2f}%")
    print(f"Accuracy Total: {accuracy_total*100:.2f}%")

    print(f"Precision : {precision:.3f}")
    print(f"Recall : {recall:.3f}")
    print(f"F1score : {f1score:.3f}")
    
    
print_metrics(nor_loss, abnor_loss, threshold, beta=0.1)
```
```
Accuracy Normal: 85.83%
Accuracy Abnormal: 82.50%
Accuracy Total: 84.17%
Precision : 0.853
Recall : 0.825
F1score : 0.839
```
- **F1 Scoreê°€ 0.839ë¼ëŠ” ê´„ëª©í•  ë§Œí•œ ì„±ëŠ¥ì„ ë³´ì´ë©° Anomaly Detectionì„ ì˜ ìˆ˜í–‰í•  ìˆ˜ ìˆìŒì„ ì‹¤í—˜ì ìœ¼ë¡œ í™•ì¸**í•˜ì˜€ìŠµë‹ˆë‹¤.
- ë”ë¶ˆì–´ **Abnormalì— ëŒ€í•œ ì •í™•ë„ ì—­ì‹œ 82.50%ë¥¼ ê¸°ë¡í•˜ë©°, ì¤€ìˆ˜í•œ ì„±ëŠ¥ìœ¼ë¡œ ì´ìƒì¹˜ íƒì§€ë¥¼ í•´ë‚´ëŠ” ê²ƒì„ í™•ì¸**í•˜ì˜€ìŠµë‹ˆë‹¤.

<br/>

------

<br/>

# Experiments
## Experiment 1
- ì§€ê¸ˆê¹Œì§€ì˜ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” ê¸°ë³¸ì ì¸ MLPë¥¼ ì´ìš©í•œ AutoEncderë¥¼ êµ¬ì¶•í•˜ì˜€ê³ , ì´ì— ëŒ€í•œ Anomaly Detection ê²°ê³¼ë¥¼ ë³´ì•˜ìŠµë‹ˆë‹¤. ì´ë•Œ ì•ì„œ **AutoEncoderëŠ” ë°ì´í„° íƒ€ì…ì— ë”°ë¼ì„œ ë‹¤ì–‘í•œ í˜•íƒœë¡œ êµ¬ì¶•ì´ ê°€ëŠ¥í•˜ë‹¤ê³  ë§í•œ ë°” ìˆìŠµë‹ˆë‹¤.**. ì¦‰, **Backboneì„ LSTM, CNN ë“± ë°ì´í„° íƒ€ì…ì— ì•Œë§ì€ ê²ƒìœ¼ë¡œ ë°”ê¾¸ì–´ ì‚¬ìš©í•˜ë©´ í–¥ìƒëœ ì„±ëŠ¥ì„ ê¸°ëŒ€**í•  ìˆ˜ ìˆëŠ” ê²ƒì´ì£ .

- ìš°ë¦¬ê°€ ë‹¤ë£¬ ECG ë°ì´í„°ì…‹ì€ **Time Series Data**ì´ë©°, ë‹¨ì¼ Segment ê¸¸ì´(187)ë¥¼ ë§ì¶”ê¸° ìœ„í•´ Zero paddingì„ í•œ ìƒí™©ì…ë‹ˆë‹¤. ì´ë•Œ **LSTMì´ë‚˜ 1D-CNNì„ ì´ìš©í•˜ë©´ ë³´ë‹¤ Signalì˜ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ëŠ” ë° ìœ ë¦¬í•˜ì§€ëŠ” ì•Šì„ê¹Œìš”?**

- ë”°ë¼ì„œ **Experiment 1**ì—ì„œëŠ” **ì‹œê³„ì—´ ë°ì´í„° ì²˜ë¦¬ì— ìš©ì´í•œ Backbone êµ¬ì¡°ë¡œ AutoEncoderë¥¼ ì„¤ê³„í•˜ë©´ ë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì´ ë‚˜ì˜¬ ê²ƒì´ë¼ëŠ” ê°€ì„¤**í•˜ì—, LSTMê³¼ Conv1D AutoEncoderë¥¼ ì„¤ê³„í•˜ê³  ê°ê°ì˜ ì„±ëŠ¥ì„ ë¹„êµí•´ë³´ê³ ì í•©ë‹ˆë‹¤.

### Parameters
- Datalaoder í˜¸ì¶œë¶€í„° Model Evaluationê¹Œì§€, **Anomaly Detectionì— í•„ìš”í•œ ì£¼ìš” Hyperparameter ëª©ë¡**ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

|__Argument__|__Type__|__Default__|__Help__|
|-------------|-------|------------|-------|
|`data_dir`|str|"./Data/"|csv ë°ì´í„° íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì‹œë©´ ë©ë‹ˆë‹¤.|
|`normal_train_path`|str|data_dir+"normal_train.csv"|í•™ìŠµ ì‹œ ì‚¬ìš©í•  Normal Datasetì´ í¬í•¨ëœ ê²½ë¡œ ëª…ì„ ì…ë ¥í•˜ì‹œë©´ ë©ë‹ˆë‹¤.|
|`normal_valid_path`|str|data_dir+"normal_valid.csv"|ê²€ì¦ ì‹œ ì‚¬ìš©í•  Normal Datasetì´ í¬í•¨ëœ ê²½ë¡œ ëª…ì„ ì…ë ¥í•˜ì‹œë©´ ë©ë‹ˆë‹¤.|
|`patient_valid_path`|str|data_dir+"patient_valid.csv"|ê²€ì¦ ì‹œ ì‚¬ìš©í•  Abnormal Datasetì´ í¬í•¨ëœ ê²½ë¡œ ëª…ì„ ì…ë ¥í•˜ì‹œë©´ ë©ë‹ˆë‹¤.|
|`num_features`|int|1|ECG ë°ì´í„°ì˜ ë³€ìˆ˜ ê°œìˆ˜ì…ë‹ˆë‹¤. 1ì°¨ì› Signalì…ë‹ˆë‹¤.|
|`seq_len`|int|187|ECG ë°ì´í„°ì˜ Sequence Lengthì…ë‹ˆë‹¤. í•œ íŒŒí˜•ì— ìµœëŒ€ 187ê°œê°€ ìˆìœ¼ë©°, ëª¨ë“  ë°ì´í„°ê°€ 187ê°œì˜ í¬ì¸íŠ¸ë¥¼ ê°€ì§€ë„ë¡ Zero paddingì„ ìˆ˜í–‰í•˜ì˜€ìŠµë‹ˆë‹¤.|
|`hidden_1`|int|500|MLPAutoEncoder êµ¬ì¶• ì‹œ í•„ìš”í•œ Hidden Dimensionì…ë‹ˆë‹¤. Enocerì˜ ì²« ë²ˆì§¸ Layerì˜ output ì°¨ì›ì´ ë©ë‹ˆë‹¤.|
|`hidden_2`|int|100|MLPAutoEncoder êµ¬ì¶• ì‹œ í•„ìš”í•œ Hidden Dimensionì…ë‹ˆë‹¤. Enocerì˜ ë‘ ë²ˆì§¸ Layerì˜ output ì°¨ì›ì´ ë©ë‹ˆë‹¤.|
|`hidden_dim`|int|20|LSTMAutoEncoder êµ¬ì¶• ì‹œ í•„ìš”í•œ Hidden Dimensionì…ë‹ˆë‹¤.|
|`epochs`|int|100|Number of epochs to train|
|`train_batch_size`|int|64|Input batch size for training|
|`valid_batch_size`|int|1|Input batch size for validation|
|`test_batch_size`|int|1|input batch size for test|
|`early_stop_patience`|int|50|Number of epochs with no improvement after which training will be stopped.|
|`optimizer`|str|'adam'|Stochastic Gradient Descent Optimizer|
|`lr`|float|1e-2|learning rate|
|`optim_clip_value`|float|5.0|Gradient clip value|
|`early_stop_patience`|int|50|Number of epochs with no improvement after which training will be stopped|
|`device`|str|"cuda:0"|Defualt == "cuda:0", however try torch.device("cuda:0" if torch.cuda.is_available() else "cpu")|


### LSTM AutoEncoder

 
**LSTM(Long Short Term Memory)** ì€ ê¸°ì¡´ RNNì´ ì¶œë ¥ê³¼ ë¨¼ ìœ„ì¹˜ì˜ ì •ë³´ë¥¼ ê¸°ì–µí•˜ê¸° ì–´ë µë‹¤ëŠ” ë‹¨ì ì„ ë³´ì™„í•˜ì—¬ ì¥/ë‹¨ê¸° ê¸°ì–µì´ ëª¨ë‘ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ê³„í•œ ì‹ ê²½ë§ êµ¬ì¡°ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. **ë°ì´í„°ì˜ ì‹œê³„ì—´ì„±ì„ ë°˜ì˜**í•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì  ë•ë¶„ì— ì‹œê³„ì—´ ë°ì´í„° ë° ìì—°ì–´ ì²˜ë¦¬ì— ì£¼ë¡œ ì‚¬ìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤. ê¸°ë³¸ì ì¸ LSTM ëª¨ë¸ì˜ êµ¬ì¡°ëŠ” ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ìŠµë‹ˆë‹¤.
 <p align="center">
     <img src="Anomaly_Detection_Img/LSTM.png" width="500"/>
 </p>


```python
class LSTMAutoEncoder(nn.Module):
    def __init__(self, config: object):
        super().__init__()
        # input = (seq_len, Batch, num_features)
        self.hidden_dim = config.hidden_dim
        self.num_features = config.num_features
        self.seq_len = config.seq_len
        
        self.lstm1 = nn.LSTMCell(self.num_features, self.hidden_dim)  # lstm cell = (num_features, hidden_dim)
        self.lstm2 = nn.LSTMCell(self.num_features, self.hidden_dim)
        self.linear = nn.Linear(self.hidden_dim, self.num_features)
        self.sigmoid = nn.Sigmoid()

    def Encoder(self, x):
        ht = torch.zeros(x.size(0), self.hidden_dim, dtype=torch.float, device=config.device)
        ct = torch.zeros(x.size(0), self.hidden_dim, dtype=torch.float, device=config.device)
        
        for input in x.chunk(x.size(1), dim=1):
            ht, ct = self.lstm1(input, (ht, ct))
            
        return ht, ct
    
    def Decoder(self, ht, ct):
        out = torch.zeros(ht.size(0), self.num_features, dtype=torch.float, device=config.device)
        outputs = torch.zeros(ht.size(0), self.seq_len, dtype=torch.float, device=config.device)
        
        for i in range(self.seq_len):
            ht, ct = self.lstm2(out, (ht,ct))
            out = self.sigmoid(self.linear(ht))
            outputs[:, i] = out.squeeze()
            
        return outputs
        
    def forward(self, x):
        
        he, ce = self.Encoder(x)    # hidden encoder, cell_state encoder
        out = self.Decoder(he, ce)
    
        return torch.flip(out, dims=[1])
```
- ìœ„ì²˜ëŸ¼ LSTM AutoEncoderë¥¼ ì •ì˜í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ì œ ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ í•™ìŠµì„ ì§„í–‰í•´ ë³´ê² ìŠµë‹ˆë‹¤.

```python
model = LSTMAutoEncoder(config)
Trainer = ADTrainer(config = config, model = model, check_path = './Results/LSTM')
Trainer.run()
```
```python
total_loss = []
model.train()
start = time.time()

# Train Model
for epoch in range(train_config.epochs + 1):
    loss_count=0
    for i, signal in enumerate(train_loader):
        signal = signal.to(device).float()  # batch_size, 187
        model.zero_grad()
        output = model(signal)
        loss = loss_function(output, signal)
        loss_count+=loss.item()

        loss.backward()
        optimizer.step() 

    total_loss.append(loss_count)
    if(epoch % 10 == 0):
        print("Epoch : {} Loss : {}".format(epoch, loss_count))
        # Saving Model
        torch.save(model.state_dict(),"LSTM_AE_epoch_{}.pt".format(epoch))

end = time.time()
time_taken = (end - start)/60
print(f"Total Time Taken : {time_taken:.2f}")
```
```python
plt.plot(list(range(len(total_loss))), total_loss)
plt.xlabel("Total Epochs")
plt.ylabel("Loss value")
plt.show();
```
 <p align="center">
     <img src="Anomaly_Detection_Img/lstm_train_loss.png" width="300">
 </p>

- Train Lossë¥¼ ì‹œê°ì ìœ¼ë¡œ í™•ì¸í•´ë³´ë‹ˆ Lossê°€ ì˜ ìˆ˜ë ´í•˜ë©° í•™ìŠµì´ ì´ë£¨ì–´ì§„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì´ì œ í•´ë‹¹ ëª¨ë¸ì„ ë°”íƒ•ìœ¼ë¡œ Anomaly Detecionì„ ìˆ˜í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤.

```python
model = LSTMAutoEncoder(config).to(config.device)
model.load_state_dict(torch.load('./LSTM_epoch_100.pt'))

plot_signal(valid_loader, config, title="Normal")
plot_signal(test_loader, config, title="Abnormal")
```
 <p align="center">
     <img src="Anomaly_Detection_Img/lstm_nor_sig.png" width="800">
 </p>
  <p align="center">
     <img src="Anomaly_Detection_Img/lstm_abnor_sig.png" width="800">
 </p>

- ë§ˆì°¬ê°€ì§€ë¡œ Normalì„ ë³µì›í•œ ê°’ê³¼ Abnormalì„ ë³µì›í•œ ê°’ì€ ì‹œê°ì ìœ¼ë¡œë„, ì •ëŸ‰ì ìœ¼ë¡œë„ í° ì°¨ì´ê°€ ë‚˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ Thresholdë¥¼ ì •í•˜ê³  Anomaly Detectionì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

```python
nor_loss = get_loss(valid_loader)
abnor_loss = get_loss(test_loader)

max_loss = max(nor_loss.max(), abnor_loss.max())
min_loss = min(nor_loss.min(), abnor_loss.min())

nor_loss = normalize_loss(nor_loss)
abnor_loss = normalize_loss(abnor_loss)
```
```python
threshold=find_optimal_threshold(nor_loss, abnor_loss)
print(threshold)
```
```
0.03
```
```python
print_metrics(nor_loss, abnor_loss, threshold, 1)
```
```
Accuracy Normal: 45.83%
Accuracy Abnormal: 93.33%
Accuracy Total: 69.58%
Precision : 0.633
Recall : 0.933
F1score : 0.754
```
- F1 Scoreê°€ 0.754ë¡œ ë†’ì€ ì„±ëŠ¥ì´ë©° Accuracy Abnormalì´ 93&ë¥¼ ë„˜ìœ¼ë©° ì¤€ìˆ˜í•œ ì´ìƒì¹˜ íƒì§€ ì„±ëŠ¥ì„ ê¸°ë¡í•˜ì˜€ìœ¼ë‚˜, ê¸°ëŒ€í–ˆë˜ ì„±ëŠ¥ì´ ë‚˜ì˜¤ëŠ” ëŒ€ì‹  MLPë¡œ êµ¬ì¶•í•œ ë‹¨ìˆœ AutoEncoderë³´ë‹¤ ë‚®ì€ ì„±ëŠ¥ì„ ê¸°ë¡í•œ ê²ƒì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤.

- í•´ë‹¹ ê²°ê³¼ë¡œëŠ” ìš°ë¦¬ì˜ ê°€ì„¤ì„ ì¦ëª…í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ Conv1Dë¡œ êµ¬ì¶•í•œ AutoEncoderëŠ” ë‹¤ë¥¸ ì„±ëŠ¥ì„ ë³´ì—¬ì¤„ê¹Œìš”?

<br/>

### Convolution AutoEncoder   

**CNN(Convolution Neural Network)** ì€ **Spatial** Patternì„ ì¡ì•„ë‚´ëŠ” ë° íŠ¹í™”ëœ ì¸ê³µ ì‹ ê²½ë§ì´ë©°, **LSTM**ì€ **Temporal** Patternì„ ì¸ì‹í•˜ëŠ” ë° ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤.

- ë”°ë¼ì„œ CNN ì—­ì‹œ ì…ë ¥ ë°ì´í„°ì˜ íŠ¹ì§•ì„ ìë™ìœ¼ë¡œ ê²€ì¶œí•˜ëŠ” ë° ìœ ìš©í•˜ë‹¤ëŠ” ì ì—ì„œ, ì…ë ¥ì„ ì¬êµ¬ì¶•í•˜ëŠ” AutoEncoder ê¸°ë²•ìœ¼ë¡œ ì‚¬ìš©ë˜ê³ , LSTMê³¼ ê²°í•©ë˜ì–´ ConvLSTM ë“±ì˜ ê³ ë„í™”ëœ ëª¨ë¸ë¡œë„ í™œìš©ë©ë‹ˆë‹¤.
- ì¼ë°˜ì ìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•˜ëŠ” CNNê³¼ ë‹¤ë¥´ê²Œ, 1D Convolutionì€ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë° ìœ ìš©í•˜ê¸°ì—, ì´ë¥¼ PyTorchë¡œ êµ¬í˜„í•˜ê³  ì„±ëŠ¥ì„ MLPì™€ LSTMê³¼ ë¹„êµí•´ë³´ê² ìŠµë‹ˆë‹¤.

 <p align="center">
     <img src="Anomaly_Detection_Img/1DConv.png" width="800"/>
 </p>

<br/>

```python
class ConvAutoencoder(nn.Module):
    
    def __init__(self, config: object):
        super().__init__()

        self.Encoder = nn.Sequential(
            
            nn.Conv1d(config.num_features, 2, 8),
            nn.ReLU(inplace=True),
            nn.MaxPool1d(2),
            
            nn.Conv1d(2,4,11),
            nn.ReLU(inplace=True),
            nn.MaxPool1d(2),
            
            nn.Conv1d(4,8,11),
            nn.ReLU(inplace=True),
            nn.MaxPool1d(2),
            
            nn.Conv1d(8,16,6),
            nn.ReLU(inplace=True),
            nn.MaxPool1d(2),
            
            nn.Conv1d(16,32,5),
            nn.ReLU(inplace=True)
        )
        
        self.Decoder=nn.Sequential(
            
            nn.ConvTranspose1d(32,16,15,1,0),
            nn.ReLU(inplace=True),
            
            nn.ConvTranspose1d(16,8,12,2,0),
            nn.ReLU(inplace=True),
            
            nn.ConvTranspose1d(8,4,12,2,0),
            nn.Sigmoid(),
            
            nn.ConvTranspose1d(4,2,12,2,5),
            nn.Sigmoid(),
            
            nn.ConvTranspose1d(2,1,8,1,0),
            nn.Sigmoid(),
        )
        
    def forward(self, x):
        x = x.view(-1, config.num_features, config.seq_len)
        latent_vector=self.Encoder(x)
        preds=self.Decoder(latent_vector)
        
        return preds.view(-1, config.seq_len)
```
- Conv1Dë¡œ êµ¬ì¶•í•œ AutoEncoder êµ¬ì¡°ì…ë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•™ìŠµì„ ì™„ë£Œí•œ ì´í›„, evaluation ê³¼ì •ì„ í†µí•´ ë‚˜ì˜¨ Loss ë° ì¬êµ¬ì¶• ì˜¤ì°¨ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê²°ê³¼ë¥¼ ì‹œê°í™”í•´ë³´ê² ìŠµë‹ˆë‹¤.


```python
def plot_signal(loader, config: object, title: str):
    model.eval()
    for i,signal in enumerate(loader):
        signal=signal.to(config.device).float()
        output=model(signal)
        loss=loss_function(output, signal)

    s1=output[0]
    s2=signal[0]
    
    plt.figure(figsize=(15, 8))
    plt.plot(list(range(187)),s1.detach().cpu().numpy(),label="Reconstructed")
    plt.plot(list(range(187)),s2.detach().cpu().numpy(),label="True")
    plt.title(f"{title}, Reconstrunction Error: {loss.item()}")
    plt.legend(prop={'size': 15})
    
plot_signal(valid_loader, config, title="Normal")
plot_signal(test_loader, config, title="Abnormal")
```

 <p align="center">
     <img src="Anomaly_Detection_Img/conv_nor.png" width="800">
 </p>
  <p align="center">
     <img src="Anomaly_Detection_Img/conv_abnor.png" width="800">
 </p>

- ë§ˆì°¬ê°€ì§€ë¡œ ì •ìƒ ë°ì´í„°ì™€ ë¹„ì •ìƒ ë°ì´í„°ì˜ ë³µì› ê°’ ì°¨ì´ê°€ ì‹œê°ì ìœ¼ë¡œë‚˜ ì •ëŸ‰ì ìœ¼ë¡œë‚˜ ë¶„ëª…íˆ ì¡´ì¬í•˜ëŠ” ê²ƒì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ì„±ëŠ¥ì€ ì–´ë–¨ê¹Œìš”?

```pthon
threshold=find_optimal_threshold(nor_loss, abnor_loss)
print(threshold)
```
```
0.03
```
```python
print_metrics(nor_loss, abnor_loss, threshold, 1)
```
```
Accuracy Normal: 97.50%
Accuracy Abnormal: 80.00%
Accuracy Total: 88.75%
Precision : 0.970
Recall : 0.800
F1score : 0.877
```
- LSTMê³¼ ThresholdëŠ” ê°™ìœ¼ë‚˜, í›¨ì”¬ ì¢‹ì€ ì„±ëŠ¥ì„ ê¸°ë¡í•œ ê²ƒì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤.
- Accuracy Normal ë° Abnormalì´ ëª¨ë‘ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ë©°, Conv1Dë¥¼ í†µí•œ AutoEncoderê°€ ECGì˜ ì´ìƒ ì‹¬ì¥ ë°•ë™ì„ ì˜ ê²€ì¶œí•  ìˆ˜ ìˆìŒì„ ì‹¤í—˜ì ìœ¼ë¡œ ì¦ëª…í•˜ì˜€ìŠµë‹ˆë‹¤.

<br/>

### Result of Experiment 1
ê²°ê³¼ë¥¼ ì •ë¦¬í•˜ê³  í•´ì„í•˜ê² ìŠµë‹ˆë‹¤. ì´ Epochì„ 100íšŒë¥¼ ëŒë¦° ê²°ê³¼, MLP/LSTM/Conv1D AutoEncoderì˜ í•™ìŠµ ì†Œìš” ì‹œê°„ ë° ì„±ëŠ¥ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.   

|                 |__MLP AutoEncoder__|__LSTM AutoEncoder__|__Conv1D AutoEncoder__|  
|-----------------|:-----------------:|:------------------:|:--------------------:|
|__Training Time (sec)__| __69__ | 854 | 90 |
|__Threshold__| 0.10 | 0.03 | 0.03 | 
|__F1 Score ($\beta = 1.0$)__| 0.84 | 0.75 | __0.88__ | 
|__Accuracy Abnormal (%)__| 82.50 | __93.33__ | 80.00 |

- - MLP, LSTM, Conv1Dë¡œ ê°ê° AutoEncoderë¥¼ êµ¬ì¶•í•´ë³¸ ê²°ê³¼, **Conv1Dì—ì„œ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©° MLP, LSTM ìˆœìœ¼ë¡œ ì´ìƒì¹˜ íƒì§€ ì„±ëŠ¥ì´ ì¢‹ìŒì„ í™•ì¸**í•˜ì˜€ìŠµë‹ˆë‹¤.

- ê·¸ëŸ¬ë‚˜ ì‹œê³„ì—´ ë°ì´í„° ì²˜ë¦¬ì— ìœ ìš©í•˜ë‹¤ê³  ì•Œë ¤ì§„ LSTM êµ¬ì¡°ì˜ ëª¨ë¸ì„ ì ìš©í•˜ë‹ˆ ì˜¤íˆë ¤ ì„±ëŠ¥ í•˜ë½ì„ ê¸°ë¡í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ë¡œì¨ **ì‹œê³„ì—´ì„±ì„ ë°˜ì˜í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ êµ¬ì¡°ë¥¼ í™œìš©í•˜ë©´ ì „ì²´ì ì¸ ì„±ëŠ¥ì´ í–¥ìƒë  ê²ƒ**ì´ë¼ëŠ” ê°€ì„¤ì€ ì¦ëª…í•˜ê¸° ì–´ë ¤ì› ìŠµë‹ˆë‹¤.

<br/>

### Conclusion of Experiment 1

- **Conv1Dê°€ ë” ë†’ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ì´ìœ **ëŠ”, **ECG ë°ì´í„°ì…‹ ê³µê°„ ë¶„í¬ì— ëŒ€í•´ì„œ ë†’ì€ ë…¸ì´ì¦ˆë¥¼ ë³´ì´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.** ë‹¨ì‹œê°„ ë‚´ì—ì„œë„ ì—¬ëŸ¬ ë²ˆ ìš”ë™ì¹˜ëŠ” í˜•íƒœë¥¼ ë³´ì´ê¸°ì— ë°ì´í„°ì˜ **Spatial Featuresë¥¼ ì˜ ì¶”ì¶œí•  ìˆ˜ ìˆëŠ” CNNì´ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ê²ƒìœ¼ë¡œ íŒë‹¨**ë©ë‹ˆë‹¤.

- ë”ë¶ˆì–´ ì‹œê³„ì—´ ë°ì´í„°ë¼ í•˜ë”ë¼ë„, ë³¸ ë¶„ì„ì—ì„œ í™œìš©í•œ **ECG ë°ì´í„°ëŠ” ê´€ì¸¡ì¹˜ê°€ ìµœëŒ€ 187ê°œë¿ì¸ ì†Œí˜• ë°ì´í„°ì…‹ì´ê¸°ì— ì‹œê³„ì—´ì  íŠ¹ì„±ì´ í¬ê²Œ ë‘ë“œëŸ¬ì§€ì§€ ì•Šë‹¤ëŠ” ì  ì—­ì‹œ LSTM ì„±ëŠ¥ì´ ë‹¤ë¥¸ ëª¨ë¸ë“¤ë³´ë‹¤ ë‚®ì€ ì´ìœ ê°€ ë  ê²ƒ**ì…ë‹ˆë‹¤.

<br/>

- ê·¸ëŸ¬ë‚˜ ì¢…í•©ì ìœ¼ë¡œ íŒë‹¨í•´ ë³´ì•˜ì„ ë•Œ, **ECGì˜ Anomaly Detectionì„ ìœ„í•´ Conv1D AutoEncoderë¥¼ ì‚¬ìš©í•´ì•¼ í•œë‹¤ê³  ë‹¨ì–¸í•˜ê¸°ëŠ” ì–´ë µìŠµë‹ˆë‹¤.**
- **MLP AutoEncoderì˜ ì¥ì ì€ í•™ìŠµ ì‹œê°„ì´ ê°€ì¥ ë¹ ë¥´ë‹¤ëŠ” ê²ƒ**ì´ë©°, ë”ë¶ˆì–´ **ë¹„êµì  ë†’ì€ ì„±ëŠ¥**ì„ ë³´ì´ê³  ìˆê¸°ì— **1ì°¨ì  ì´ìƒ íƒì§€ ëª¨ë¸ë¡œ ì ê·¹ í™œìš©í•  ìˆ˜ ìˆë‹¤ëŠ” íŠ¹ì§•**ì„ ê°€ì§‘ë‹ˆë‹¤.
- **LSTM AutoEncoder**ëŠ” í•™ìŠµ ì‹œê°„ë„ ê°€ì¥ ëŠë¦¬ê³  F1 Scoreë„ ìƒëŒ€ì ìœ¼ë¡œ ë‚®ì•„ ì„ í˜¸ë˜ì§€ ì•Šì„ ìˆ˜ ìˆì§€ë§Œ, **Accuracy Abnormalì´ ê°€ì¥ ë†’ë‹¤ëŠ” íŠ¹ì§•**ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.
    - Accuracy Abnormalì€ 'Abnormal'ì´ë¼ íƒì§€í•œ ê²ƒ ì¤‘ ì‹¤ì œ Abnormalì˜ ë¹„ìœ¨ì„ %ë¡œ ë‚˜íƒ€ë‚¸ ê²ƒì…ë‹ˆë‹¤.
    - **ECGë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¶€ì •ë§¥ê³¼ ê°™ì€ ì´ìƒ ì‹¬ì¥ ë°•ë™ì„ íƒì§€í•˜ëŠ” í—¬ìŠ¤ì¼€ì–´ ë¶„ì•¼ëŠ” ë†’ì€ ì •í™•ë„ë¥¼ ìš”êµ¬í•œë‹¤ëŠ” íŠ¹ì§•**ì´ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ê¸´ í•™ìŠµ ì‹œê°„ì„ í•„ìš”ë¡œ í•˜ë”ë¼ë„ **ì‹¤ì œ ì´ìƒì„ ì •í™•íˆ íƒì§€í•˜ëŠ” ê²ƒì´ ëª©ì ì´ë¼ë©´, LSTM ëª¨ë¸ì„ ì ê·¹ì ìœ¼ë¡œ ì´ìš©**í•  ìˆ˜ë„ ìˆì„ ê²ƒì…ë‹ˆë‹¤.
    - ë³¸ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” ë‹¤ë£¨ì§€ ì•Šì•˜ìœ¼ë‚˜, ì‹¤ì œë¡œ Signalê³¼ ê°™ì€ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ë¶„ì„í•¨ì— ìˆì–´ CNNê³¼ LSTMì„ ê²°í•©í•œ ëª¨ë¸ì„ ì´ìš©í•˜ëŠ” ê²½ìš°ê°€ ë§¤ìš° ë§ìŠµë‹ˆë‹¤. ë‘ ëª¨ë¸ì„ í•¨ê»˜ ì‚¬ìš©í•˜ë©´ Spatial Features ë¿ ì•„ë‹ˆë¼ Temporal ì •ë³´ê¹Œì§€ ëª¨ë‘ ê³ ë ¤í•  ìˆ˜ ìˆë‹¤ëŠ” íŠ¹ì¥ì ì„ ì§€ë‹ˆê²Œ ë  ê²ƒì…ë‹ˆë‹¤.

<br/>

## Experiment 2
### Add Regularized Loss! 
ì•ì„œ AutoEncoderì˜ ì£¼ ëª©ì ì€ Normal ë°ì´í„°ì˜ ì •ë³´ë¥¼ ì˜ ë³´ì¡´í•˜ì—¬ ì••ì¶•í•˜ëŠ” ê²ƒì´ë¼ ì–¸ê¸‰í•œ ë°” ìˆìŠµë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ **ì›ë³¸ ì…ë ¥ ë°ì´í„°ì˜ ì •ë³´ê°€ ì••ì¶•ëœ Latent Vectorì™€ Input Vector ê°„ ì°¨ì´ë¥¼ ê³„ì‚°í•˜ì—¬ ì´ë¥¼ ì¤„ì¼ ìˆ˜ëŠ” ì—†ì„ê¹Œìš”?**

<br/>

> __(ì°¸ê³ )__   
> **Q. ì™œ Bottlencek LayersëŠ” ì…ë ¥ ë³€ìˆ˜ì˜ ì°¨ì›ë³´ë‹¤ ë‚®ì•„ì•¼ í•˜ë‚˜ìš”? ğŸ¤”**   
> **A. ìš°ë¦¬ê°€ ê°€ì§€ëŠ” ê³ ì°¨ì›ì˜ ë°ì´í„°ë“¤ì€ ë‚´ì¬ëœ ì ì¬ ìš”ì¸, ì¦‰ Latent Variableì— ì˜í•´ ë°œí˜„ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.**
>
> ë”°ë¼ì„œ ì°¨ì› ì¶•ì†Œë¥¼ í†µí•´ ì €ì°¨ì›ì˜ ê¸°ì € ìš”ì¸ë“¤ì„ ì˜ ì••ì¶•í•´ì„œ ì¶”ì¶œí•´ë‚´ëŠ” ê²ƒì´ AutoEncoderì˜ ëª©ì ì´ ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì»¨ì…‰ì„ ì–´ë ¤ìš´ ë§ë¡œ **Manifold Hypothesis**ë¼ê³  í•©ë‹ˆë‹¤.
> 

<br/>

- ì–´ë–¤ ëŒ€ìƒì˜ ì°¨ì´ë¥¼ êµ¬í•˜ëŠ” ë°©ë²•ë¡ ì€ ë§¤ìš° ë§ìŠµë‹ˆë‹¤. ê³§ ë‹¤ë£¨ê²Œ ë  Semi-Supervised Learningì˜ ë°©ë²•ë¡  ì¤‘ Consistency Regularization ê¸°ë°˜ì˜ ì—°êµ¬ë“¤ë„ ì¼ê´€ì„± ìœ ì§€ë¥¼ ìœ„í•´ ëŒ€ìƒ ê°„ ì°¨ì´ë¥¼ ì¢íˆëŠ” ë°©ë²•ì„ ì—¬ëŸ¬ ê°€ì§€ í™œìš©í•˜ê³  ìˆì£ .
    - ê·¸ ì°¨ì´ë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ ì¸¡ì •í•˜ëŠ” ëŒ€í‘œì ì¸ ë°©ë²•ìœ¼ë¡œëŠ” MSE, KL-Divergence, Jensen-Shannon Divergence ë“±ì´ ìˆìŠµë‹ˆë‹¤.

<br/>

- ì‹¤ì œë¡œ ì°¸ê³ ë¬¸í—Œ [1]ì—ì„œëŠ” **Normal ë°ì´í„°ì˜ ë¶„í¬ì™€ Latent Vectorì˜ ì°¨ì´ë¥¼ ì¢íˆë©´ ì„±ëŠ¥ í–¥ìƒì„ ê¸°ëŒ€í•  ìˆ˜ ìˆë‹¤ê³  ì£¼ì¥**í•œ ë°” ìˆìŠµë‹ˆë‹¤.
    - ë…¼ë¬¸ì—ì„œëŠ” Signal ë°ì´í„°ì— ëŒ€í•´ì„œ, Latent Vector Representationì´ Smoothí•˜ì§€ ì•Šê³ , ì—°ì†ì ì¸ ê°’ì„ ê°€ì§€ê¸° ì–´ë µë‹¤ëŠ” ì ì„ ì§€ì í•˜ë©° ì´ëŸ¬í•œ ë¬¸ì œê°€ Input Signalì— ëŒ€í•œ ë³µì›ì„ ì–´ë µê²Œ í•œë‹¤ê³  ì£¼ì¥í•©ë‹ˆë‹¤.
    - ë”ë¶ˆì–´ Objective Functionìœ¼ë¡œ Reconstruction Errorë§Œì„ í™œìš©í•˜ì—¬ ëª¨ë¸ì„ êµ¬ì¶•í•˜ë©´ ì…ë ¥ ê°’ì˜ Outliersê¹Œì§€ í•™ìŠµí•˜ì—¬ ë³µì›ì„ ì–´ë µê²Œ í•˜ëŠ” ë¬¸ì œë„ í•¨ê»˜ ì§€ì í•˜ê³  ìˆìŠµë‹ˆë‹¤.

<br/>

- ë”°ë¼ì„œ ë³¸ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” [1]ì˜ ë°©ë²•ë¡ ì„ ì°¸ê³ í•˜ì—¬ **KL Divergenceë¥¼ í†µí•´ Latent Vectorì™€ Input Data ê°„ ì°¨ì´ë¥¼ ì¢íˆë©´ ë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë„ì¶œí•  ìˆ˜ ìˆëŠ”ì§€**ë¥¼ í™•ì¸í•´ë³´ê³ ì í•©ë‹ˆë‹¤.

<br/>

> __(ì°¸ê³ )__   
> __Q. KL Divergenceê°€ ë¬´ì—‡ì¸ê°€ìš”? ğŸ¤”__   
> __A. Kullback-Leibler Divergenceì˜ ì¤€ë§ë¡œ, ì—¬ê¸°ì„œ DivergenceëŠ” "ì°¨ì´"ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ë‘ í™•ë¥  ë¶„í¬ì˜ ì°¨ì´ë¥¼ ë¹„êµí•˜ì—¬ ì´ë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ì²™ë„ë¡œì„œ í™œìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤.__
>
> $$D_{KL}(P||Q) = \sum_{x\in \chi }P(x)log_b(\frac{P(x)}{Q(x)})   $$
> KL divergenceëŠ” ìœ„ ì‹ì„ í†µí•´ ê³„ì‚°ë˜ë©°, í™•ë¥  ë¶„í¬ Pë¥¼ ëª¨ë¸ë§í•˜ëŠ” ê²ƒì´ ëª©ì ì´ë¼ í•  ë•Œ, ì´ì‚°í™•ë¥ ë¶„í¬ Pì™€ Qê°€ ë™ì¼í•œ ìƒ˜í”Œ ê³µê°„ $\chi$ì—ì„œ ì •ì˜ëœë‹¤ê³  í•˜ì˜€ì„ ë•Œì˜ ê°’ì…ë‹ˆë‹¤.

<br/>

```python
def loss_function(out, real):
    pred, latent= out[0], out[1]
    z = torch.rand(latent.size(0), latent.size(1), device=device)
    
    mse=F.mse_loss(pred,real,reduction="sum")
    kl=F.kl_div(latent,z,reduction="batchmean")
    
    return kl+mse
```
- Loss Functionì„ **MSEì™€ ë”ë¶ˆì–´ KL Divergenceë¥¼ ì¶”ê°€í•˜ì—¬ í•¨ê»˜ ìµœì†Œí™” ì‹œí‚¤ëŠ” ë°©í–¥ìœ¼ë¡œ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸**í•˜ë„ë¡ í•˜ì˜€ìŠµë‹ˆë‹¤.
- ê·¸ë ‡ë‹¤ë©´ ìµœì¢…ì ì¸ Objective Functionì€ ì•„ë˜ì™€ ê°™ì•„ì§ˆ ê²ƒì…ë‹ˆë‹¤.
$$Objective Function = MSE + KLD$$

<br/>

ì´ë•Œ ëª¨ë¸ êµ¬ì¡°ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.
### MLP AutoEnocder for Regularized Loss
```python
class KL_MLPAutoencoder(nn.Module):
    
    def __init__(self, config: object):
        super().__init__()
        
        self.Encoder = nn.Sequential(
            nn.Linear(self.seq_len, self.hidden_1),
            nn.ReLU(inplace=True),
            
            nn.Linear(self.hidden_1, self.hidden_2),
            nn.Softmax(dim=1),
            
            nn.Linear(self.hidden_2, self.hidden_3),
            nn.Softmax(dim=1),
        )
        
        self.Decoder = nn.Sequential(
            
            nn.Linear(self.hidden_3, self.hidden_2),
            nn.ReLU(inplace=True),
            
            nn.Linear(self.hidden_2, self.hidden_1),
            nn.ReLU(inplace=True),
            
            nn.Linear(self.hidden_1, self.seq_len),
            nn.Sigmoid()
        )
        
    def forward(self, x):
        x = x.view(-1, self.seq_len)
        latent_vector=self.Encoder(x)
        pred = self.Decoder(latent_vector)
        
        return pred.view(-1,187),torch.log(latent_vector.view(-1,32))    
```

### LSTM AutoEnocder for Regularized Loss
```python
class KL_LSTMEncoder(nn.Module):
    def __init__(self, config: object):
        super().__init__()
        self.hidden_dim= config.hideen_dim
        self.num_features = config.num_features
        self.seq_len = config.seq_len

        self.lstm1 = nn.LSTMCell(self.num_features,self.hidden_dim)
        self.lstm2 = nn.LSTMCell(self.num_features,self.hidden_dim)
        self.linear = nn.Linear(self.hidden_dim, self.num_features)
        self.sigmoid = nn.Sigmoid()

    def Encoder(self, x):
        ht=torch.zeros(x.size(0),self.hidden_dim,dtype=torch.float,device=device)
        ct=torch.zeros(x.size(0),self.hidden_dim,dtype=torch.float,device=device)
        
        for input in x.chunk(x.size(1),dim = 1):
            ht,ct=self.lstm1(input,(ht, ct))
            
        return ht,ct
    
    def Decoder(self,ht,ct):
        ot=torch.zeros(ht.size(0),self.num_features, type=torch.float, device=config.device)
        outputs=torch.zeros(ht.size(0),187,dtype=torch.float,device=config.device)
        
        for i in range(self.seq_len):
            ht,ct=self.lstm2(ot,(ht,ct))
            ot=self.sigmoid(self.linear(ht))
            outputs[:,i]=ot.squeeze()
            
        return outputs

    def forward(self, x):
        he, ce = self.Encoder(x) #hidden encoder,cell_state encoder
        out = self.Decoder(he, ce)
        return torch.flip(out, dims=[1]), torch.log(F.softmax(he, dim=1))
```

### 1D Convolution AutoEncoder for Regulaized Loss
```python
class KLAutoencoder(nn.Module):
    
    def __init__(self, config: object):
        super().__init__()
        
        self.Encoder=nn.Sequential(
            nn.Conv1d(1,2,8),
            nn.ReLU(inplace=True),
            nn.MaxPool1d(2),
            
            nn.Conv1d(2,4,11),
            nn.ReLU(inplace=True),
            nn.MaxPool1d(2),
            
            nn.Conv1d(4,8,11),
            nn.ReLU(inplace=True),
            nn.MaxPool1d(2),
            
            nn.Conv1d(8,16,6),
            nn.ReLU(inplace=True),
            nn.MaxPool1d(2),
            
            nn.Conv1d(16,32,5),
            nn.Softmax(dim=1),
        )
        
        self.Decoder=nn.Sequential(
            nn.ConvTranspose1d(32,16,15,1,0),
            nn.ReLU(inplace=True),
            
            nn.ConvTranspose1d(16,8,12,2,0),
            nn.ReLU(inplace=True),
            
            nn.ConvTranspose1d(8,4,12,2,0),
            nn.Sigmoid(),
            
            nn.ConvTranspose1d(4,2,12,2,5),
            nn.Sigmoid(),
            
            nn.ConvTranspose1d(2,1,8,1,0),
            nn.Sigmoid(),
        )
        
    def forward(self,x):
        x=x.view(-1,config.num_features, config.seq_len)
        latent_vector=self.Encoder(x)
        fake_signal=self.Decoder(latent_vector)
        
        return fake_signal.view(-1,config.seq_len),torch.log(latent_vector.view(-1,32))
```

### Result of Experiment 2
ê²°ê³¼ë¥¼ ì •ë¦¬í•˜ê³  í•´ì„í•˜ê² ìŠµë‹ˆë‹¤. ì´ Epochì„ 100íšŒë¥¼ ëŒë¦° ê²°ê³¼, ê¸°ì¡´ MLP/LSTM/Conv1D AutoEncoderê³¼ ë”ë¶ˆì–´ KL Divergenceë¥¼ Loss Termì— ì¶”ê°€í•œ ëª¨ë¸ì˜ í•™ìŠµ ì†Œìš” ì‹œê°„ ë° ì„±ëŠ¥ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.  

|                 |__MLP AutoEncoder__|__LSTM AutoEncoder__|__Conv1D AutoEncoder__|__KL MLP_AutoEncoder__|__KL LSTM_AutoEncoder__|__KL Conv1D AutoEncoder__|   
|-----------------|:-----------------:|:------------------:|:--------------------:|:---:|:---:|:---:|
|__Training Time (sec)__| __69__ | 854 | 90 | 75 | 2720 | 96 |
|__Threshold__| 0.10 | 0.03 | 0.03 | 0.32 |  | 0.03 |
|__F1 Score ($\beta = 1.0$)__| 0.84 | 0.75 | __0.88__ | 0.69 | 0.59 | 0.67 |
|__Accuracy Abnormal (%)__| 82.50 | 93.33 | 80.00 | 84.17 | __99.57__ | 99.17 |

- ì‹¤í—˜ ê²°ê³¼ ë™ì¼ ì¡°ê±´ì—ì„œ KL Divergence ì—°ì‚°ì„ Loss Termë§Œ ì¶”ê°€í–ˆì„ ë•Œ **ì˜¤íˆë ¤ ì„±ëŠ¥ì´ ë‚®ì•„ì§€ëŠ” ê²ƒì„ í™•ì¸**í•˜ì˜€ìŠµë‹ˆë‹¤.
- ë…¼ë¬¸ì—ì„œëŠ” ë™ì¼ ì¡°ê±´ì—ì„œ MSE Lossë§Œì„ í™œìš©í•œ MLP, LSTM, Conv1D ëª¨ë¸ë³´ë‹¤ KL Divergenceë¥¼ í™œìš©í–ˆì„ ë•Œ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆë‹¤ê³  ì£¼ì¥í•˜ì§€ë§Œ, Hyperparameterë¥¼ ë…¼ë¬¸ê³¼ëŠ” ë‹¤ë¥´ê²Œ ë³€ê²½í•œ ê²°ê³¼ ì˜¤íˆë ¤ ë‚®ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ê²ƒì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤.
- ê·¸ëŸ¬ë‚˜ Accuracy Abnormalì˜ ê²½ìš° KL LSTM AutoEncoderê°€ ê°€ì¥ ë†’ì•˜ìŠµë‹ˆë‹¤.

<br/>

### Conclusion of Experiment 2

- [1]ì—ì„œëŠ” epochì„ 1000ìœ¼ë¡œ ëŒë¦¬ë©° Lossê°€ ì ì  ìˆ˜ë ´í•˜ëŠ” í˜•íƒœì˜ Plotì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ epochì„ 100ì´ ì•„ë‹Œ 1000ìœ¼ë¡œ ëŠ˜ë ¤ í•™ìŠµí•˜ë©´ ë…¼ë¬¸ì—ì„œì˜ ì„±ëŠ¥ì„ ê¸°ëŒ€í•  ìˆ˜ë„ ìˆì„ ê²ƒì…ë‹ˆë‹¤. ê²Œë‹¤ê°€ Train ì‹œì— í™œìš©í•˜ëŠ” Dataset Setting ë° Trainer ë™ì‘ ë°©ì‹ì´ ë…¼ë¬¸ì˜ ê²ƒê³¼ëŠ” ë‹¬ëê¸°ì—, **Setting ë³€ê²½ì— ë”°ë¼ ì„±ëŠ¥ ë³€í™”ê°€ í¬ê²Œ ë‚¨ì„ ì‹¤í—˜ì ìœ¼ë¡œ í™•ì¸**í•˜ì˜€ìŠµë‹ˆë‹¤.
- í•˜ì§€ë§Œ KL Divergenceë¡œ **Regularization Termì„ ì¶”ê°€í•œ ê²½ìš°, KL LSTM AutoEncoderì˜ Accuracy Abnormal ì„±ëŠ¥ì´ ê¸°ì¡´ì˜ ê²ƒë³´ë‹¤ í›¨ì”¬ ìƒí–¥ë˜ë©° ë¶€ì •ë§¥ ê²€ì¶œì— ë”ìš± ìš©ì´**í•´ì§„ ê²ƒì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤.
- ê·¸ëŸ¬ë‚˜ **ì—°ì‚°ëŸ‰ì´ ë§ê³ , Hyperparameter ë³€ê²½ì— ë”°ë¼ ê¸°ëŒ€í•œ ë§Œí¼ì˜ ì„±ëŠ¥ì´ ë„ì¶œë˜ì§€ ì•Šì•˜ê¸°ì—, ë³¸ íŠœí† ë¦¬ì–¼ì˜ Settingìœ¼ë¡œëŠ” Latent Vectorì˜ Representationì´ ê·¸ë‹¤ì§€ ìœ ìš©í•˜ì§€ ì•ŠìŒì„ í™•ì¸**í•˜ì˜€ìŠµë‹ˆë‹¤.
- ë˜í•œ Loss ê·œì œë¥¼ í†µí•´ ì—°ì‚°ëŸ‰ì´ í›¨ì”¬ ë§ì•„ì§€ë©° **Training Costë„ ë§¤ìš° ì˜¬ë¼ê°„ ê²ƒì„ í™•ì¸**í•˜ì˜€ìŠµë‹ˆë‹¤.

<br/>

# Appendix
- ë³¸ íŠœí† ë¦¬ì–¼ì—ì„œ Thresholdë¥¼ ì„¤ì •í•˜ëŠ” ë°©ë²•ì€, ë°ì´í„°ì— ì •ìƒ/ë¹„ì •ìƒì„ êµ¬ë¶„í•˜ëŠ” Labelì´ ìˆë‹¤ëŠ” ê°€ì • í•˜ì—ì„œ ê°€ëŠ¥í•œ ê²ƒì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì‹¤ì œ ë°ì´í„°ì…‹ì—ëŠ” Labelì´ ì—†ëŠ” ê²½ìš°ê°€ ë§ìœ¼ë©°, ë”°ë¼ì„œ Unsupervised Methodë¥¼ ì´ìš©í•œ Threshold Search ë°©ë²•ë¡ ì„ ì¶”ê°€ì ìœ¼ë¡œ íƒìƒ‰í•˜ì—¬ ì ìš©í•  ìˆ˜ë„ ìˆì„ ê²ƒì…ë‹ˆë‹¤.

----

<br/>

# References
[1] Chandra, A., & Kala, R. (2019, December). Regularised encoder-decoder architecture for anomaly detection in ECG time signals. In 2019 IEEE Conference on Information and Communication Technology (pp. 1-6). IEEE.
[2] https://curiousily.com/posts/time-series-anomaly-detection-using-lstm-autoencoder-with-pytorch-in-python/